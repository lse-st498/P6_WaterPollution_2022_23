{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrR-uN_IM30X"
      },
      "source": [
        "**Models**:\n",
        "- Logistic Regression (LR)\n",
        "- Random Forest (RF)\n",
        "- Neural Network (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vIrrlI5qYYy"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "bs7kFeNpqVMd"
      },
      "source": [
        ">[Table of Contents](#scrollTo=1vIrrlI5qYYy)\n",
        "\n",
        ">[Load Packages](#scrollTo=7kYT-Av0M30Y)\n",
        "\n",
        ">[Load Datasets and Basic Data Cleaning](#scrollTo=W8j2Yk93M30a)\n",
        "\n",
        ">>[BC & RF: features dataset](#scrollTo=ogBM3dkMM30a)\n",
        "\n",
        ">>[NN: sites_data.csv](#scrollTo=XsGpGZJoM30b)\n",
        "\n",
        ">>[All: Pollution Data](#scrollTo=iAjdKpWAM30b)\n",
        "\n",
        ">[Data Manipulation and Further Data Cleaning](#scrollTo=7jfpVEdhM30b)\n",
        "\n",
        ">>>[BC & RF: Replace NaNs with mean](#scrollTo=2tNYFCXyM30b)\n",
        "\n",
        ">>>[BC & RF: Replace NaNs with -10](#scrollTo=w2nMfJ_MM30c)\n",
        "\n",
        ">[Train-Test Split](#scrollTo=k5Rh25eDM30d)\n",
        "\n",
        ">>[5-fold CV on Training Data](#scrollTo=NVfVZtslVDgN)\n",
        "\n",
        ">[Train & Test Models](#scrollTo=LqVCHBwqM30d)\n",
        "\n",
        ">>[Baseline](#scrollTo=WCq5OTnjM30d)\n",
        "\n",
        ">>[BC](#scrollTo=fCm5MtmEM30d)\n",
        "\n",
        ">>>[df_merged_mean](#scrollTo=sk2ahJhvM30d)\n",
        "\n",
        ">>>>[ROC](#scrollTo=AUtGhFBxM30f)\n",
        "\n",
        ">>>[df_merged_neg](#scrollTo=LbH3oq34M30f)\n",
        "\n",
        ">>[RF](#scrollTo=tX_m83LwM30g)\n",
        "\n",
        ">>>[df_merged_mean](#scrollTo=WwjWWDddM30g)\n",
        "\n",
        ">>>>[Importance of Feature Aggregation](#scrollTo=HSd3THghM30h)\n",
        "\n",
        ">>>>[PCA](#scrollTo=ce8UQqnCM30h)\n",
        "\n",
        ">>>[df_merged_neg](#scrollTo=yNtOjtXHM30h)\n",
        "\n",
        ">>>>[Importance of Feature Aggregation](#scrollTo=d85180_iM30i)\n",
        "\n",
        ">>>>[PCA](#scrollTo=3iz1OD5WM30i)\n",
        "\n",
        ">>[NN](#scrollTo=HqmPxsTQM30j)\n",
        "\n",
        ">>>[Helper Functions](#scrollTo=kweB7fnOM30j)\n",
        "\n",
        ">>>[Training Models](#scrollTo=42b7DOTtM30k)\n",
        "\n",
        ">>>>[Training From Scratch](#scrollTo=orGBi9j_QSm6)\n",
        "\n",
        ">>>>>[With Cross Validation (Takes up too much RAM)](#scrollTo=CQ2WRerRdX5E)\n",
        "\n",
        ">>>>>[Save Model](#scrollTo=VY6Q610dder6)\n",
        "\n",
        ">>>>[Training From Existing Model](#scrollTo=7_MflnfHQXCz)\n",
        "\n",
        ">>>>>[Load Model From File](#scrollTo=qbDjSSIaoPmH)\n",
        "\n",
        ">>>>>[Training](#scrollTo=um8esTkeoRAe)\n",
        "\n",
        ">[Final Results](#scrollTo=KCRcg5wRM30k)\n",
        "\n",
        ">[Window Size Comparison](#scrollTo=MhySSdKwM30l)\n",
        "\n",
        ">>[BC](#scrollTo=_xgmshK1M30l)\n",
        "\n",
        ">>[RF](#scrollTo=kiYFhCJiM30l)\n",
        "\n",
        ">>[NN](#scrollTo=Y54BwAv-M30l)\n",
        "\n",
        ">[Accuracy Comparison](#scrollTo=yw0R9N0gM30m)\n",
        "\n",
        ">>[NN](#scrollTo=OZhXFnbPM30m)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kYT-Av0M30Y"
      },
      "source": [
        "# &nbsp; Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRa3XwTrM30Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BswLk-JvM30a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score, make_scorer, f1_score, precision_score, recall_score\n",
        "from datetime import datetime\n",
        "from keras.optimizers import Adam\n",
        "import itertools\n",
        "from IPython.display import clear_output\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import gc\n",
        "import csv\n",
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wJIjVGNr_zi"
      },
      "source": [
        "# Load Cleaned Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsGpGZJoM30b"
      },
      "source": [
        "## NN: sites_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvGzcancMq8s",
        "outputId": "db741af4-4063-4d06-b71f-0890b1bbb511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.75 s, sys: 2.5 s, total: 4.26 s\n",
            "Wall time: 7.76 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# L3 data, Sentinel 3, 1km x 1km, 6 features\n",
        "# sites_data = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_15x15_6_features_na_mean_zero_no_norm.pkl\")\n",
        "sites_data = pd.read_pickle(\"~data/nn_15x15_6_features_na_mean_neg10_no_norm.pkl\")\n",
        "# sites_data = pd.read_pickle(\"~data/nn_15x15_6_features_na_mean_zero_no_norm.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Rh25eDM30d"
      },
      "source": [
        "# Train-Test-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H3A3QaYFoMww"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/train_labels_mean_zero.csv\")[['time', 'site']]\n",
        "# time_site_pairs_valid = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/validation_labels_mean_zero.csv\")[['time', 'site']]\n",
        "# time_site_pairs_test = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/test_labels_mean_zero.csv\")[['time', 'site']]\n",
        "\n",
        "time_site_pairs_train = pd.read_csv(\"~data/train_labels_mean_zero.csv\")[['time', 'site']]\n",
        "time_site_pairs_valid = pd.read_csv(\"~data/validation_labels_mean_zero.csv\")[['time', 'site']]\n",
        "time_site_pairs_test = pd.read_csv(\"~data/test_labels_mean_zero.csv\")[['time', 'site']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THBcRhhjrHJE"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train2 = time_site_pairs2[~time_site_pairs2.isin(time_site_pairs_test2)].dropna()\n",
        "# time_site_pairs_train2 = time_site_pairs_train2.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd6JZin8DtKR"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs = sites_data[['time', 'site']]\n",
        "\n",
        "# # shuffle df\n",
        "# time_site_pairs = time_site_pairs.sample(frac=1, random_state=42)\n",
        "\n",
        "# # rows of split\n",
        "# total_rows = time_site_pairs.shape[0]\n",
        "# train_split = int(0.6 * total_rows)\n",
        "# validation_split = int(0.2 * total_rows)\n",
        "\n",
        "# # split\n",
        "# train_df = time_site_pairs[:train_split]\n",
        "# validation_df = time_site_pairs[train_split:(train_split + validation_split)]\n",
        "# test_df = time_site_pairs[(train_split + validation_split):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "317lNtS9Eql8"
      },
      "outputs": [],
      "source": [
        "# # sanity checks\n",
        "\n",
        "# print(f\"Rows of entire dataset: {time_site_pairs.shape[0]}\")\n",
        "# print(f\"Rows of train: {train_df.shape[0]}\")\n",
        "# print(f\"Rows of validation: {validation_df.shape[0]}\")\n",
        "# print(f\"Rows of test: {test_df.shape[0]}\")\n",
        "\n",
        "# train = sites_data.merge(train_df, on=['time', 'site'], how='inner')\n",
        "# validation = sites_data.merge(validation_df, on=['time', 'site'], how='inner')\n",
        "# test = sites_data.merge(test_df, on=['time', 'site'], how='inner')\n",
        "\n",
        "# print(f\"Positive samples in train: {sum(train['riskLevelLabel'])}. % of Positive samples in train: {sum(train['riskLevelLabel'])/len(train_df)*100}%\")\n",
        "# print(f\"Positive samples in validation: {sum(validation['riskLevelLabel'])}. % of Positive samples in validation: {sum(validation['riskLevelLabel'])/len(validation_df)*100}%\")\n",
        "# print(f\"Positive samples in test: {sum(test['riskLevelLabel'])}. % of Positive samples in test: {sum(test['riskLevelLabel'])/len(test_df)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2cmb4MEZqb"
      },
      "outputs": [],
      "source": [
        "# # export to csv\n",
        "\n",
        "# train_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/train_labels_mean_zero.csv\")\n",
        "# validation_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/validation_labels_mean_zero.csv\")\n",
        "# test_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/test_labels_mean_zero.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klJXVycTM30d"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs = sites_data[['time', 'site']]\n",
        "\n",
        "# # 80/20 split\n",
        "# time_site_pairs_test = time_site_pairs.sample(frac=.2, random_state=42)\n",
        "# time_site_pairs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MikeDtoyM30d"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train = time_site_pairs[~time_site_pairs.isin(time_site_pairs_test)].dropna()\n",
        "# time_site_pairs_train = time_site_pairs_train.sample(frac=1, random_state=42)\n",
        "# time_site_pairs_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdjRP_JPNN4"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_test.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_test_6_features.csv\")\n",
        "# time_site_pairs_train.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_train_6_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id1Ckt-5RHxW"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_test = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_test_6_features.csv\")[['time', 'site']]\n",
        "# time_site_pairs_train = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_train_6_features.csv\")[['time', 'site']]\n",
        "# #time_site_pairs_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNWJaO4_lX7e"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train.shape[0] + time_site_pairs_test.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfVZtslVDgN"
      },
      "source": [
        "## 5-fold CV on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J-9bdnQsTB8s"
      },
      "outputs": [],
      "source": [
        "# Randomise order of training pairs\n",
        "time_site_pairs_train_new = time_site_pairs_train.sample(frac = 1)\n",
        "\n",
        "train_val_dict = {}\n",
        "\n",
        "# Get CV Train and Test time-site pairs\n",
        "for i in range(5):\n",
        "  split_index = round(len(time_site_pairs_train_new)/5)\n",
        "  train_val_dict[f'val_{i+1}'] = time_site_pairs_train_new[i*split_index: (i+1)*split_index]\n",
        "  train_val_dict[f'train_{i+1}'] = time_site_pairs_train_new.drop(time_site_pairs_train_new.index[i*split_index: (i+1)*split_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqVCHBwqM30d"
      },
      "source": [
        "# Train & Test Models\n",
        "- Train on training time-site pairs\n",
        "- Test on testing time-site pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kweB7fnOM30j"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG9pjA12XV_n"
      },
      "source": [
        "#### Reshaping Data as Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ixN0KB4UM30j"
      },
      "outputs": [],
      "source": [
        "def get_train_test_val_nn(input_data, train_labels, test_labels, dim = 15, oversampling = False, augment=True, desired_pos_ratio =  0.5, train_val_ratio = 0.8):\n",
        "    '''\n",
        "    Gets train, test and validation datasets for a neural network model.\n",
        "\n",
        "    input:\n",
        "        - input_data (pd.DataFrame):\n",
        "            - dataframe of shape (m, n)\n",
        "            - number of datapoints = m\n",
        "            - features to consider = n-1\n",
        "            - one of the columns = 'riskLevelLabel'\n",
        "\n",
        "        - train_labels / test_labels (pd.DataFrame):\n",
        "            - dataframe with two columns 'time' and 'site'\n",
        "            - time and site pairs for train/test data\n",
        "\n",
        "        - oversampling (boolean):\n",
        "            - Whether oversampling should be performed\n",
        "\n",
        "        - desired_pos_ratio (float):\n",
        "            - desired ratio of positive samples when performing random oversampling\n",
        "\n",
        "        - train_val_ratio (float):\n",
        "            - ratio of training data to validation data\n",
        "\n",
        "    output:\n",
        "        - X_train (tensor)\n",
        "        - X_test (tensor)\n",
        "        - X_val (tensor)\n",
        "        - y_train (np.array)\n",
        "        - y_test (np.array)\n",
        "        - y_val (np.array)\n",
        "    '''\n",
        "\n",
        "\n",
        "    train = pd.merge(train_labels, input_data, on=['time', 'site'])\n",
        "    test = pd.merge(test_labels, input_data, on=['time', 'site'])\n",
        "\n",
        "    ################\n",
        "    # Changing window size\n",
        "    ################\n",
        "    w = int((dim-1)/2)\n",
        "\n",
        "    def get_windowed_data(row):\n",
        "        indices = np.array(range(1,226)).reshape(15,15)[7-w:8+w, 7-w:8+w].flatten()\n",
        "        indices = [i-1 for i in indices]\n",
        "        values = row.flatten()[[indices]].reshape(dim,dim)\n",
        "        return values\n",
        "\n",
        "    if dim != 15:\n",
        "        for feature in ['ZSD','CHL','SPM','KD490','BBP','CDM']:\n",
        "            train[f'{feature}'] = train[f'{feature}'].apply(get_windowed_data)\n",
        "            test[f'{feature}'] = test[f'{feature}'].apply(get_windowed_data)\n",
        "\n",
        "    ################\n",
        "    # Getting X & y, train & val\n",
        "    ################\n",
        "\n",
        "    # Getting X & y\n",
        "    features_column_names = list(input_data.columns)\n",
        "    for x in ['riskLevelLabel', 'time', 'site']:\n",
        "        features_column_names.remove(x)\n",
        "\n",
        "    X_train, X_test = train[features_column_names], test[features_column_names]\n",
        "    y_train = train['riskLevelLabel']\n",
        "    y_test = test['riskLevelLabel']\n",
        "\n",
        "    # Train Validation Split\n",
        "    i = int(X_train.shape[0] * train_val_ratio)\n",
        "    X_val, y_val = X_train[i:], y_train[i:]\n",
        "    X_train, y_train = X_train[:i], y_train[:i]\n",
        "\n",
        "    ################\n",
        "    # Oversampling (on training data)\n",
        "    ################\n",
        "\n",
        "    if oversampling:\n",
        "        # Counting number of samples to oversample\n",
        "        num_positives, num_negatives = sum(y_train), len(y_train)-sum(y_train)\n",
        "        num_positives_to_repeat = int(desired_pos_ratio * num_negatives * 2) - num_positives\n",
        "\n",
        "        # Oversampling\n",
        "        ros = RandomOverSampler(sampling_strategy={1: num_positives_to_repeat}, random_state=42)\n",
        "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "    ################\n",
        "    # Reshaping into tensors\n",
        "    ################\n",
        "\n",
        "    X_dfs = []\n",
        "    for X_df in [X_train, X_test, X_val]:\n",
        "\n",
        "        # Reshape and Convert to Tensor\n",
        "        if X_df.shape[1] == 1:\n",
        "            X_df = np.array([i for i in X_df[features_column_names[0]]])\n",
        "            X_df = tf.convert_to_tensor(X_df)\n",
        "            X_df = tf.expand_dims(X_df, axis=3, name=None)\n",
        "        else:\n",
        "            X_df = np.stack([np.stack(X_df[col].values) for col in X_df.columns], axis=1)\n",
        "            X_df = np.transpose(X_df, (0, 2, 3, 1))\n",
        "            X_df = tf.convert_to_tensor(X_df)\n",
        "\n",
        "        # Padding\n",
        "#         X_df = tf.pad(X_df, [[0, 0], [16-w,15-w], [16-w,15-w], [0,0]])\n",
        "\n",
        "        # Append\n",
        "        X_dfs.append(X_df)\n",
        "\n",
        "    X_train, X_test, X_val = X_dfs\n",
        "\n",
        "    ################\n",
        "    # Data Augmentation on minority class\n",
        "    ################\n",
        "\n",
        "    if augment:\n",
        "      # separating positive class data points\n",
        "      positive_indices = [[i] for i in np.where(y_train == 1)[0]]\n",
        "      X_train_pos = tf.gather_nd(X_train, indices=positive_indices)\n",
        "\n",
        "      # augment\n",
        "      augmented_X = []\n",
        "\n",
        "      for j in np.arange(len(X_train_pos)):\n",
        "          original = X_train_pos[j]\n",
        "          hor_flip = tf.image.flip_left_right(original)\n",
        "          ver_flip = tf.image.flip_up_down(original)\n",
        "\n",
        "          augmented_X.append(hor_flip)\n",
        "          augmented_X.append(ver_flip)\n",
        "\n",
        "          for i in [1,2,3]:\n",
        "              augmented_X.append(tf.image.rot90(original, k=i))\n",
        "              augmented_X.append(tf.image.rot90(hor_flip, k=i))\n",
        "              augmented_X.append(tf.image.rot90(ver_flip, k=i))\n",
        "\n",
        "      # add augmented data to X_train\n",
        "      augmented_X_tensor = tf.convert_to_tensor(np.array(augmented_X))\n",
        "      X_train = tf.concat([X_train, augmented_X_tensor], 0)\n",
        "\n",
        "      # expanding y_train to match\n",
        "      y_train = pd.concat([y_train, pd.Series([1]*len(augmented_X))]).reset_index(drop=True)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(X_train.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      idx = [[i] for i in indices]\n",
        "      X_train = tf.gather_nd(X_train, indices=idx)\n",
        "      y_train = y_train[indices]\n",
        "\n",
        "\n",
        "    ################\n",
        "    # Data type for y values\n",
        "    ################\n",
        "\n",
        "    y_train = y_train.astype('float32')\n",
        "    y_test = y_test.astype('float32')\n",
        "    y_val = y_val.astype('float32')\n",
        "\n",
        "    return {'X_train': X_train,\n",
        "            'X_test': X_test,\n",
        "            'X_val': X_val,\n",
        "            'y_train': y_train,\n",
        "            'y_test': y_test,\n",
        "            'y_val': y_val}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "8_xAJjEJM30j"
      },
      "outputs": [],
      "source": [
        "# Custom F1 Score\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "        recall = TP / (Positives+K.epsilon())\n",
        "        return recall\n",
        "\n",
        "\n",
        "    def precision_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = TP / (Pred_Positives+K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
        "\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# custom loss function\n",
        "def wbce_custom(weight1_=30):\n",
        "\n",
        "    def wbce(y_true, y_pred, weight1=weight1_, weight0=1.):\n",
        "\n",
        "        tf.cast(y_true, tf.float32)\n",
        "        tf.cast(y_pred, tf.float32)\n",
        "        logloss = -(y_true * K.log(y_pred) * weight1 + (1 - y_true) * K.log(1 - y_pred) * weight0 )\n",
        "\n",
        "        return K.mean(logloss, axis=-1)\n",
        "\n",
        "    return wbce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnAa8YjDXMnx"
      },
      "source": [
        "#### Fitting NN Model (Model Specifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "rJVnzvojM30j"
      },
      "outputs": [],
      "source": [
        "def fit_nn(xy_data, model_type=\"convolution\", existing_model=None, metrics=[f1, 'Precision', 'Recall', tf.keras.metrics.AUC(curve='PR', name='PR-AUC'), 'AUC','acc'],\n",
        "           loss=wbce_custom, optimizer='adam',\n",
        "          batch_size=64, epochs=20, dropout=0.25, patience=5, verbose=1):\n",
        "    '''\n",
        "    xy_data: dictionary with X_train, X_test, X_val, y_train, y_test, y_val in this order (dict)\n",
        "    model_type: \"baseline\"/\"convolution\" (string)\n",
        "    loss: \"binary_crossentropy\" (string)\n",
        "    metrics: list of metrics to track. available metrics are: (list of string/function)\n",
        "        - \"acc\"\n",
        "        - \"AUC\"\n",
        "        - \"Precision\"\n",
        "        - \"Recall\"\n",
        "        - f1\n",
        "    '''\n",
        "\n",
        "    # unpacking data\n",
        "    X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "    # weight initializer\n",
        "    # initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "\n",
        "    output_bias = tf.keras.initializers.Constant(np.log([sum(y_train)/(len(y_train)-sum(y_train))]))\n",
        "\n",
        "    # building model\n",
        "    if existing_model==None:\n",
        "      model = models.Sequential()\n",
        "\n",
        "      if model_type == \"baseline\":\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4, input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "      elif model_type == \"convolution\":\n",
        "        model.add(layers.Conv2D(filters=6, kernel_size=5, input_shape=(15,15,6), kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "        model.add(layers.Conv2D(filters=16, kernel_size=3, kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(84, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "      elif model_type == \"convolution_v3\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=5, padding='same', input_shape=X_train.shape[1:], kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "          model.add(BatchNormalization())\n",
        "          model.add(layers.Activation('relu'))\n",
        "          model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "          model.add(layers.Conv2D(filters=16, kernel_size=5, kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "          model.add(BatchNormalization())\n",
        "          model.add(layers.Activation('relu'))\n",
        "          model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(84, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "          # model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "      elif model_type == \"convolution_v2\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=3, activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(5, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "      elif model_type == \"convolution_v1\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=3, activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(5, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "      else:\n",
        "          print('Model Type Undefined')\n",
        "\n",
        "      model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "      print(model.optimizer.get_config())\n",
        "      print(model.get_config())\n",
        "\n",
        "    else:\n",
        "      model = existing_model\n",
        "      print(model.optimizer.get_config())\n",
        "      print(model.get_config())\n",
        "\n",
        "    # callback for early stopping\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_f1', mode='max', patience=patience)\n",
        "\n",
        "    # callback for model checkpoint\n",
        "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/CapstoneProject/Models/Checkpoints',\n",
        "    #                                                                save_weights_only=True,\n",
        "    #                                                                monitor='val_f1',\n",
        "    #                                                                mode='max',\n",
        "    #                                                                save_best_only=True)\n",
        "\n",
        "    # Fit Model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[GarbageCollectorCallback(), es], verbose=verbose)\n",
        "\n",
        "    # Evaluate Model\n",
        "    result = model.evaluate(X_test, y_test, callbacks=[GarbageCollectorCallback()])\n",
        "\n",
        "    for i in [X_train, X_test, X_val, y_train, y_test, y_val]:\n",
        "      del i\n",
        "    gc.collect()\n",
        "\n",
        "    return model, history, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "geJfSI_osOUE"
      },
      "outputs": [],
      "source": [
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ykHPdYLKsnlk"
      },
      "outputs": [],
      "source": [
        "def run_nn(num_feature=3, model_type=\"convolution\", batch_size=64, epochs=100,\n",
        "           loss=wbce_custom(50), optimizer=Adam(learning_rate=0.0001),\n",
        "           existing_model = None, metrics=[\"f1\"], dropout=0.25, patience=5, verbose=1,\n",
        "           train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_test,\n",
        "           dim=15):\n",
        "\n",
        "    # start time\n",
        "    print(datetime.now())\n",
        "\n",
        "    # Getting Input Data\n",
        "    if num_feature == 1:\n",
        "        input_data_ = sites_data[['CHL', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 3:\n",
        "        input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 6:\n",
        "        input_data_ = sites_data[['ZSD', 'CHL', 'SPM', 'KD490', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 6.1:\n",
        "        input_data_ = sites_data[['SPM', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    else:\n",
        "      print('Number of features Error')\n",
        "\n",
        "    # Getting xy_data\n",
        "    xy_data = get_train_test_val_nn(input_data_,\n",
        "                          train_pairs,\n",
        "                          test_pairs\n",
        "                          dim=dim)\n",
        "\n",
        "    # Get history and result\n",
        "    model_, history, result = fit_nn(xy_data, model_type, existing_model=existing_model,\n",
        "                                     batch_size=batch_size, epochs=epochs, loss=loss,\n",
        "                                     optimizer=optimizer, dropout=dropout, patience=patience, verbose=verbose)\n",
        "\n",
        "    # Plot\n",
        "    if (existing_model == None) & (verbose != 2) & (verbose != 0):\n",
        "      plot_train_val_loss(history.history, metrics=metrics)\n",
        "\n",
        "    del xy_data\n",
        "    gc.collect()\n",
        "\n",
        "    # end time\n",
        "    print(datetime.now())\n",
        "\n",
        "    return model_, history, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfnHrElXTDp"
      },
      "source": [
        "#### Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "CiSo_cEfbBzI"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_plot(model):\n",
        "    pred_prob = model.predict(X_test)\n",
        "    pred_class = [1 if i>0.5 else 0 for i in pred_prob] \n",
        "\n",
        "    cm = confusion_matrix(y_test, pred_class)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZaJTTR3nM30k"
      },
      "outputs": [],
      "source": [
        "def plot_train_val_loss(his, metrics=['f1', 'precision', 'recall']):\n",
        "    '''\n",
        "    input:\n",
        "        - history (dictionary)\n",
        "        - metrics (list of strings)\n",
        "    output: 2 graphs\n",
        "    '''\n",
        "    fig, axs = plt.subplots(len(metrics)+1, 1, figsize=(5,5+len(metrics)*2))\n",
        "    fig.tight_layout(pad=5)\n",
        "    axs[0].plot(his['loss'])\n",
        "    axs[0].plot(his['val_loss'])\n",
        "    axs[0].title.set_text('Training Loss vs Validation Loss')\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend(['Training', 'Validation'])\n",
        "\n",
        "    for j, metric in enumerate(metrics):\n",
        "        axs[j+1].plot(his[f'{metric}'])\n",
        "        axs[j+1].plot(his[f'val_{metric}'])\n",
        "        axs[j+1].title.set_text(f'Training {metric} vs Validation {metric}')\n",
        "        axs[j+1].legend(['Training', 'Validation'])\n",
        "        axs[j+1].set_xlabel(\"Epochs\")\n",
        "        axs[j+1].set_ylabel(f\"{metric}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2xsH78d7Pdo_"
      },
      "outputs": [],
      "source": [
        "def get_combined_history(history_list):\n",
        "  '''\n",
        "  history_list (list): list of dictionaries containing histories, in order\n",
        "  '''\n",
        "  dd = defaultdict(list)\n",
        "  all_history = {}\n",
        "\n",
        "  for d in [old_history, history.history]:\n",
        "    for key, value in d.items():\n",
        "      dd[key].append(value)\n",
        "\n",
        "  for key, value in dd.items():\n",
        "      all_history[key] = [item for sublist in value for item in sublist]\n",
        "\n",
        "  return all_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5O5qTNXy3Hk"
      },
      "source": [
        "#### Save & Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dIx4yB2Xy4HR"
      },
      "outputs": [],
      "source": [
        "def save_model(model, history, result, model_notes, model_specs = '51x51'):\n",
        "  # model name\n",
        "  now = datetime.now()\n",
        "  day, month, time = now.day, now.month, now.strftime(\"%H%M\")\n",
        "  model_name = f\"nn_{model_specs}_{month}_{day}_{time}\"\n",
        "\n",
        "  # save model\n",
        "  model.save(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}.keras')\n",
        "\n",
        "  # save history\n",
        "  with open(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}_hist', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "\n",
        "  model_notes_row = [model_name, model_notes, result]\n",
        "\n",
        "  with open('/content/drive/My Drive/CapstoneProject/Models/model_notes.csv', 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    f.write(\"\\n\")\n",
        "    writer.writerow(model_notes_row)\n",
        "    f.close()\n",
        "\n",
        "  print(f\"Model Name: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l3-TWlbe0kbI"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, loss_weight=30):\n",
        "  # Load Model\n",
        "  loss = wbce_custom(loss_weight)\n",
        "  metric = f1\n",
        "  old_model = tf.keras.models.load_model(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})\n",
        "\n",
        "  # Load History\n",
        "  with open(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}_hist', \"rb\") as file_pi:\n",
        "      old_history = pickle.load(file_pi)\n",
        "\n",
        "  notes = pd.read_csv((\"/content/drive/My Drive/CapstoneProject/Models/model_notes.csv\"))\n",
        "  print(notes[notes['Model Name'] == model_name]['Notes'].iloc[0])\n",
        "\n",
        "  return old_model, old_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b7DOTtM30k"
      },
      "source": [
        "### Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orGBi9j_QSm6"
      },
      "source": [
        "#### Training From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inlpz-2fyRS-"
      },
      "source": [
        "##### Without Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iZBA7FJpdPQ7",
        "outputId": "99589db1-fe8b-4167-d871-6371d2fe2702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-19 02:24:00.026959\n",
            "{'name': 'Adam', 'learning_rate': 1e-05, 'decay': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_22', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_44_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_44', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_44', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_44', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_43', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'same', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_45', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_45', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_45', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_44', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_22', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_88', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_66', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_89', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_67', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_90', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_68', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_91', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "Epoch 1/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 3.1044 - acc: 0.5766 - auc: 0.5294 - precision: 0.2758 - recall: 0.4111 - f1: 0.3038 - val_loss: 2.6415 - val_acc: 0.4776 - val_auc: 0.5807 - val_precision: 0.0371 - val_recall: 0.6687 - val_f1: 0.0691\n",
            "Epoch 2/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.9692 - acc: 0.5181 - auc: 0.5678 - precision: 0.2953 - recall: 0.6482 - f1: 0.4001 - val_loss: 2.5647 - val_acc: 0.4915 - val_auc: 0.5875 - val_precision: 0.0379 - val_recall: 0.6642 - val_f1: 0.0702\n",
            "Epoch 3/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8999 - acc: 0.5200 - auc: 0.5784 - precision: 0.3004 - recall: 0.6709 - f1: 0.4105 - val_loss: 2.5109 - val_acc: 0.4984 - val_auc: 0.5889 - val_precision: 0.0380 - val_recall: 0.6566 - val_f1: 0.0704\n",
            "Epoch 4/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8505 - acc: 0.5268 - auc: 0.5875 - precision: 0.3047 - recall: 0.6747 - f1: 0.4151 - val_loss: 2.4695 - val_acc: 0.4978 - val_auc: 0.5916 - val_precision: 0.0382 - val_recall: 0.6611 - val_f1: 0.0708\n",
            "Epoch 5/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8126 - acc: 0.5311 - auc: 0.5881 - precision: 0.3069 - recall: 0.6735 - f1: 0.4171 - val_loss: 2.4370 - val_acc: 0.4996 - val_auc: 0.5922 - val_precision: 0.0383 - val_recall: 0.6596 - val_f1: 0.0709\n",
            "Epoch 6/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7815 - acc: 0.5337 - auc: 0.5917 - precision: 0.3080 - recall: 0.6715 - f1: 0.4178 - val_loss: 2.4098 - val_acc: 0.5031 - val_auc: 0.5943 - val_precision: 0.0384 - val_recall: 0.6581 - val_f1: 0.0712\n",
            "Epoch 7/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.7547 - acc: 0.5364 - auc: 0.5926 - precision: 0.3091 - recall: 0.6694 - f1: 0.4184 - val_loss: 2.3868 - val_acc: 0.5037 - val_auc: 0.5951 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 8/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7318 - acc: 0.5391 - auc: 0.5953 - precision: 0.3104 - recall: 0.6681 - f1: 0.4189 - val_loss: 2.3659 - val_acc: 0.5043 - val_auc: 0.5966 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 9/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7111 - acc: 0.5412 - auc: 0.5976 - precision: 0.3123 - recall: 0.6723 - f1: 0.4220 - val_loss: 2.3484 - val_acc: 0.5039 - val_auc: 0.5956 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 10/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.6938 - acc: 0.5414 - auc: 0.5974 - precision: 0.3122 - recall: 0.6708 - f1: 0.4212 - val_loss: 2.3335 - val_acc: 0.5033 - val_auc: 0.5971 - val_precision: 0.0388 - val_recall: 0.6642 - val_f1: 0.0719\n",
            "Epoch 11/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.6788 - acc: 0.5422 - auc: 0.5973 - precision: 0.3119 - recall: 0.6669 - f1: 0.4206 - val_loss: 2.3195 - val_acc: 0.5006 - val_auc: 0.5968 - val_precision: 0.0387 - val_recall: 0.6672 - val_f1: 0.0718\n",
            "Epoch 12/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.6629 - acc: 0.5439 - auc: 0.5996 - precision: 0.3139 - recall: 0.6722 - f1: 0.4228 - val_loss: 2.3056 - val_acc: 0.5050 - val_auc: 0.5961 - val_precision: 0.0389 - val_recall: 0.6642 - val_f1: 0.0720\n",
            "Epoch 13/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.6501 - acc: 0.5432 - auc: 0.5994 - precision: 0.3134 - recall: 0.6721 - f1: 0.4226 - val_loss: 2.2947 - val_acc: 0.4998 - val_auc: 0.5990 - val_precision: 0.0387 - val_recall: 0.6672 - val_f1: 0.0716\n",
            "Epoch 14/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6375 - acc: 0.5427 - auc: 0.5990 - precision: 0.3126 - recall: 0.6690 - f1: 0.4218 - val_loss: 2.2823 - val_acc: 0.5048 - val_auc: 0.5988 - val_precision: 0.0390 - val_recall: 0.6657 - val_f1: 0.0721\n",
            "Epoch 15/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6257 - acc: 0.5446 - auc: 0.6017 - precision: 0.3139 - recall: 0.6703 - f1: 0.4229 - val_loss: 2.2736 - val_acc: 0.5013 - val_auc: 0.5983 - val_precision: 0.0388 - val_recall: 0.6672 - val_f1: 0.0718\n",
            "Epoch 16/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6157 - acc: 0.5444 - auc: 0.6009 - precision: 0.3139 - recall: 0.6707 - f1: 0.4233 - val_loss: 2.2624 - val_acc: 0.5050 - val_auc: 0.5978 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0718\n",
            "Epoch 17/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6052 - acc: 0.5453 - auc: 0.6023 - precision: 0.3146 - recall: 0.6718 - f1: 0.4238 - val_loss: 2.2539 - val_acc: 0.5045 - val_auc: 0.5992 - val_precision: 0.0389 - val_recall: 0.6642 - val_f1: 0.0719\n",
            "Epoch 18/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5959 - acc: 0.5469 - auc: 0.6018 - precision: 0.3158 - recall: 0.6734 - f1: 0.4254 - val_loss: 2.2441 - val_acc: 0.5058 - val_auc: 0.5994 - val_precision: 0.0388 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 19/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5870 - acc: 0.5453 - auc: 0.6019 - precision: 0.3145 - recall: 0.6715 - f1: 0.4237 - val_loss: 2.2366 - val_acc: 0.5046 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 20/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5791 - acc: 0.5438 - auc: 0.6007 - precision: 0.3128 - recall: 0.6665 - f1: 0.4214 - val_loss: 2.2292 - val_acc: 0.5047 - val_auc: 0.5989 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 21/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5708 - acc: 0.5464 - auc: 0.6021 - precision: 0.3151 - recall: 0.6708 - f1: 0.4242 - val_loss: 2.2224 - val_acc: 0.5034 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6627 - val_f1: 0.0717\n",
            "Epoch 22/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5631 - acc: 0.5469 - auc: 0.6032 - precision: 0.3156 - recall: 0.6725 - f1: 0.4248 - val_loss: 2.2150 - val_acc: 0.5050 - val_auc: 0.5972 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 23/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5552 - acc: 0.5472 - auc: 0.6048 - precision: 0.3158 - recall: 0.6725 - f1: 0.4249 - val_loss: 2.2089 - val_acc: 0.5021 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6642 - val_f1: 0.0717\n",
            "Epoch 24/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5490 - acc: 0.5472 - auc: 0.6034 - precision: 0.3155 - recall: 0.6709 - f1: 0.4240 - val_loss: 2.2020 - val_acc: 0.5036 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6627 - val_f1: 0.0717\n",
            "Epoch 25/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5426 - acc: 0.5455 - auc: 0.6030 - precision: 0.3141 - recall: 0.6684 - f1: 0.4232 - val_loss: 2.1952 - val_acc: 0.5049 - val_auc: 0.5994 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 26/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5357 - acc: 0.5475 - auc: 0.6043 - precision: 0.3156 - recall: 0.6702 - f1: 0.4244 - val_loss: 2.1914 - val_acc: 0.5033 - val_auc: 0.5981 - val_precision: 0.0386 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 27/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5304 - acc: 0.5476 - auc: 0.6038 - precision: 0.3158 - recall: 0.6711 - f1: 0.4253 - val_loss: 2.1840 - val_acc: 0.5061 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0717\n",
            "Epoch 28/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5237 - acc: 0.5484 - auc: 0.6047 - precision: 0.3167 - recall: 0.6734 - f1: 0.4261 - val_loss: 2.1786 - val_acc: 0.5062 - val_auc: 0.5985 - val_precision: 0.0388 - val_recall: 0.6611 - val_f1: 0.0720\n",
            "Epoch 29/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5188 - acc: 0.5477 - auc: 0.6039 - precision: 0.3156 - recall: 0.6692 - f1: 0.4244 - val_loss: 2.1745 - val_acc: 0.5029 - val_auc: 0.5985 - val_precision: 0.0388 - val_recall: 0.6642 - val_f1: 0.0718\n",
            "Epoch 30/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5120 - acc: 0.5487 - auc: 0.6074 - precision: 0.3166 - recall: 0.6718 - f1: 0.4255 - val_loss: 2.1685 - val_acc: 0.5071 - val_auc: 0.5989 - val_precision: 0.0390 - val_recall: 0.6627 - val_f1: 0.0722\n",
            "Epoch 31/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5078 - acc: 0.5489 - auc: 0.6036 - precision: 0.3170 - recall: 0.6737 - f1: 0.4260 - val_loss: 2.1644 - val_acc: 0.5045 - val_auc: 0.5989 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 32/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5029 - acc: 0.5491 - auc: 0.6057 - precision: 0.3173 - recall: 0.6746 - f1: 0.4273 - val_loss: 2.1593 - val_acc: 0.5053 - val_auc: 0.5997 - val_precision: 0.0387 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 33/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4981 - acc: 0.5489 - auc: 0.6055 - precision: 0.3170 - recall: 0.6736 - f1: 0.4265 - val_loss: 2.1542 - val_acc: 0.5057 - val_auc: 0.5987 - val_precision: 0.0387 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 34/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4939 - acc: 0.5478 - auc: 0.6048 - precision: 0.3157 - recall: 0.6694 - f1: 0.4244 - val_loss: 2.1500 - val_acc: 0.5077 - val_auc: 0.5977 - val_precision: 0.0387 - val_recall: 0.6566 - val_f1: 0.0716\n",
            "Epoch 35/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4883 - acc: 0.5487 - auc: 0.6062 - precision: 0.3164 - recall: 0.6710 - f1: 0.4253 - val_loss: 2.1463 - val_acc: 0.5056 - val_auc: 0.5984 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 36/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4845 - acc: 0.5481 - auc: 0.6050 - precision: 0.3163 - recall: 0.6725 - f1: 0.4256 - val_loss: 2.1408 - val_acc: 0.5070 - val_auc: 0.5992 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 37/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4798 - acc: 0.5494 - auc: 0.6074 - precision: 0.3170 - recall: 0.6718 - f1: 0.4259 - val_loss: 2.1373 - val_acc: 0.5065 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 38/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4754 - acc: 0.5487 - auc: 0.6066 - precision: 0.3168 - recall: 0.6731 - f1: 0.4263 - val_loss: 2.1364 - val_acc: 0.5013 - val_auc: 0.6000 - val_precision: 0.0386 - val_recall: 0.6642 - val_f1: 0.0716\n",
            "Epoch 39/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4718 - acc: 0.5481 - auc: 0.6069 - precision: 0.3164 - recall: 0.6725 - f1: 0.4253 - val_loss: 2.1301 - val_acc: 0.5063 - val_auc: 0.5985 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 40/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4676 - acc: 0.5497 - auc: 0.6068 - precision: 0.3175 - recall: 0.6736 - f1: 0.4269 - val_loss: 2.1274 - val_acc: 0.5056 - val_auc: 0.5993 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 41/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4635 - acc: 0.5487 - auc: 0.6076 - precision: 0.3171 - recall: 0.6748 - f1: 0.4271 - val_loss: 2.1234 - val_acc: 0.5046 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 42/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4605 - acc: 0.5485 - auc: 0.6062 - precision: 0.3170 - recall: 0.6751 - f1: 0.4266 - val_loss: 2.1193 - val_acc: 0.5066 - val_auc: 0.5990 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 43/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4579 - acc: 0.5500 - auc: 0.6052 - precision: 0.3176 - recall: 0.6734 - f1: 0.4270 - val_loss: 2.1146 - val_acc: 0.5079 - val_auc: 0.5992 - val_precision: 0.0387 - val_recall: 0.6566 - val_f1: 0.0717\n",
            "Epoch 44/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4532 - acc: 0.5492 - auc: 0.6064 - precision: 0.3171 - recall: 0.6731 - f1: 0.4266 - val_loss: 2.1132 - val_acc: 0.5061 - val_auc: 0.5984 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 45/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4501 - acc: 0.5502 - auc: 0.6057 - precision: 0.3179 - recall: 0.6739 - f1: 0.4274 - val_loss: 2.1104 - val_acc: 0.5051 - val_auc: 0.5989 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 46/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4473 - acc: 0.5484 - auc: 0.6049 - precision: 0.3166 - recall: 0.6729 - f1: 0.4262 - val_loss: 2.1082 - val_acc: 0.5042 - val_auc: 0.5982 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 47/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4430 - acc: 0.5492 - auc: 0.6066 - precision: 0.3175 - recall: 0.6756 - f1: 0.4268 - val_loss: 2.1033 - val_acc: 0.5066 - val_auc: 0.5982 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0717\n",
            "Epoch 48/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4403 - acc: 0.5485 - auc: 0.6056 - precision: 0.3165 - recall: 0.6722 - f1: 0.4260 - val_loss: 2.1015 - val_acc: 0.5052 - val_auc: 0.5981 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 49/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4368 - acc: 0.5492 - auc: 0.6066 - precision: 0.3174 - recall: 0.6749 - f1: 0.4268 - val_loss: 2.0979 - val_acc: 0.5053 - val_auc: 0.5984 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 50/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.4336 - acc: 0.5496 - auc: 0.6070 - precision: 0.3174 - recall: 0.6735 - f1: 0.4264 - val_loss: 2.0964 - val_acc: 0.5036 - val_auc: 0.5982 - val_precision: 0.0386 - val_recall: 0.6596 - val_f1: 0.0715\n",
            "Epoch 51/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.4309 - acc: 0.5483 - auc: 0.6063 - precision: 0.3168 - recall: 0.6743 - f1: 0.4262 - val_loss: 2.0931 - val_acc: 0.5047 - val_auc: 0.5977 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 52/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4273 - acc: 0.5500 - auc: 0.6073 - precision: 0.3182 - recall: 0.6770 - f1: 0.4284 - val_loss: 2.0884 - val_acc: 0.5075 - val_auc: 0.5981 - val_precision: 0.0389 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 53/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4250 - acc: 0.5494 - auc: 0.6062 - precision: 0.3173 - recall: 0.6737 - f1: 0.4268 - val_loss: 2.0853 - val_acc: 0.5082 - val_auc: 0.5977 - val_precision: 0.0389 - val_recall: 0.6596 - val_f1: 0.0720\n",
            "Epoch 54/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4215 - acc: 0.5489 - auc: 0.6083 - precision: 0.3175 - recall: 0.6763 - f1: 0.4275 - val_loss: 2.0833 - val_acc: 0.5071 - val_auc: 0.5988 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 55/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4194 - acc: 0.5487 - auc: 0.6065 - precision: 0.3173 - recall: 0.6761 - f1: 0.4275 - val_loss: 2.0820 - val_acc: 0.5042 - val_auc: 0.5985 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 56/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4172 - acc: 0.5490 - auc: 0.6052 - precision: 0.3174 - recall: 0.6755 - f1: 0.4272 - val_loss: 2.0792 - val_acc: 0.5049 - val_auc: 0.5992 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 57/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4137 - acc: 0.5482 - auc: 0.6071 - precision: 0.3170 - recall: 0.6758 - f1: 0.4274 - val_loss: 2.0761 - val_acc: 0.5045 - val_auc: 0.5991 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 58/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4115 - acc: 0.5498 - auc: 0.6053 - precision: 0.3175 - recall: 0.6737 - f1: 0.4268 - val_loss: 2.0745 - val_acc: 0.5045 - val_auc: 0.5986 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 59/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4081 - acc: 0.5495 - auc: 0.6073 - precision: 0.3178 - recall: 0.6762 - f1: 0.4275 - val_loss: 2.0712 - val_acc: 0.5064 - val_auc: 0.5983 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0718\n",
            "Epoch 60/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4061 - acc: 0.5495 - auc: 0.6078 - precision: 0.3178 - recall: 0.6763 - f1: 0.4281 - val_loss: 2.0705 - val_acc: 0.5023 - val_auc: 0.5987 - val_precision: 0.0387 - val_recall: 0.6642 - val_f1: 0.0717\n",
            "1170/1170 [==============================] - 1s 993us/step - loss: 2.0690 - acc: 0.5054 - auc: 0.6128 - precision: 0.0401 - recall: 0.6863 - f1: 0.0717\n",
            "2023-08-19 02:32:23.350497\n",
            "CPU times: user 15min 32s, sys: 2min 57s, total: 18min 30s\n",
            "Wall time: 8min 23s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "# run model\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=64, epochs=500,\n",
        "           loss=wbce_custom(3.5), optimizer=Adam(learning_rate=0.00001, decay=0.001), dropout=0.1, patience=30,\n",
        "           existing_model = None, metrics=['f1'], train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ2WRerRdX5E"
      },
      "source": [
        "##### With Cross Validation (Takes up too much RAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Qp17GxM30k"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "\n",
        "# model_types = [\"convolution\"]\n",
        "# num_features = [3]\n",
        "# i = 0\n",
        "\n",
        "# for model_type, num_feature in itertools.product(model_types, num_features):\n",
        "\n",
        "#     for j in range(5):\n",
        "#       # Getting Input Data\n",
        "#       if num_feature == 1:\n",
        "#           input_data_ = sites_data[['CHL', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "#       elif num_feature == 3:\n",
        "#           input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "#       # Getting xy_data\n",
        "#       xy_data = get_train_test_val_nn(input_data_,\n",
        "#                             train_test_dict[f'train_{j+1}'],\n",
        "#                             train_test_dict[f'test_{j+1}'])\n",
        "\n",
        "#       # Get history and result\n",
        "#       model_, history, result = fit_nn(xy_data, model_type, batch_size=32, epochs=3, loss=wbce_custom(50), optimizer=Adam(learning_rate=0.0005))\n",
        "#       model_list.append(model_)\n",
        "#       histories.append(history)\n",
        "#       results.append(result)\n",
        "\n",
        "#       i += 1\n",
        "#       clear_output(wait=True)\n",
        "\n",
        "#       K.clear_session()\n",
        "#       tf.compat.v1.reset_default_graph()\n",
        "#       gc.collect()\n",
        "\n",
        "#       print(f'Progress: {i}/{len(model_types)*len(num_features)*5}')\n",
        "#       print(datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6Q610dder6"
      },
      "source": [
        "##### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Wr3Fjwdr4l",
        "outputId": "82613b31-6680-4cba-f903-ed648697e14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Name: nn_15x15_7_23_2222\n"
          ]
        }
      ],
      "source": [
        "model_notes = '''dataset: L3 1km x 1km 15x15, fillna = mean then 0, num_feature=6, model_type=\"convolution v3 (lenet)\", batch_size=64, epochs=98,\n",
        "           loss=wbce_custom(30), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = None, metrics=[\"f1\"]. added weight decay l2 regularisation. added batch norm. use kernel_size = 3 instead of 5.\n",
        "           early stopping patience = 30. '''\n",
        "save_model(model, history, result, model_notes, model_specs = '15x15')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvuJxT6xz2u2"
      },
      "source": [
        "##### (Attempt to) Clear RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxeFhWesldyu"
      },
      "outputs": [],
      "source": [
        "# del model\n",
        "# del history\n",
        "# del result\n",
        "# K.clear_session()\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrurCkNFrJ3-"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# def sizeof_fmt(num, suffix='B'):\n",
        "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "#         if abs(num) < 1024.0:\n",
        "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "#         num /= 1024.0\n",
        "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "#                          key= lambda x: -x[1])[:10]:\n",
        "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MflnfHQXCz"
      },
      "source": [
        "#### Training From Existing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbDjSSIaoPmH"
      },
      "source": [
        "##### Load Model From File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paz-QwSVn2o_",
        "outputId": "c1b14e6e-8c01-497a-d20f-e4356b527b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: L3 1km x 1km 15x15, fillna = mean then 0, num_feature=6, model_type=\"convolution v3 (lenet)\", batch_size=64, epochs=200,\n",
            "           loss=wbce_custom(40), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
            "           existing_model = None, metrics=[\"f1\"]. added weight decay l2 regularisation. added batch norm. removed time and site pairs with >8000\n"
          ]
        }
      ],
      "source": [
        "old_model, old_history = load_model('nn_15x15_7_20_1323', loss_weight=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um8esTkeoRAe"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCXBh1-BPFG",
        "outputId": "7b713ac3-8871-41a6-b9d9-c9d1e6d0412e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-20 13:41:36.949359\n",
            "X_train shape: (6052, 15, 15, 6)\n",
            "{'name': 'Adam', 'learning_rate': 5e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_2', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_4_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_4', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_4', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_4', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_5', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_5', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_5', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_6', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "Epoch 1/200\n",
            "95/95 [==============================] - 3s 21ms/step - loss: 1.4283 - acc: 0.6281 - auc: 0.8495 - precision: 0.0959 - recall: 0.8973 - f1: 0.1693 - val_loss: 2.4292 - val_acc: 0.6440 - val_auc: 0.5914 - val_precision: 0.0645 - val_recall: 0.5303 - val_f1: 0.1084\n",
            "Epoch 2/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.4484 - acc: 0.6112 - auc: 0.8412 - precision: 0.0940 - recall: 0.9202 - f1: 0.1620 - val_loss: 2.3875 - val_acc: 0.5416 - val_auc: 0.5840 - val_precision: 0.0540 - val_recall: 0.5758 - val_f1: 0.0951\n",
            "Epoch 3/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.4310 - acc: 0.5905 - auc: 0.8460 - precision: 0.0900 - recall: 0.9240 - f1: 0.1594 - val_loss: 2.3954 - val_acc: 0.6176 - val_auc: 0.5911 - val_precision: 0.0645 - val_recall: 0.5758 - val_f1: 0.1131\n",
            "Epoch 4/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4084 - acc: 0.6279 - auc: 0.8539 - precision: 0.0975 - recall: 0.9163 - f1: 0.1747 - val_loss: 2.4252 - val_acc: 0.6004 - val_auc: 0.5918 - val_precision: 0.0604 - val_recall: 0.5606 - val_f1: 0.1053\n",
            "Epoch 5/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4651 - acc: 0.6059 - auc: 0.8340 - precision: 0.0922 - recall: 0.9125 - f1: 0.1631 - val_loss: 2.4361 - val_acc: 0.5760 - val_auc: 0.5865 - val_precision: 0.0556 - val_recall: 0.5455 - val_f1: 0.0972\n",
            "Epoch 6/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4218 - acc: 0.6006 - auc: 0.8470 - precision: 0.0914 - recall: 0.9163 - f1: 0.1608 - val_loss: 2.4751 - val_acc: 0.6724 - val_auc: 0.5914 - val_precision: 0.0630 - val_recall: 0.4697 - val_f1: 0.1054\n",
            "Epoch 7/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4087 - acc: 0.6015 - auc: 0.8510 - precision: 0.0919 - recall: 0.9202 - f1: 0.1625 - val_loss: 2.5540 - val_acc: 0.6757 - val_auc: 0.5818 - val_precision: 0.0582 - val_recall: 0.4242 - val_f1: 0.0961\n",
            "Epoch 8/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4498 - acc: 0.6138 - auc: 0.8400 - precision: 0.0933 - recall: 0.9049 - f1: 0.1657 - val_loss: 2.4363 - val_acc: 0.5687 - val_auc: 0.5816 - val_precision: 0.0560 - val_recall: 0.5606 - val_f1: 0.0994\n",
            "Epoch 9/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4544 - acc: 0.6140 - auc: 0.8395 - precision: 0.0914 - recall: 0.8821 - f1: 0.1629 - val_loss: 2.4466 - val_acc: 0.6050 - val_auc: 0.5812 - val_precision: 0.0596 - val_recall: 0.5455 - val_f1: 0.1053\n",
            "Epoch 10/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4195 - acc: 0.6152 - auc: 0.8506 - precision: 0.0943 - recall: 0.9125 - f1: 0.1655 - val_loss: 2.4895 - val_acc: 0.6229 - val_auc: 0.5823 - val_precision: 0.0593 - val_recall: 0.5152 - val_f1: 0.1030\n",
            "Epoch 11/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4130 - acc: 0.6310 - auc: 0.8507 - precision: 0.0986 - recall: 0.9202 - f1: 0.1721 - val_loss: 2.4521 - val_acc: 0.6255 - val_auc: 0.5833 - val_precision: 0.0643 - val_recall: 0.5606 - val_f1: 0.1126\n",
            "Epoch 12/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4381 - acc: 0.6423 - auc: 0.8478 - precision: 0.0994 - recall: 0.8973 - f1: 0.1768 - val_loss: 2.4726 - val_acc: 0.5363 - val_auc: 0.5770 - val_precision: 0.0521 - val_recall: 0.5606 - val_f1: 0.0920\n",
            "Epoch 13/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4365 - acc: 0.6147 - auc: 0.8453 - precision: 0.0932 - recall: 0.9011 - f1: 0.1659 - val_loss: 2.4458 - val_acc: 0.6196 - val_auc: 0.5899 - val_precision: 0.0603 - val_recall: 0.5303 - val_f1: 0.1079\n",
            "Epoch 14/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4508 - acc: 0.6274 - auc: 0.8403 - precision: 0.0968 - recall: 0.9087 - f1: 0.1711 - val_loss: 2.4734 - val_acc: 0.5476 - val_auc: 0.5771 - val_precision: 0.0547 - val_recall: 0.5758 - val_f1: 0.0969\n",
            "Epoch 15/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3991 - acc: 0.6254 - auc: 0.8547 - precision: 0.0969 - recall: 0.9163 - f1: 0.1698 - val_loss: 2.4979 - val_acc: 0.6222 - val_auc: 0.5826 - val_precision: 0.0592 - val_recall: 0.5152 - val_f1: 0.1049\n",
            "Epoch 16/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4242 - acc: 0.6256 - auc: 0.8438 - precision: 0.0973 - recall: 0.9202 - f1: 0.1740 - val_loss: 2.5476 - val_acc: 0.6044 - val_auc: 0.5800 - val_precision: 0.0566 - val_recall: 0.5152 - val_f1: 0.0980\n",
            "Epoch 17/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3655 - acc: 0.6173 - auc: 0.8666 - precision: 0.0967 - recall: 0.9354 - f1: 0.1703 - val_loss: 2.4236 - val_acc: 0.5746 - val_auc: 0.5895 - val_precision: 0.0554 - val_recall: 0.5455 - val_f1: 0.0979\n",
            "Epoch 18/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3886 - acc: 0.6371 - auc: 0.8584 - precision: 0.1005 - recall: 0.9240 - f1: 0.1753 - val_loss: 2.4677 - val_acc: 0.5211 - val_auc: 0.5797 - val_precision: 0.0529 - val_recall: 0.5909 - val_f1: 0.0939\n",
            "Epoch 19/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4079 - acc: 0.6074 - auc: 0.8533 - precision: 0.0926 - recall: 0.9125 - f1: 0.1640 - val_loss: 2.5111 - val_acc: 0.5548 - val_auc: 0.5752 - val_precision: 0.0529 - val_recall: 0.5455 - val_f1: 0.0939\n",
            "Epoch 20/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3836 - acc: 0.6373 - auc: 0.8611 - precision: 0.1005 - recall: 0.9240 - f1: 0.1768 - val_loss: 2.5431 - val_acc: 0.6050 - val_auc: 0.5844 - val_precision: 0.0596 - val_recall: 0.5455 - val_f1: 0.1065\n",
            "Epoch 21/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4135 - acc: 0.6165 - auc: 0.8491 - precision: 0.0952 - recall: 0.9202 - f1: 0.1703 - val_loss: 2.5034 - val_acc: 0.5892 - val_auc: 0.5844 - val_precision: 0.0559 - val_recall: 0.5303 - val_f1: 0.0982\n",
            "Epoch 22/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4140 - acc: 0.6319 - auc: 0.8492 - precision: 0.0985 - recall: 0.9163 - f1: 0.1729 - val_loss: 2.4616 - val_acc: 0.5277 - val_auc: 0.5839 - val_precision: 0.0524 - val_recall: 0.5758 - val_f1: 0.0936\n",
            "Epoch 23/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3787 - acc: 0.6191 - auc: 0.8614 - precision: 0.0971 - recall: 0.9354 - f1: 0.1714 - val_loss: 2.5500 - val_acc: 0.6387 - val_auc: 0.5824 - val_precision: 0.0619 - val_recall: 0.5152 - val_f1: 0.1094\n",
            "Epoch 24/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3977 - acc: 0.6385 - auc: 0.8573 - precision: 0.0998 - recall: 0.9125 - f1: 0.1749 - val_loss: 2.5209 - val_acc: 0.5594 - val_auc: 0.5812 - val_precision: 0.0548 - val_recall: 0.5606 - val_f1: 0.0974\n",
            "Epoch 25/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4051 - acc: 0.6267 - auc: 0.8517 - precision: 0.0973 - recall: 0.9163 - f1: 0.1726 - val_loss: 2.5558 - val_acc: 0.6816 - val_auc: 0.5890 - val_precision: 0.0667 - val_recall: 0.4848 - val_f1: 0.1138\n",
            "Epoch 26/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3446 - acc: 0.6550 - auc: 0.8689 - precision: 0.1058 - recall: 0.9316 - f1: 0.1847 - val_loss: 2.6724 - val_acc: 0.6962 - val_auc: 0.5816 - val_precision: 0.0603 - val_recall: 0.4091 - val_f1: 0.0964\n",
            "Epoch 27/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3998 - acc: 0.6421 - auc: 0.8552 - precision: 0.1000 - recall: 0.9049 - f1: 0.1756 - val_loss: 2.5955 - val_acc: 0.5694 - val_auc: 0.5738 - val_precision: 0.0506 - val_recall: 0.5000 - val_f1: 0.0867\n",
            "Epoch 28/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3811 - acc: 0.6246 - auc: 0.8582 - precision: 0.0987 - recall: 0.9392 - f1: 0.1772 - val_loss: 2.5551 - val_acc: 0.5799 - val_auc: 0.5786 - val_precision: 0.0533 - val_recall: 0.5152 - val_f1: 0.0931\n",
            "Epoch 29/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3695 - acc: 0.6545 - auc: 0.8608 - precision: 0.1050 - recall: 0.9240 - f1: 0.1839 - val_loss: 2.4970 - val_acc: 0.5238 - val_auc: 0.5757 - val_precision: 0.0520 - val_recall: 0.5758 - val_f1: 0.0931\n",
            "Epoch 30/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3589 - acc: 0.6404 - auc: 0.8662 - precision: 0.1020 - recall: 0.9316 - f1: 0.1818 - val_loss: 2.5298 - val_acc: 0.5865 - val_auc: 0.5808 - val_precision: 0.0541 - val_recall: 0.5152 - val_f1: 0.0955\n",
            "Epoch 31/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3724 - acc: 0.6328 - auc: 0.8628 - precision: 0.1007 - recall: 0.9392 - f1: 0.1773 - val_loss: 2.5811 - val_acc: 0.6328 - val_auc: 0.5801 - val_precision: 0.0594 - val_recall: 0.5000 - val_f1: 0.1039\n",
            "Epoch 32/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3535 - acc: 0.6548 - auc: 0.8658 - precision: 0.1041 - recall: 0.9125 - f1: 0.1805 - val_loss: 2.5402 - val_acc: 0.5416 - val_auc: 0.5765 - val_precision: 0.0514 - val_recall: 0.5455 - val_f1: 0.0908\n",
            "Epoch 33/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3147 - acc: 0.6507 - auc: 0.8782 - precision: 0.1040 - recall: 0.9240 - f1: 0.1839 - val_loss: 2.5473 - val_acc: 0.5462 - val_auc: 0.5790 - val_precision: 0.0519 - val_recall: 0.5455 - val_f1: 0.0923\n",
            "Epoch 34/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.3746 - acc: 0.6492 - auc: 0.8627 - precision: 0.1029 - recall: 0.9163 - f1: 0.1806 - val_loss: 2.7018 - val_acc: 0.5984 - val_auc: 0.5723 - val_precision: 0.0557 - val_recall: 0.5152 - val_f1: 0.0942\n",
            "Epoch 35/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.3630 - acc: 0.6406 - auc: 0.8656 - precision: 0.1013 - recall: 0.9240 - f1: 0.1789 - val_loss: 2.6780 - val_acc: 0.5964 - val_auc: 0.5733 - val_precision: 0.0525 - val_recall: 0.4848 - val_f1: 0.0909\n",
            "Epoch 36/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3670 - acc: 0.6302 - auc: 0.8659 - precision: 0.0987 - recall: 0.9240 - f1: 0.1752 - val_loss: 2.5538 - val_acc: 0.6176 - val_auc: 0.5845 - val_precision: 0.0585 - val_recall: 0.5152 - val_f1: 0.1023\n",
            "Epoch 37/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3623 - acc: 0.6434 - auc: 0.8653 - precision: 0.1014 - recall: 0.9163 - f1: 0.1797 - val_loss: 2.5896 - val_acc: 0.5707 - val_auc: 0.5732 - val_precision: 0.0508 - val_recall: 0.5000 - val_f1: 0.0896\n",
            "Epoch 38/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3833 - acc: 0.6461 - auc: 0.8604 - precision: 0.1034 - recall: 0.9316 - f1: 0.1833 - val_loss: 2.7649 - val_acc: 0.6777 - val_auc: 0.5739 - val_precision: 0.0586 - val_recall: 0.4242 - val_f1: 0.0948\n",
            "Epoch 39/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3925 - acc: 0.6466 - auc: 0.8578 - precision: 0.1019 - recall: 0.9125 - f1: 0.1763 - val_loss: 2.5852 - val_acc: 0.5984 - val_auc: 0.5820 - val_precision: 0.0572 - val_recall: 0.5303 - val_f1: 0.1011\n",
            "Epoch 40/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3557 - acc: 0.6431 - auc: 0.8669 - precision: 0.1020 - recall: 0.9240 - f1: 0.1808 - val_loss: 2.6771 - val_acc: 0.6156 - val_auc: 0.5745 - val_precision: 0.0521 - val_recall: 0.4545 - val_f1: 0.0871\n",
            "Epoch 41/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3493 - acc: 0.6545 - auc: 0.8683 - precision: 0.1036 - recall: 0.9087 - f1: 0.1794 - val_loss: 2.6053 - val_acc: 0.5436 - val_auc: 0.5740 - val_precision: 0.0516 - val_recall: 0.5455 - val_f1: 0.0912\n",
            "Epoch 42/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3827 - acc: 0.6233 - auc: 0.8587 - precision: 0.0961 - recall: 0.9125 - f1: 0.1668 - val_loss: 2.6816 - val_acc: 0.6347 - val_auc: 0.5744 - val_precision: 0.0548 - val_recall: 0.4545 - val_f1: 0.0926\n",
            "Epoch 43/200\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 1.3594 - acc: 0.6391 - auc: 0.8652 - precision: 0.0990 - recall: 0.9011 - f1: 0.1743 - val_loss: 2.7020 - val_acc: 0.6585 - val_auc: 0.5747 - val_precision: 0.0621 - val_recall: 0.4848 - val_f1: 0.1056\n",
            "Epoch 44/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3179 - acc: 0.6745 - auc: 0.8776 - precision: 0.1108 - recall: 0.9240 - f1: 0.1935 - val_loss: 2.7295 - val_acc: 0.6162 - val_auc: 0.5725 - val_precision: 0.0553 - val_recall: 0.4848 - val_f1: 0.0933\n",
            "Epoch 45/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3413 - acc: 0.6672 - auc: 0.8704 - precision: 0.1090 - recall: 0.9278 - f1: 0.1881 - val_loss: 2.7156 - val_acc: 0.5799 - val_auc: 0.5686 - val_precision: 0.0519 - val_recall: 0.5000 - val_f1: 0.0915\n",
            "Epoch 46/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3444 - acc: 0.6537 - auc: 0.8703 - precision: 0.1034 - recall: 0.9087 - f1: 0.1806 - val_loss: 2.7931 - val_acc: 0.6664 - val_auc: 0.5762 - val_precision: 0.0601 - val_recall: 0.4545 - val_f1: 0.0993\n",
            "Epoch 47/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3363 - acc: 0.6487 - auc: 0.8725 - precision: 0.1051 - recall: 0.9430 - f1: 0.1852 - val_loss: 2.7092 - val_acc: 0.6162 - val_auc: 0.5735 - val_precision: 0.0583 - val_recall: 0.5152 - val_f1: 0.1019\n",
            "Epoch 48/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3467 - acc: 0.6591 - auc: 0.8661 - precision: 0.1073 - recall: 0.9354 - f1: 0.1895 - val_loss: 2.7326 - val_acc: 0.5594 - val_auc: 0.5662 - val_precision: 0.0481 - val_recall: 0.4848 - val_f1: 0.0820\n",
            "Epoch 49/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3444 - acc: 0.6525 - auc: 0.8709 - precision: 0.1052 - recall: 0.9316 - f1: 0.1853 - val_loss: 2.7674 - val_acc: 0.6664 - val_auc: 0.5717 - val_precision: 0.0619 - val_recall: 0.4697 - val_f1: 0.1058\n",
            "Epoch 50/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3358 - acc: 0.6664 - auc: 0.8722 - precision: 0.1084 - recall: 0.9240 - f1: 0.1916 - val_loss: 2.7567 - val_acc: 0.5812 - val_auc: 0.5681 - val_precision: 0.0506 - val_recall: 0.4848 - val_f1: 0.0865\n",
            "Epoch 51/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3210 - acc: 0.6543 - auc: 0.8734 - precision: 0.1039 - recall: 0.9125 - f1: 0.1811 - val_loss: 2.6520 - val_acc: 0.5760 - val_auc: 0.5757 - val_precision: 0.0542 - val_recall: 0.5303 - val_f1: 0.0958\n",
            "Epoch 52/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3379 - acc: 0.6690 - auc: 0.8727 - precision: 0.1081 - recall: 0.9125 - f1: 0.1902 - val_loss: 2.8116 - val_acc: 0.6876 - val_auc: 0.5740 - val_precision: 0.0605 - val_recall: 0.4242 - val_f1: 0.1030\n",
            "Epoch 53/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3080 - acc: 0.6647 - auc: 0.8788 - precision: 0.1089 - recall: 0.9354 - f1: 0.1931 - val_loss: 2.7711 - val_acc: 0.5819 - val_auc: 0.5671 - val_precision: 0.0521 - val_recall: 0.5000 - val_f1: 0.0905\n",
            "Epoch 54/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.2857 - acc: 0.6609 - auc: 0.8867 - precision: 0.1078 - recall: 0.9354 - f1: 0.1886 - val_loss: 2.7937 - val_acc: 0.6229 - val_auc: 0.5704 - val_precision: 0.0547 - val_recall: 0.4697 - val_f1: 0.0942\n",
            "Epoch 55/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3027 - acc: 0.6644 - auc: 0.8804 - precision: 0.1095 - recall: 0.9430 - f1: 0.1921 - val_loss: 2.8557 - val_acc: 0.6896 - val_auc: 0.5738 - val_precision: 0.0628 - val_recall: 0.4394 - val_f1: 0.1021\n",
            "Epoch 56/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2868 - acc: 0.6861 - auc: 0.8864 - precision: 0.1137 - recall: 0.9163 - f1: 0.1966 - val_loss: 2.8026 - val_acc: 0.5918 - val_auc: 0.5676 - val_precision: 0.0548 - val_recall: 0.5152 - val_f1: 0.0955\n",
            "Epoch 57/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2937 - acc: 0.6519 - auc: 0.8848 - precision: 0.1060 - recall: 0.9430 - f1: 0.1846 - val_loss: 2.8333 - val_acc: 0.6004 - val_auc: 0.5676 - val_precision: 0.0560 - val_recall: 0.5152 - val_f1: 0.0988\n",
            "Epoch 58/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2924 - acc: 0.6895 - auc: 0.8835 - precision: 0.1145 - recall: 0.9125 - f1: 0.1966 - val_loss: 2.8858 - val_acc: 0.6572 - val_auc: 0.5689 - val_precision: 0.0568 - val_recall: 0.4394 - val_f1: 0.0945\n",
            "Epoch 59/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3145 - acc: 0.6745 - auc: 0.8757 - precision: 0.1090 - recall: 0.9049 - f1: 0.1904 - val_loss: 2.7755 - val_acc: 0.6480 - val_auc: 0.5742 - val_precision: 0.0586 - val_recall: 0.4697 - val_f1: 0.1017\n",
            "Epoch 60/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3000 - acc: 0.6680 - auc: 0.8827 - precision: 0.1092 - recall: 0.9278 - f1: 0.1904 - val_loss: 2.8239 - val_acc: 0.6328 - val_auc: 0.5680 - val_precision: 0.0594 - val_recall: 0.5000 - val_f1: 0.1041\n",
            "Epoch 61/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3090 - acc: 0.6889 - auc: 0.8809 - precision: 0.1143 - recall: 0.9125 - f1: 0.1962 - val_loss: 2.8610 - val_acc: 0.6532 - val_auc: 0.5699 - val_precision: 0.0578 - val_recall: 0.4545 - val_f1: 0.0953\n",
            "Epoch 62/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3180 - acc: 0.6689 - auc: 0.8758 - precision: 0.1088 - recall: 0.9202 - f1: 0.1891 - val_loss: 2.7414 - val_acc: 0.5456 - val_auc: 0.5640 - val_precision: 0.0506 - val_recall: 0.5303 - val_f1: 0.0894\n",
            "Epoch 63/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2917 - acc: 0.6776 - auc: 0.8819 - precision: 0.1128 - recall: 0.9354 - f1: 0.1977 - val_loss: 2.8922 - val_acc: 0.6612 - val_auc: 0.5713 - val_precision: 0.0592 - val_recall: 0.4545 - val_f1: 0.0982\n",
            "Epoch 64/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2855 - acc: 0.6786 - auc: 0.8841 - precision: 0.1132 - recall: 0.9354 - f1: 0.1952 - val_loss: 2.8740 - val_acc: 0.6268 - val_auc: 0.5672 - val_precision: 0.0553 - val_recall: 0.4697 - val_f1: 0.0958\n",
            "Epoch 65/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3131 - acc: 0.6970 - auc: 0.8785 - precision: 0.1174 - recall: 0.9163 - f1: 0.2042 - val_loss: 2.8729 - val_acc: 0.5945 - val_auc: 0.5654 - val_precision: 0.0493 - val_recall: 0.4545 - val_f1: 0.0837\n",
            "Epoch 66/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3235 - acc: 0.6674 - auc: 0.8759 - precision: 0.1083 - recall: 0.9202 - f1: 0.1856 - val_loss: 2.9261 - val_acc: 0.6638 - val_auc: 0.5652 - val_precision: 0.0579 - val_recall: 0.4394 - val_f1: 0.0963\n",
            "Epoch 67/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2924 - acc: 0.6791 - auc: 0.8820 - precision: 0.1119 - recall: 0.9202 - f1: 0.1982 - val_loss: 2.8731 - val_acc: 0.5641 - val_auc: 0.5603 - val_precision: 0.0486 - val_recall: 0.4848 - val_f1: 0.0828\n",
            "Epoch 68/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3237 - acc: 0.6646 - auc: 0.8734 - precision: 0.1079 - recall: 0.9240 - f1: 0.1887 - val_loss: 2.9056 - val_acc: 0.6684 - val_auc: 0.5698 - val_precision: 0.0587 - val_recall: 0.4394 - val_f1: 0.0974\n",
            "Epoch 69/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3076 - acc: 0.6884 - auc: 0.8802 - precision: 0.1163 - recall: 0.9354 - f1: 0.2028 - val_loss: 2.9241 - val_acc: 0.5997 - val_auc: 0.5642 - val_precision: 0.0515 - val_recall: 0.4697 - val_f1: 0.0865\n",
            "Epoch 70/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2785 - acc: 0.6512 - auc: 0.8824 - precision: 0.1065 - recall: 0.9506 - f1: 0.1866 - val_loss: 2.8758 - val_acc: 0.6446 - val_auc: 0.5715 - val_precision: 0.0581 - val_recall: 0.4697 - val_f1: 0.1005\n",
            "Epoch 71/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2779 - acc: 0.6951 - auc: 0.8876 - precision: 0.1190 - recall: 0.9392 - f1: 0.2028 - val_loss: 2.9202 - val_acc: 0.5911 - val_auc: 0.5645 - val_precision: 0.0504 - val_recall: 0.4697 - val_f1: 0.0860\n",
            "Epoch 72/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3064 - acc: 0.6472 - auc: 0.8805 - precision: 0.1031 - recall: 0.9240 - f1: 0.1798 - val_loss: 2.9628 - val_acc: 0.6810 - val_auc: 0.5680 - val_precision: 0.0573 - val_recall: 0.4091 - val_f1: 0.0938\n",
            "Epoch 73/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2995 - acc: 0.6763 - auc: 0.8796 - precision: 0.1124 - recall: 0.9354 - f1: 0.1954 - val_loss: 3.0020 - val_acc: 0.7028 - val_auc: 0.5683 - val_precision: 0.0636 - val_recall: 0.4242 - val_f1: 0.1036\n",
            "Epoch 74/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2704 - acc: 0.6861 - auc: 0.8882 - precision: 0.1141 - recall: 0.9202 - f1: 0.1990 - val_loss: 2.8844 - val_acc: 0.6123 - val_auc: 0.5700 - val_precision: 0.0516 - val_recall: 0.4545 - val_f1: 0.0903\n",
            "Epoch 75/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2498 - acc: 0.6854 - auc: 0.8937 - precision: 0.1161 - recall: 0.9430 - f1: 0.2012 - val_loss: 2.9225 - val_acc: 0.6750 - val_auc: 0.5734 - val_precision: 0.0653 - val_recall: 0.4848 - val_f1: 0.1111\n",
            "Epoch 76/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2828 - acc: 0.6870 - auc: 0.8831 - precision: 0.1137 - recall: 0.9125 - f1: 0.1949 - val_loss: 2.9351 - val_acc: 0.5878 - val_auc: 0.5635 - val_precision: 0.0514 - val_recall: 0.4848 - val_f1: 0.0877\n",
            "Epoch 77/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2780 - acc: 0.6770 - auc: 0.8864 - precision: 0.1119 - recall: 0.9278 - f1: 0.1932 - val_loss: 3.0122 - val_acc: 0.6717 - val_auc: 0.5654 - val_precision: 0.0575 - val_recall: 0.4242 - val_f1: 0.0954\n",
            "Epoch 78/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2868 - acc: 0.6920 - auc: 0.8838 - precision: 0.1172 - recall: 0.9316 - f1: 0.2051 - val_loss: 3.0032 - val_acc: 0.6083 - val_auc: 0.5625 - val_precision: 0.0541 - val_recall: 0.4848 - val_f1: 0.0942\n",
            "Epoch 79/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3046 - acc: 0.6717 - auc: 0.8792 - precision: 0.1103 - recall: 0.9278 - f1: 0.1926 - val_loss: 2.9594 - val_acc: 0.6612 - val_auc: 0.5689 - val_precision: 0.0609 - val_recall: 0.4697 - val_f1: 0.1046\n",
            "Epoch 80/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2717 - acc: 0.6857 - auc: 0.8905 - precision: 0.1154 - recall: 0.9354 - f1: 0.2027 - val_loss: 3.0240 - val_acc: 0.6169 - val_auc: 0.5629 - val_precision: 0.0538 - val_recall: 0.4697 - val_f1: 0.0905\n",
            "Epoch 81/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2769 - acc: 0.7090 - auc: 0.8875 - precision: 0.1236 - recall: 0.9354 - f1: 0.2080 - val_loss: 3.0177 - val_acc: 0.6460 - val_auc: 0.5671 - val_precision: 0.0583 - val_recall: 0.4697 - val_f1: 0.0985\n",
            "Epoch 82/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2916 - acc: 0.6882 - auc: 0.8822 - precision: 0.1152 - recall: 0.9240 - f1: 0.1977 - val_loss: 2.9293 - val_acc: 0.5958 - val_auc: 0.5673 - val_precision: 0.0510 - val_recall: 0.4697 - val_f1: 0.0871\n",
            "Epoch 83/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2869 - acc: 0.6786 - auc: 0.8850 - precision: 0.1124 - recall: 0.9278 - f1: 0.1941 - val_loss: 2.8612 - val_acc: 0.5608 - val_auc: 0.5622 - val_precision: 0.0510 - val_recall: 0.5152 - val_f1: 0.0903\n",
            "Epoch 84/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2778 - acc: 0.6975 - auc: 0.8864 - precision: 0.1176 - recall: 0.9163 - f1: 0.2035 - val_loss: 3.0229 - val_acc: 0.5991 - val_auc: 0.5608 - val_precision: 0.0514 - val_recall: 0.4697 - val_f1: 0.0874\n",
            "Epoch 85/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2641 - acc: 0.6717 - auc: 0.8899 - precision: 0.1103 - recall: 0.9278 - f1: 0.1908 - val_loss: 3.1191 - val_acc: 0.6229 - val_auc: 0.5583 - val_precision: 0.0483 - val_recall: 0.4091 - val_f1: 0.0806\n",
            "Epoch 86/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2325 - acc: 0.7027 - auc: 0.8974 - precision: 0.1209 - recall: 0.9316 - f1: 0.2086 - val_loss: 3.0538 - val_acc: 0.6295 - val_auc: 0.5595 - val_precision: 0.0524 - val_recall: 0.4394 - val_f1: 0.0871\n",
            "Epoch 87/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2597 - acc: 0.6727 - auc: 0.8899 - precision: 0.1117 - recall: 0.9392 - f1: 0.1912 - val_loss: 3.1698 - val_acc: 0.6929 - val_auc: 0.5622 - val_precision: 0.0596 - val_recall: 0.4091 - val_f1: 0.0962\n",
            "Epoch 88/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2535 - acc: 0.7054 - auc: 0.8925 - precision: 0.1204 - recall: 0.9163 - f1: 0.2094 - val_loss: 3.1656 - val_acc: 0.7213 - val_auc: 0.5658 - val_precision: 0.0594 - val_recall: 0.3636 - val_f1: 0.0972\n",
            "Epoch 89/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2492 - acc: 0.6910 - auc: 0.8925 - precision: 0.1176 - recall: 0.9392 - f1: 0.2017 - val_loss: 3.0969 - val_acc: 0.6559 - val_auc: 0.5611 - val_precision: 0.0530 - val_recall: 0.4091 - val_f1: 0.0893\n",
            "Epoch 90/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2791 - acc: 0.6854 - auc: 0.8849 - precision: 0.1150 - recall: 0.9316 - f1: 0.1986 - val_loss: 3.1210 - val_acc: 0.6948 - val_auc: 0.5634 - val_precision: 0.0580 - val_recall: 0.3939 - val_f1: 0.0947\n",
            "Epoch 91/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2413 - acc: 0.7115 - auc: 0.8957 - precision: 0.1234 - recall: 0.9240 - f1: 0.2110 - val_loss: 3.0143 - val_acc: 0.5958 - val_auc: 0.5616 - val_precision: 0.0525 - val_recall: 0.4848 - val_f1: 0.0925\n",
            "Epoch 92/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2751 - acc: 0.6839 - auc: 0.8864 - precision: 0.1159 - recall: 0.9468 - f1: 0.2039 - val_loss: 3.0824 - val_acc: 0.5865 - val_auc: 0.5581 - val_precision: 0.0498 - val_recall: 0.4697 - val_f1: 0.0836\n",
            "Epoch 93/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2383 - acc: 0.7151 - auc: 0.8979 - precision: 0.1256 - recall: 0.9316 - f1: 0.2148 - val_loss: 3.0997 - val_acc: 0.6526 - val_auc: 0.5643 - val_precision: 0.0560 - val_recall: 0.4394 - val_f1: 0.0929\n",
            "Epoch 94/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2445 - acc: 0.7082 - auc: 0.8944 - precision: 0.1210 - recall: 0.9125 - f1: 0.2073 - val_loss: 2.9777 - val_acc: 0.5793 - val_auc: 0.5605 - val_precision: 0.0518 - val_recall: 0.5000 - val_f1: 0.0911\n",
            "Epoch 95/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2456 - acc: 0.6880 - auc: 0.8944 - precision: 0.1158 - recall: 0.9316 - f1: 0.2010 - val_loss: 3.1644 - val_acc: 0.6770 - val_auc: 0.5587 - val_precision: 0.0566 - val_recall: 0.4091 - val_f1: 0.0949\n",
            "Epoch 96/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2749 - acc: 0.6928 - auc: 0.8864 - precision: 0.1171 - recall: 0.9278 - f1: 0.2031 - val_loss: 3.1152 - val_acc: 0.6004 - val_auc: 0.5512 - val_precision: 0.0486 - val_recall: 0.4394 - val_f1: 0.0823\n",
            "Epoch 97/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2404 - acc: 0.6872 - auc: 0.8949 - precision: 0.1156 - recall: 0.9316 - f1: 0.1963 - val_loss: 3.1919 - val_acc: 0.6757 - val_auc: 0.5564 - val_precision: 0.0545 - val_recall: 0.3939 - val_f1: 0.0887\n",
            "Epoch 98/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2354 - acc: 0.6963 - auc: 0.8979 - precision: 0.1205 - recall: 0.9506 - f1: 0.2053 - val_loss: 3.1293 - val_acc: 0.6433 - val_auc: 0.5586 - val_precision: 0.0545 - val_recall: 0.4394 - val_f1: 0.0913\n",
            "Epoch 99/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2427 - acc: 0.7098 - auc: 0.8942 - precision: 0.1251 - recall: 0.9468 - f1: 0.2179 - val_loss: 3.0868 - val_acc: 0.6169 - val_auc: 0.5583 - val_precision: 0.0507 - val_recall: 0.4394 - val_f1: 0.0867\n",
            "Epoch 100/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2921 - acc: 0.7041 - auc: 0.8846 - precision: 0.1199 - recall: 0.9163 - f1: 0.2060 - val_loss: 3.0533 - val_acc: 0.6499 - val_auc: 0.5614 - val_precision: 0.0573 - val_recall: 0.4545 - val_f1: 0.1003\n",
            "Epoch 101/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2447 - acc: 0.6913 - auc: 0.8956 - precision: 0.1177 - recall: 0.9392 - f1: 0.2080 - val_loss: 3.1510 - val_acc: 0.7371 - val_auc: 0.5699 - val_precision: 0.0632 - val_recall: 0.3636 - val_f1: 0.1018\n",
            "Epoch 102/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2454 - acc: 0.7031 - auc: 0.8928 - precision: 0.1210 - recall: 0.9316 - f1: 0.2087 - val_loss: 3.1834 - val_acc: 0.6704 - val_auc: 0.5603 - val_precision: 0.0536 - val_recall: 0.3939 - val_f1: 0.0888\n",
            "Epoch 103/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2253 - acc: 0.7094 - auc: 0.8982 - precision: 0.1230 - recall: 0.9278 - f1: 0.2133 - val_loss: 3.1346 - val_acc: 0.6565 - val_auc: 0.5631 - val_precision: 0.0549 - val_recall: 0.4242 - val_f1: 0.0922\n",
            "Epoch 104/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2151 - acc: 0.6857 - auc: 0.9019 - precision: 0.1172 - recall: 0.9544 - f1: 0.2042 - val_loss: 3.2794 - val_acc: 0.7384 - val_auc: 0.5652 - val_precision: 0.0565 - val_recall: 0.3182 - val_f1: 0.0900\n",
            "Epoch 105/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.2439 - acc: 0.7056 - auc: 0.8922 - precision: 0.1227 - recall: 0.9392 - f1: 0.2148 - val_loss: 3.2890 - val_acc: 0.6975 - val_auc: 0.5599 - val_precision: 0.0586 - val_recall: 0.3939 - val_f1: 0.0945\n",
            "Epoch 106/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2420 - acc: 0.6984 - auc: 0.8932 - precision: 0.1190 - recall: 0.9278 - f1: 0.2038 - val_loss: 3.1283 - val_acc: 0.6229 - val_auc: 0.5604 - val_precision: 0.0515 - val_recall: 0.4394 - val_f1: 0.0872\n",
            "Epoch 107/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2318 - acc: 0.6993 - auc: 0.8969 - precision: 0.1193 - recall: 0.9278 - f1: 0.2066 - val_loss: 3.3071 - val_acc: 0.7299 - val_auc: 0.5630 - val_precision: 0.0614 - val_recall: 0.3636 - val_f1: 0.0989\n",
            "Epoch 108/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1946 - acc: 0.7234 - auc: 0.9061 - precision: 0.1293 - recall: 0.9354 - f1: 0.2193 - val_loss: 3.3381 - val_acc: 0.6466 - val_auc: 0.5547 - val_precision: 0.0516 - val_recall: 0.4091 - val_f1: 0.0854\n",
            "Epoch 109/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2061 - acc: 0.7242 - auc: 0.9062 - precision: 0.1296 - recall: 0.9354 - f1: 0.2209 - val_loss: 3.1036 - val_acc: 0.6143 - val_auc: 0.5596 - val_precision: 0.0534 - val_recall: 0.4697 - val_f1: 0.0933\n",
            "Epoch 110/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2116 - acc: 0.7084 - auc: 0.9019 - precision: 0.1230 - recall: 0.9316 - f1: 0.2104 - val_loss: 3.1495 - val_acc: 0.6268 - val_auc: 0.5627 - val_precision: 0.0553 - val_recall: 0.4697 - val_f1: 0.0954\n",
            "Epoch 111/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2295 - acc: 0.7237 - auc: 0.8986 - precision: 0.1278 - recall: 0.9202 - f1: 0.2160 - val_loss: 3.1460 - val_acc: 0.6790 - val_auc: 0.5643 - val_precision: 0.0570 - val_recall: 0.4091 - val_f1: 0.0976\n",
            "Epoch 112/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2040 - acc: 0.7184 - auc: 0.9046 - precision: 0.1276 - recall: 0.9392 - f1: 0.2190 - val_loss: 3.3023 - val_acc: 0.6328 - val_auc: 0.5559 - val_precision: 0.0496 - val_recall: 0.4091 - val_f1: 0.0820\n",
            "Epoch 113/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2484 - acc: 0.6996 - auc: 0.8938 - precision: 0.1205 - recall: 0.9392 - f1: 0.2102 - val_loss: 3.2491 - val_acc: 0.7173 - val_auc: 0.5616 - val_precision: 0.0607 - val_recall: 0.3788 - val_f1: 0.0992\n",
            "Epoch 114/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2092 - acc: 0.7247 - auc: 0.9021 - precision: 0.1290 - recall: 0.9278 - f1: 0.2218 - val_loss: 3.2172 - val_acc: 0.6123 - val_auc: 0.5569 - val_precision: 0.0501 - val_recall: 0.4394 - val_f1: 0.0845\n",
            "Epoch 115/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1981 - acc: 0.7095 - auc: 0.9064 - precision: 0.1242 - recall: 0.9392 - f1: 0.2158 - val_loss: 3.3746 - val_acc: 0.6902 - val_auc: 0.5537 - val_precision: 0.0552 - val_recall: 0.3788 - val_f1: 0.0912\n",
            "Epoch 116/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2159 - acc: 0.7201 - auc: 0.9012 - precision: 0.1275 - recall: 0.9316 - f1: 0.2189 - val_loss: 3.1854 - val_acc: 0.5931 - val_auc: 0.5561 - val_precision: 0.0492 - val_recall: 0.4545 - val_f1: 0.0839\n",
            "Epoch 117/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2456 - acc: 0.6978 - auc: 0.8933 - precision: 0.1195 - recall: 0.9354 - f1: 0.2047 - val_loss: 3.2848 - val_acc: 0.6321 - val_auc: 0.5574 - val_precision: 0.0495 - val_recall: 0.4091 - val_f1: 0.0824\n",
            "Epoch 118/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2175 - acc: 0.7161 - auc: 0.8999 - precision: 0.1260 - recall: 0.9316 - f1: 0.2219 - val_loss: 3.3462 - val_acc: 0.7021 - val_auc: 0.5568 - val_precision: 0.0575 - val_recall: 0.3788 - val_f1: 0.0923\n",
            "Epoch 119/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2341 - acc: 0.6937 - auc: 0.8938 - precision: 0.1185 - recall: 0.9392 - f1: 0.2104 - val_loss: 3.1442 - val_acc: 0.6764 - val_auc: 0.5662 - val_precision: 0.0602 - val_recall: 0.4394 - val_f1: 0.1031\n",
            "Epoch 120/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2192 - acc: 0.7123 - auc: 0.8987 - precision: 0.1245 - recall: 0.9316 - f1: 0.2116 - val_loss: 3.3062 - val_acc: 0.6948 - val_auc: 0.5627 - val_precision: 0.0600 - val_recall: 0.4091 - val_f1: 0.0996\n",
            "Epoch 121/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2246 - acc: 0.7031 - auc: 0.8964 - precision: 0.1214 - recall: 0.9354 - f1: 0.2098 - val_loss: 3.2966 - val_acc: 0.6929 - val_auc: 0.5625 - val_precision: 0.0576 - val_recall: 0.3939 - val_f1: 0.0954\n",
            "Epoch 122/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1818 - acc: 0.7226 - auc: 0.9064 - precision: 0.1293 - recall: 0.9392 - f1: 0.2199 - val_loss: 3.1662 - val_acc: 0.6209 - val_auc: 0.5615 - val_precision: 0.0512 - val_recall: 0.4394 - val_f1: 0.0870\n",
            "Epoch 123/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1858 - acc: 0.7204 - auc: 0.9067 - precision: 0.1281 - recall: 0.9354 - f1: 0.2164 - val_loss: 3.4535 - val_acc: 0.7299 - val_auc: 0.5624 - val_precision: 0.0614 - val_recall: 0.3636 - val_f1: 0.0975\n",
            "Epoch 124/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1941 - acc: 0.7075 - auc: 0.9026 - precision: 0.1231 - recall: 0.9354 - f1: 0.2092 - val_loss: 3.2472 - val_acc: 0.6156 - val_auc: 0.5612 - val_precision: 0.0536 - val_recall: 0.4697 - val_f1: 0.0908\n",
            "Epoch 125/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1961 - acc: 0.7089 - auc: 0.9046 - precision: 0.1236 - recall: 0.9354 - f1: 0.2114 - val_loss: 3.3224 - val_acc: 0.6136 - val_auc: 0.5533 - val_precision: 0.0487 - val_recall: 0.4242 - val_f1: 0.0809\n",
            "Epoch 126/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2393 - acc: 0.7059 - auc: 0.8937 - precision: 0.1224 - recall: 0.9354 - f1: 0.2050 - val_loss: 3.3334 - val_acc: 0.6083 - val_auc: 0.5527 - val_precision: 0.0480 - val_recall: 0.4242 - val_f1: 0.0814\n",
            "Epoch 127/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1862 - acc: 0.7125 - auc: 0.9055 - precision: 0.1253 - recall: 0.9392 - f1: 0.2167 - val_loss: 3.2316 - val_acc: 0.6664 - val_auc: 0.5615 - val_precision: 0.0566 - val_recall: 0.4242 - val_f1: 0.0988\n",
            "Epoch 128/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2377 - acc: 0.7094 - auc: 0.8957 - precision: 0.1237 - recall: 0.9354 - f1: 0.2138 - val_loss: 3.2418 - val_acc: 0.6770 - val_auc: 0.5597 - val_precision: 0.0566 - val_recall: 0.4091 - val_f1: 0.0997\n",
            "Epoch 129/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2117 - acc: 0.7229 - auc: 0.9016 - precision: 0.1271 - recall: 0.9163 - f1: 0.2185 - val_loss: 3.4120 - val_acc: 0.6453 - val_auc: 0.5561 - val_precision: 0.0514 - val_recall: 0.4091 - val_f1: 0.0850\n",
            "Epoch 130/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1897 - acc: 0.7082 - auc: 0.9060 - precision: 0.1229 - recall: 0.9316 - f1: 0.2082 - val_loss: 3.4366 - val_acc: 0.6506 - val_auc: 0.5512 - val_precision: 0.0522 - val_recall: 0.4091 - val_f1: 0.0856\n",
            "Epoch 131/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2154 - acc: 0.7123 - auc: 0.9008 - precision: 0.1268 - recall: 0.9544 - f1: 0.2214 - val_loss: 3.2745 - val_acc: 0.6143 - val_auc: 0.5529 - val_precision: 0.0519 - val_recall: 0.4545 - val_f1: 0.0887\n",
            "Epoch 132/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2230 - acc: 0.6943 - auc: 0.8968 - precision: 0.1176 - recall: 0.9278 - f1: 0.2047 - val_loss: 3.3480 - val_acc: 0.6030 - val_auc: 0.5478 - val_precision: 0.0489 - val_recall: 0.4394 - val_f1: 0.0826\n",
            "Epoch 133/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1909 - acc: 0.7284 - auc: 0.9085 - precision: 0.1309 - recall: 0.9316 - f1: 0.2238 - val_loss: 3.5596 - val_acc: 0.7061 - val_auc: 0.5512 - val_precision: 0.0583 - val_recall: 0.3788 - val_f1: 0.0951\n",
            "Epoch 134/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1868 - acc: 0.7011 - auc: 0.9070 - precision: 0.1229 - recall: 0.9582 - f1: 0.2109 - val_loss: 3.4458 - val_acc: 0.6618 - val_auc: 0.5504 - val_precision: 0.0504 - val_recall: 0.3788 - val_f1: 0.0841\n",
            "Epoch 135/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1550 - acc: 0.7454 - auc: 0.9157 - precision: 0.1418 - recall: 0.9620 - f1: 0.2410 - val_loss: 3.3646 - val_acc: 0.6466 - val_auc: 0.5545 - val_precision: 0.0533 - val_recall: 0.4242 - val_f1: 0.0900\n",
            "Epoch 136/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2124 - acc: 0.7127 - auc: 0.9006 - precision: 0.1265 - recall: 0.9506 - f1: 0.2152 - val_loss: 3.4200 - val_acc: 0.6565 - val_auc: 0.5547 - val_precision: 0.0549 - val_recall: 0.4242 - val_f1: 0.0915\n",
            "Epoch 137/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1768 - acc: 0.7241 - auc: 0.9087 - precision: 0.1303 - recall: 0.9430 - f1: 0.2239 - val_loss: 3.4282 - val_acc: 0.6803 - val_auc: 0.5576 - val_precision: 0.0553 - val_recall: 0.3939 - val_f1: 0.0910\n",
            "Epoch 138/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1719 - acc: 0.7388 - auc: 0.9093 - precision: 0.1367 - recall: 0.9430 - f1: 0.2319 - val_loss: 3.3552 - val_acc: 0.5799 - val_auc: 0.5558 - val_precision: 0.0505 - val_recall: 0.4848 - val_f1: 0.0851\n",
            "Epoch 139/200\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 1.1696 - acc: 0.7100 - auc: 0.9081 - precision: 0.1255 - recall: 0.9506 - f1: 0.2172 - val_loss: 3.3905 - val_acc: 0.6598 - val_auc: 0.5602 - val_precision: 0.0554 - val_recall: 0.4242 - val_f1: 0.0968\n",
            "Epoch 140/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1431 - acc: 0.7308 - auc: 0.9161 - precision: 0.1336 - recall: 0.9468 - f1: 0.2282 - val_loss: 3.4068 - val_acc: 0.6361 - val_auc: 0.5570 - val_precision: 0.0550 - val_recall: 0.4545 - val_f1: 0.0944\n",
            "Epoch 141/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1866 - acc: 0.7312 - auc: 0.9056 - precision: 0.1317 - recall: 0.9278 - f1: 0.2196 - val_loss: 3.4417 - val_acc: 0.6618 - val_auc: 0.5547 - val_precision: 0.0593 - val_recall: 0.4545 - val_f1: 0.0986\n",
            "Epoch 142/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1175 - acc: 0.7450 - auc: 0.9207 - precision: 0.1400 - recall: 0.9468 - f1: 0.2364 - val_loss: 3.5328 - val_acc: 0.6592 - val_auc: 0.5535 - val_precision: 0.0536 - val_recall: 0.4091 - val_f1: 0.0890\n",
            "Epoch 143/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1833 - acc: 0.7234 - auc: 0.9060 - precision: 0.1297 - recall: 0.9392 - f1: 0.2220 - val_loss: 3.5721 - val_acc: 0.6717 - val_auc: 0.5518 - val_precision: 0.0538 - val_recall: 0.3939 - val_f1: 0.0897\n",
            "Epoch 144/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1707 - acc: 0.7290 - auc: 0.9097 - precision: 0.1328 - recall: 0.9468 - f1: 0.2270 - val_loss: 3.6983 - val_acc: 0.7517 - val_auc: 0.5532 - val_precision: 0.0546 - val_recall: 0.2879 - val_f1: 0.0816\n",
            "Epoch 145/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1607 - acc: 0.7345 - auc: 0.9133 - precision: 0.1352 - recall: 0.9468 - f1: 0.2287 - val_loss: 3.5511 - val_acc: 0.6215 - val_auc: 0.5459 - val_precision: 0.0497 - val_recall: 0.4242 - val_f1: 0.0825\n",
            "Epoch 146/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1834 - acc: 0.7260 - auc: 0.9066 - precision: 0.1292 - recall: 0.9240 - f1: 0.2211 - val_loss: 3.5325 - val_acc: 0.7133 - val_auc: 0.5569 - val_precision: 0.0577 - val_recall: 0.3636 - val_f1: 0.0957\n",
            "Epoch 147/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2087 - acc: 0.7120 - auc: 0.9021 - precision: 0.1221 - recall: 0.9087 - f1: 0.2102 - val_loss: 3.5518 - val_acc: 0.6367 - val_auc: 0.5528 - val_precision: 0.0519 - val_recall: 0.4242 - val_f1: 0.0857\n",
            "Epoch 148/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1646 - acc: 0.7264 - auc: 0.9081 - precision: 0.1328 - recall: 0.9582 - f1: 0.2277 - val_loss: 3.5111 - val_acc: 0.6380 - val_auc: 0.5521 - val_precision: 0.0520 - val_recall: 0.4242 - val_f1: 0.0869\n",
            "Epoch 149/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1403 - acc: 0.7335 - auc: 0.9171 - precision: 0.1343 - recall: 0.9430 - f1: 0.2248 - val_loss: 3.5533 - val_acc: 0.6433 - val_auc: 0.5508 - val_precision: 0.0528 - val_recall: 0.4242 - val_f1: 0.0871\n",
            "Epoch 150/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1428 - acc: 0.7503 - auc: 0.9154 - precision: 0.1422 - recall: 0.9430 - f1: 0.2418 - val_loss: 3.7306 - val_acc: 0.7213 - val_auc: 0.5488 - val_precision: 0.0637 - val_recall: 0.3939 - val_f1: 0.1017\n",
            "Epoch 151/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1571 - acc: 0.7358 - auc: 0.9124 - precision: 0.1354 - recall: 0.9430 - f1: 0.2320 - val_loss: 3.5187 - val_acc: 0.6460 - val_auc: 0.5548 - val_precision: 0.0532 - val_recall: 0.4242 - val_f1: 0.0898\n",
            "Epoch 152/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1100 - acc: 0.7593 - auc: 0.9229 - precision: 0.1451 - recall: 0.9278 - f1: 0.2388 - val_loss: 3.7728 - val_acc: 0.7517 - val_auc: 0.5524 - val_precision: 0.0621 - val_recall: 0.3333 - val_f1: 0.0932\n",
            "Epoch 153/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1208 - acc: 0.7607 - auc: 0.9199 - precision: 0.1488 - recall: 0.9544 - f1: 0.2441 - val_loss: 3.6261 - val_acc: 0.7259 - val_auc: 0.5603 - val_precision: 0.0560 - val_recall: 0.3333 - val_f1: 0.0892\n",
            "Epoch 154/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1773 - acc: 0.7312 - auc: 0.9038 - precision: 0.1333 - recall: 0.9430 - f1: 0.2332 - val_loss: 3.6460 - val_acc: 0.6836 - val_auc: 0.5500 - val_precision: 0.0540 - val_recall: 0.3788 - val_f1: 0.0895\n",
            "Epoch 155/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1569 - acc: 0.7265 - auc: 0.9127 - precision: 0.1314 - recall: 0.9430 - f1: 0.2251 - val_loss: 3.5209 - val_acc: 0.6387 - val_auc: 0.5537 - val_precision: 0.0538 - val_recall: 0.4394 - val_f1: 0.0906\n",
            "Epoch 156/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1642 - acc: 0.7483 - auc: 0.9126 - precision: 0.1429 - recall: 0.9582 - f1: 0.2390 - val_loss: 3.7954 - val_acc: 0.7028 - val_auc: 0.5470 - val_precision: 0.0576 - val_recall: 0.3788 - val_f1: 0.0927\n",
            "Epoch 157/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1443 - acc: 0.7302 - auc: 0.9146 - precision: 0.1329 - recall: 0.9430 - f1: 0.2272 - val_loss: 3.8316 - val_acc: 0.7543 - val_auc: 0.5533 - val_precision: 0.0578 - val_recall: 0.3030 - val_f1: 0.0872\n",
            "Epoch 158/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1455 - acc: 0.7406 - auc: 0.9165 - precision: 0.1375 - recall: 0.9430 - f1: 0.2268 - val_loss: 3.6884 - val_acc: 0.7001 - val_auc: 0.5567 - val_precision: 0.0611 - val_recall: 0.4091 - val_f1: 0.0989\n",
            "Epoch 159/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1211 - acc: 0.7396 - auc: 0.9209 - precision: 0.1383 - recall: 0.9544 - f1: 0.2365 - val_loss: 3.6504 - val_acc: 0.6625 - val_auc: 0.5554 - val_precision: 0.0594 - val_recall: 0.4545 - val_f1: 0.0987\n",
            "Epoch 160/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1260 - acc: 0.7492 - auc: 0.9196 - precision: 0.1425 - recall: 0.9506 - f1: 0.2411 - val_loss: 3.7797 - val_acc: 0.7411 - val_auc: 0.5554 - val_precision: 0.0595 - val_recall: 0.3333 - val_f1: 0.0943\n",
            "Epoch 161/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1685 - acc: 0.7346 - auc: 0.9115 - precision: 0.1333 - recall: 0.9278 - f1: 0.2210 - val_loss: 3.7408 - val_acc: 0.6592 - val_auc: 0.5503 - val_precision: 0.0571 - val_recall: 0.4394 - val_f1: 0.0942\n",
            "Epoch 162/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1371 - acc: 0.7441 - auc: 0.9154 - precision: 0.1400 - recall: 0.9506 - f1: 0.2393 - val_loss: 3.5234 - val_acc: 0.5938 - val_auc: 0.5529 - val_precision: 0.0507 - val_recall: 0.4697 - val_f1: 0.0891\n",
            "Epoch 163/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1166 - acc: 0.7441 - auc: 0.9198 - precision: 0.1416 - recall: 0.9658 - f1: 0.2372 - val_loss: 3.8093 - val_acc: 0.7517 - val_auc: 0.5579 - val_precision: 0.0597 - val_recall: 0.3182 - val_f1: 0.0918\n",
            "Epoch 164/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1859 - acc: 0.7497 - auc: 0.9059 - precision: 0.1398 - recall: 0.9240 - f1: 0.2385 - val_loss: 3.4599 - val_acc: 0.5720 - val_auc: 0.5551 - val_precision: 0.0495 - val_recall: 0.4848 - val_f1: 0.0872\n",
            "Epoch 165/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1143 - acc: 0.7231 - auc: 0.9199 - precision: 0.1334 - recall: 0.9772 - f1: 0.2271 - val_loss: 3.7961 - val_acc: 0.6486 - val_auc: 0.5470 - val_precision: 0.0519 - val_recall: 0.4091 - val_f1: 0.0855\n",
            "Epoch 166/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1524 - acc: 0.7310 - auc: 0.9131 - precision: 0.1317 - recall: 0.9278 - f1: 0.2238 - val_loss: 3.8500 - val_acc: 0.7199 - val_auc: 0.5541 - val_precision: 0.0634 - val_recall: 0.3939 - val_f1: 0.1036\n",
            "Epoch 167/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1714 - acc: 0.7515 - auc: 0.9094 - precision: 0.1386 - recall: 0.9049 - f1: 0.2310 - val_loss: 3.6317 - val_acc: 0.6882 - val_auc: 0.5528 - val_precision: 0.0568 - val_recall: 0.3939 - val_f1: 0.0931\n",
            "Epoch 168/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1440 - acc: 0.7348 - auc: 0.9138 - precision: 0.1357 - recall: 0.9506 - f1: 0.2322 - val_loss: 3.6895 - val_acc: 0.6658 - val_auc: 0.5518 - val_precision: 0.0547 - val_recall: 0.4091 - val_f1: 0.0917\n",
            "Epoch 169/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1382 - acc: 0.7356 - auc: 0.9176 - precision: 0.1357 - recall: 0.9468 - f1: 0.2312 - val_loss: 3.7034 - val_acc: 0.6559 - val_auc: 0.5463 - val_precision: 0.0548 - val_recall: 0.4242 - val_f1: 0.0911\n",
            "Epoch 170/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1281 - acc: 0.7447 - auc: 0.9173 - precision: 0.1403 - recall: 0.9506 - f1: 0.2339 - val_loss: 3.7810 - val_acc: 0.7332 - val_auc: 0.5538 - val_precision: 0.0622 - val_recall: 0.3636 - val_f1: 0.0995\n",
            "Epoch 171/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1160 - acc: 0.7617 - auc: 0.9213 - precision: 0.1481 - recall: 0.9430 - f1: 0.2521 - val_loss: 4.0510 - val_acc: 0.7563 - val_auc: 0.5485 - val_precision: 0.0634 - val_recall: 0.3333 - val_f1: 0.0975\n",
            "Epoch 172/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1052 - acc: 0.7426 - auc: 0.9228 - precision: 0.1405 - recall: 0.9620 - f1: 0.2376 - val_loss: 3.9721 - val_acc: 0.6658 - val_auc: 0.5403 - val_precision: 0.0528 - val_recall: 0.3939 - val_f1: 0.0859\n",
            "Epoch 173/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1393 - acc: 0.7469 - auc: 0.9147 - precision: 0.1405 - recall: 0.9430 - f1: 0.2354 - val_loss: 3.6651 - val_acc: 0.6162 - val_auc: 0.5475 - val_precision: 0.0522 - val_recall: 0.4545 - val_f1: 0.0880\n",
            "Epoch 174/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0913 - acc: 0.7419 - auc: 0.9250 - precision: 0.1398 - recall: 0.9582 - f1: 0.2416 - val_loss: 3.8508 - val_acc: 0.7365 - val_auc: 0.5535 - val_precision: 0.0607 - val_recall: 0.3485 - val_f1: 0.0940\n",
            "Epoch 175/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1119 - acc: 0.7761 - auc: 0.9226 - precision: 0.1562 - recall: 0.9430 - f1: 0.2617 - val_loss: 3.8771 - val_acc: 0.7133 - val_auc: 0.5505 - val_precision: 0.0577 - val_recall: 0.3636 - val_f1: 0.0943\n",
            "Epoch 176/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1168 - acc: 0.7576 - auc: 0.9204 - precision: 0.1463 - recall: 0.9468 - f1: 0.2442 - val_loss: 3.5483 - val_acc: 0.6044 - val_auc: 0.5497 - val_precision: 0.0521 - val_recall: 0.4697 - val_f1: 0.0927\n",
            "Epoch 177/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1133 - acc: 0.7449 - auc: 0.9205 - precision: 0.1404 - recall: 0.9506 - f1: 0.2373 - val_loss: 4.0341 - val_acc: 0.7563 - val_auc: 0.5453 - val_precision: 0.0583 - val_recall: 0.3030 - val_f1: 0.0909\n",
            "Epoch 178/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1481 - acc: 0.7363 - auc: 0.9127 - precision: 0.1360 - recall: 0.9468 - f1: 0.2327 - val_loss: 3.9151 - val_acc: 0.7008 - val_auc: 0.5480 - val_precision: 0.0552 - val_recall: 0.3636 - val_f1: 0.0892\n",
            "Epoch 179/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1313 - acc: 0.7500 - auc: 0.9184 - precision: 0.1412 - recall: 0.9354 - f1: 0.2395 - val_loss: 3.9752 - val_acc: 0.6962 - val_auc: 0.5450 - val_precision: 0.0563 - val_recall: 0.3788 - val_f1: 0.0898\n",
            "Epoch 180/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1337 - acc: 0.7452 - auc: 0.9161 - precision: 0.1409 - recall: 0.9544 - f1: 0.2345 - val_loss: 3.7394 - val_acc: 0.7048 - val_auc: 0.5551 - val_precision: 0.0621 - val_recall: 0.4091 - val_f1: 0.1033\n",
            "Epoch 181/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1284 - acc: 0.7518 - auc: 0.9169 - precision: 0.1417 - recall: 0.9316 - f1: 0.2379 - val_loss: 3.6234 - val_acc: 0.6235 - val_auc: 0.5561 - val_precision: 0.0548 - val_recall: 0.4697 - val_f1: 0.0965\n",
            "Epoch 182/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1054 - acc: 0.7310 - auc: 0.9219 - precision: 0.1348 - recall: 0.9582 - f1: 0.2346 - val_loss: 4.1007 - val_acc: 0.7675 - val_auc: 0.5501 - val_precision: 0.0613 - val_recall: 0.3030 - val_f1: 0.0946\n",
            "Epoch 183/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1252 - acc: 0.7437 - auc: 0.9185 - precision: 0.1390 - recall: 0.9430 - f1: 0.2315 - val_loss: 3.8303 - val_acc: 0.6513 - val_auc: 0.5438 - val_precision: 0.0558 - val_recall: 0.4394 - val_f1: 0.0917\n",
            "Epoch 184/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1276 - acc: 0.7512 - auc: 0.9171 - precision: 0.1414 - recall: 0.9316 - f1: 0.2367 - val_loss: 3.8461 - val_acc: 0.6400 - val_auc: 0.5403 - val_precision: 0.0523 - val_recall: 0.4242 - val_f1: 0.0877\n",
            "Epoch 185/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0938 - acc: 0.7355 - auc: 0.9215 - precision: 0.1380 - recall: 0.9696 - f1: 0.2364 - val_loss: 3.9184 - val_acc: 0.7213 - val_auc: 0.5460 - val_precision: 0.0572 - val_recall: 0.3485 - val_f1: 0.0933\n",
            "Epoch 186/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0963 - acc: 0.7650 - auc: 0.9246 - precision: 0.1515 - recall: 0.9582 - f1: 0.2524 - val_loss: 3.8723 - val_acc: 0.6559 - val_auc: 0.5466 - val_precision: 0.0548 - val_recall: 0.4242 - val_f1: 0.0897\n",
            "Epoch 187/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0701 - acc: 0.7521 - auc: 0.9284 - precision: 0.1439 - recall: 0.9506 - f1: 0.2377 - val_loss: 3.7709 - val_acc: 0.6843 - val_auc: 0.5571 - val_precision: 0.0560 - val_recall: 0.3939 - val_f1: 0.0953\n",
            "Epoch 188/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.0802 - acc: 0.7642 - auc: 0.9278 - precision: 0.1494 - recall: 0.9430 - f1: 0.2506 - val_loss: 4.3039 - val_acc: 0.7985 - val_auc: 0.5543 - val_precision: 0.0655 - val_recall: 0.2727 - val_f1: 0.0923\n",
            "Epoch 189/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1242 - acc: 0.7412 - auc: 0.9141 - precision: 0.1370 - recall: 0.9354 - f1: 0.2297 - val_loss: 3.6848 - val_acc: 0.6103 - val_auc: 0.5543 - val_precision: 0.0529 - val_recall: 0.4697 - val_f1: 0.0921\n",
            "Epoch 190/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0935 - acc: 0.7597 - auc: 0.9238 - precision: 0.1449 - recall: 0.9240 - f1: 0.2424 - val_loss: 4.0252 - val_acc: 0.7596 - val_auc: 0.5574 - val_precision: 0.0618 - val_recall: 0.3182 - val_f1: 0.0933\n",
            "Epoch 191/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0941 - acc: 0.7596 - auc: 0.9230 - precision: 0.1478 - recall: 0.9506 - f1: 0.2439 - val_loss: 4.1145 - val_acc: 0.7431 - val_auc: 0.5477 - val_precision: 0.0575 - val_recall: 0.3182 - val_f1: 0.0917\n",
            "Epoch 192/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1130 - acc: 0.7445 - auc: 0.9206 - precision: 0.1394 - recall: 0.9430 - f1: 0.2344 - val_loss: 3.6518 - val_acc: 0.5753 - val_auc: 0.5517 - val_precision: 0.0513 - val_recall: 0.5000 - val_f1: 0.0903\n",
            "Epoch 193/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1020 - acc: 0.7508 - auc: 0.9218 - precision: 0.1420 - recall: 0.9392 - f1: 0.2366 - val_loss: 4.1081 - val_acc: 0.6856 - val_auc: 0.5474 - val_precision: 0.0563 - val_recall: 0.3939 - val_f1: 0.0916\n",
            "Epoch 194/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0597 - acc: 0.7556 - auc: 0.9306 - precision: 0.1473 - recall: 0.9658 - f1: 0.2472 - val_loss: 4.2513 - val_acc: 0.7596 - val_auc: 0.5500 - val_precision: 0.0643 - val_recall: 0.3333 - val_f1: 0.0969\n",
            "Epoch 195/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0850 - acc: 0.7612 - auc: 0.9262 - precision: 0.1478 - recall: 0.9430 - f1: 0.2451 - val_loss: 3.9565 - val_acc: 0.6546 - val_auc: 0.5461 - val_precision: 0.0563 - val_recall: 0.4394 - val_f1: 0.0926\n",
            "Epoch 196/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0990 - acc: 0.7574 - auc: 0.9233 - precision: 0.1483 - recall: 0.9658 - f1: 0.2439 - val_loss: 4.1062 - val_acc: 0.6757 - val_auc: 0.5451 - val_precision: 0.0526 - val_recall: 0.3788 - val_f1: 0.0849\n",
            "Epoch 197/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.0533 - acc: 0.7688 - auc: 0.9318 - precision: 0.1532 - recall: 0.9544 - f1: 0.2593 - val_loss: 4.2720 - val_acc: 0.6942 - val_auc: 0.5360 - val_precision: 0.0559 - val_recall: 0.3788 - val_f1: 0.0905\n",
            "Epoch 198/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1016 - acc: 0.7407 - auc: 0.9222 - precision: 0.1372 - recall: 0.9392 - f1: 0.2369 - val_loss: 3.9556 - val_acc: 0.6737 - val_auc: 0.5469 - val_precision: 0.0560 - val_recall: 0.4091 - val_f1: 0.0906\n",
            "Epoch 199/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0507 - acc: 0.7829 - auc: 0.9336 - precision: 0.1629 - recall: 0.9658 - f1: 0.2743 - val_loss: 4.1702 - val_acc: 0.6440 - val_auc: 0.5359 - val_precision: 0.0512 - val_recall: 0.4091 - val_f1: 0.0847\n",
            "Epoch 200/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0640 - acc: 0.7512 - auc: 0.9298 - precision: 0.1434 - recall: 0.9506 - f1: 0.2481 - val_loss: 4.1831 - val_acc: 0.7001 - val_auc: 0.5442 - val_precision: 0.0550 - val_recall: 0.3636 - val_f1: 0.0889\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.1657 - acc: 0.7226 - auc: 0.6742 - precision: 0.0991 - recall: 0.5579 - f1: 0.1575\n",
            "2023-07-20 13:46:35.956762\n",
            "CPU times: user 5min 48s, sys: 22.9 s, total: 6min 11s\n",
            "Wall time: 4min 59s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# run model - change parameters as needed to match initial model!!!\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=64, epochs=200,\n",
        "           loss=wbce_custom(40), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = old_model, metrics=['f1'])\n",
        "\n",
        "# Combine new history with old history\n",
        "all_history = get_combined_history([old_history, history.history])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "7nfVA2CX4WUo",
        "outputId": "5de3376f-0492-443b-a764-86c14bc10908"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_train_val_loss(all_history, metrics=['f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMSFvc5Shd2",
        "outputId": "1e772224-f2e4-4cde-cad0-5b35958d59a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Name: nn_51x51_7_10_2317\n"
          ]
        }
      ],
      "source": [
        "# Save Model\n",
        "\n",
        "model_notes = '''dataset: L3 300mx300m CHL, 15x15, num_feature=6, model_type=\"convolution\", batch_size=64, epochs=300,\n",
        "           loss=wbce_custom(50), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = None, metrics=[\"f1\"]. added weight decay. added batch norm.'''\n",
        "save_model(model, history, result, model_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNarrgECQ_75"
      },
      "outputs": [],
      "source": [
        "# input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "# # Getting X_test and y_test\n",
        "# xy_data = get_train_test_val_nn(input_data_,\n",
        "#                       time_site_pairs_train,\n",
        "#                       time_site_pairs_test)\n",
        "\n",
        "# X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "# result = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfJitefrng7f"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TfJ6uxge_KS0"
      },
      "outputs": [],
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wLwO3QLRnlCr"
      },
      "outputs": [],
      "source": [
        "# CV\n",
        "\n",
        "# def objective_function(params):\n",
        "#   results = []\n",
        "\n",
        "#   adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience = params.values()\n",
        "\n",
        "#   for i in range(1):\n",
        "#     time_site_pairs_train = train_val_dict[f'train_{i+1}']\n",
        "#     time_site_pairs_test = train_val_dict[f'val_{i+1}']\n",
        "\n",
        "#     _, _, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "#             loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "#             existing_model = None, metrics=['f1'], verbose=2)\n",
        "\n",
        "#     results.append(-result[-1])\n",
        "\n",
        "#   return {'loss': sum(results)/len(results), 'status': STATUS_OK}\n",
        "\n",
        "# No CV\n",
        "def objective_function(params):\n",
        "\n",
        "  adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience = params.values()\n",
        "\n",
        "  _, _, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "          loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "          existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)\n",
        "\n",
        "  f1_score = result[-1]\n",
        "\n",
        "  if f1_score == np.nan:\n",
        "    result = 0\n",
        "  else:\n",
        "    result = f1_score\n",
        "\n",
        "  return {'loss': -result, 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def run_trials():\n",
        "\n",
        "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
        "    max_trials = 5  # initial max_trials. put something small to not have to wait\n",
        "    \n",
        "    try:  # try to load an already saved trials object, and increase the max\n",
        "        trials = pickle.load(open(\"my_model_final.hyperopt\", \"rb\"))\n",
        "        print(\"Found saved Trials! Loading...\")\n",
        "        max_trials = len(trials.trials) + trials_step\n",
        "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
        "    except:  # create a new trials object and start searching\n",
        "        trials = Trials()\n",
        "\n",
        "    best_param = fmin(objective_function,\n",
        "                  param_hyperopt,\n",
        "                  algo=tpe.suggest,\n",
        "                  max_evals=max_trials,\n",
        "                  trials=trials)\n",
        "\n",
        "    print(\"Best:\", best_param)\n",
        "    \n",
        "    # save the trials object\n",
        "    with open(\"my_model_final\" + \".hyperopt\", \"wb\") as f:\n",
        "        pickle.dump(trials, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "AeHofhAxBFja",
        "outputId": "f9235ed1-4aa0-4aeb-895b-092301ac88fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-30 19:29:20.180379                           \n",
            "{'name': 'Adam', 'learning_rate': 0.022781398447970013, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_1', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_1', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "  0%|          | 0/5 [00:12<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-30 19:29:32.807896: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1/1170 [..............................] - ETA: 15s - loss: 1.8413 - acc: 0.0625 - auc: 0.5000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  31/1170 [..............................] - ETA: 1s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0583 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 1.8085 - acc: 0.0362 - auc: 0.5000 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: 1.8052 - acc: 0.0335 - auc: 0.5000 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 139/1170 [==>...........................] - ETA: 1s - loss: 1.8029 - acc: 0.0317 - auc: 0.5000 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 171/1170 [===>..........................] - ETA: 1s - loss: 1.8033 - acc: 0.0320 - auc: 0.5000 - precision: 0.0320 - recall: 1.0000 - f1: 0.0601\n",
            " 205/1170 [====>.........................] - ETA: 1s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 236/1170 [=====>........................] - ETA: 1s - loss: 1.8017 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 268/1170 [=====>........................] - ETA: 1s - loss: 1.8015 - acc: 0.0306 - auc: 0.5000 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 297/1170 [======>.......................] - ETA: 1s - loss: 1.8008 - acc: 0.0300 - auc: 0.5000 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: 1.8011 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: 1.8012 - acc: 0.0303 - auc: 0.5000 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: 1.8020 - acc: 0.0309 - auc: 0.5000 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: 1.8017 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0579\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: 1.8011 - acc: 0.0303 - auc: 0.5000 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: 1.8007 - acc: 0.0299 - auc: 0.5000 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 1.8005 - acc: 0.0297 - auc: 0.5000 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 571/1170 [=============>................] - ETA: 0s - loss: 1.8006 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 604/1170 [==============>...............] - ETA: 0s - loss: 1.8002 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 636/1170 [===============>..............] - ETA: 0s - loss: 1.8000 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 669/1170 [================>.............] - ETA: 0s - loss: 1.7999 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 703/1170 [=================>............] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: 1.7995 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: 1.8001 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: 1.7996 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 1.7998 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: 1.8001 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: 1.8001 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: 1.8002 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 1.8002 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 19:33:18.309379                           \n",
            "2023-07-30 19:33:18.359034                                                       \n",
            "{'name': 'Adam', 'learning_rate': 0.31169018675714044, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_2_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_2', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_2', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_2', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_2', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_3', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_3', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_3', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_3', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_3', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_4', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_5', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 19:36:06.922159                                                       \n",
            "2023-07-30 19:36:06.988197                                                       \n",
            "{'name': 'Adam', 'learning_rate': 0.0014065921676501979, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_2', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_4_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_4', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_4', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_4', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_5', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_5', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_5', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_6', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 0.9253 - acc: 0.4688 - auc: 0.1667 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 40%|      | 2/5 [10:44<09:51, 197.27s/trial, best loss: -0.0557558499276638]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 0.7889 - acc: 0.4773 - auc: 0.6535 - precision: 0.0473 - recall: 0.7714 - f1: 0.0821             \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: 0.7994 - acc: 0.4619 - auc: 0.6300 - precision: 0.0486 - recall: 0.7500 - f1: 0.0864\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: 0.7973 - acc: 0.4590 - auc: 0.6427 - precision: 0.0462 - recall: 0.7586 - f1: 0.0816\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: 0.7939 - acc: 0.4591 - auc: 0.6411 - precision: 0.0427 - recall: 0.7544 - f1: 0.0760\n",
            " 148/1170 [==>...........................] - ETA: 1s - loss: 0.7962 - acc: 0.4592 - auc: 0.6477 - precision: 0.0464 - recall: 0.7785 - f1: 0.0833\n",
            " 183/1170 [===>..........................] - ETA: 1s - loss: 0.7958 - acc: 0.4539 - auc: 0.6389 - precision: 0.0428 - recall: 0.7622 - f1: 0.0769\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: 0.7957 - acc: 0.4517 - auc: 0.6315 - precision: 0.0412 - recall: 0.7500 - f1: 0.0741\n",
            " 245/1170 [=====>........................] - ETA: 1s - loss: 0.7940 - acc: 0.4540 - auc: 0.6263 - precision: 0.0405 - recall: 0.7417 - f1: 0.0731\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 0.7935 - acc: 0.4543 - auc: 0.6263 - precision: 0.0402 - recall: 0.7473 - f1: 0.0726\n",
            " 319/1170 [=======>......................] - ETA: 1s - loss: 0.7934 - acc: 0.4545 - auc: 0.6246 - precision: 0.0394 - recall: 0.7450 - f1: 0.0712\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: 0.7930 - acc: 0.4573 - auc: 0.6298 - precision: 0.0409 - recall: 0.7529 - f1: 0.0734\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: 0.7928 - acc: 0.4577 - auc: 0.6309 - precision: 0.0408 - recall: 0.7527 - f1: 0.0736\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: 0.7934 - acc: 0.4582 - auc: 0.6303 - precision: 0.0419 - recall: 0.7571 - f1: 0.0759\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: 0.7927 - acc: 0.4588 - auc: 0.6337 - precision: 0.0415 - recall: 0.7629 - f1: 0.0750\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: 0.7924 - acc: 0.4596 - auc: 0.6350 - precision: 0.0411 - recall: 0.7631 - f1: 0.0744\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: 0.7922 - acc: 0.4592 - auc: 0.6340 - precision: 0.0409 - recall: 0.7625 - f1: 0.0741\n",
            " 561/1170 [=============>................] - ETA: 0s - loss: 0.7919 - acc: 0.4589 - auc: 0.6317 - precision: 0.0403 - recall: 0.7542 - f1: 0.0728\n",
            " 595/1170 [==============>...............] - ETA: 0s - loss: 0.7919 - acc: 0.4586 - auc: 0.6271 - precision: 0.0396 - recall: 0.7482 - f1: 0.0715\n",
            " 630/1170 [===============>..............] - ETA: 0s - loss: 0.7919 - acc: 0.4600 - auc: 0.6260 - precision: 0.0399 - recall: 0.7483 - f1: 0.0721\n",
            " 667/1170 [================>.............] - ETA: 0s - loss: 0.7917 - acc: 0.4595 - auc: 0.6282 - precision: 0.0397 - recall: 0.7524 - f1: 0.0719\n",
            " 687/1170 [================>.............] - ETA: 0s - loss: 0.7920 - acc: 0.4582 - auc: 0.6272 - precision: 0.0394 - recall: 0.7543 - f1: 0.0713\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: 0.7921 - acc: 0.4594 - auc: 0.6267 - precision: 0.0400 - recall: 0.7564 - f1: 0.0723\n",
            " 744/1170 [==================>...........] - ETA: 0s - loss: 0.7918 - acc: 0.4591 - auc: 0.6269 - precision: 0.0397 - recall: 0.7587 - f1: 0.0718\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: 0.7923 - acc: 0.4585 - auc: 0.6269 - precision: 0.0398 - recall: 0.7572 - f1: 0.0722\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 0.7930 - acc: 0.4581 - auc: 0.6249 - precision: 0.0395 - recall: 0.7553 - f1: 0.0717\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 0.7932 - acc: 0.4574 - auc: 0.6245 - precision: 0.0394 - recall: 0.7554 - f1: 0.0714\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: 0.7944 - acc: 0.4568 - auc: 0.6236 - precision: 0.0396 - recall: 0.7579 - f1: 0.0719\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 0.7942 - acc: 0.4575 - auc: 0.6241 - precision: 0.0398 - recall: 0.7596 - f1: 0.0723\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 0.7939 - acc: 0.4571 - auc: 0.6229 - precision: 0.0395 - recall: 0.7570 - f1: 0.0718\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: 0.7943 - acc: 0.4563 - auc: 0.6208 - precision: 0.0394 - recall: 0.7576 - f1: 0.0716\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: 0.7938 - acc: 0.4574 - auc: 0.6223 - precision: 0.0397 - recall: 0.7602 - f1: 0.0722\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: 0.7943 - acc: 0.4569 - auc: 0.6244 - precision: 0.0402 - recall: 0.7653 - f1: 0.0730\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 0.7943 - acc: 0.4568 - auc: 0.6259 - precision: 0.0405 - recall: 0.7674 - f1: 0.0733\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: 0.7951 - acc: 0.4553 - auc: 0.6238 - precision: 0.0403 - recall: 0.7636 - f1: 0.0729\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.4553 - auc: 0.6234 - precision: 0.0402 - recall: 0.7620 - f1: 0.0728\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 0.7949 - acc: 0.4552 - auc: 0.6233 - precision: 0.0402 - recall: 0.7613 - f1: 0.0727\n",
            "\n",
            "2023-07-30 19:40:07.397599                                                       \n",
            "2023-07-30 19:40:07.458233                                                        \n",
            "{'name': 'Adam', 'learning_rate': 1.5695863256449222e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_3', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_6_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_6', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_6', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_6', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_6', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_7', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_7', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_7', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_7', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_3', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_12', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_9', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_10', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_11', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 1.8313 - acc: 0.0625 - auc: 0.1333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  31/1170 [..............................] - ETA: 1s - loss: 1.6318 - acc: 0.0312 - auc: 0.6328 - precision: 0.0312 - recall: 1.0000 - f1: 0.0583 \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: 1.6416 - acc: 0.0361 - auc: 0.6439 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  95/1170 [=>............................] - ETA: 1s - loss: 1.6330 - acc: 0.0316 - auc: 0.6491 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 135/1170 [==>...........................] - ETA: 1s - loss: 1.6335 - acc: 0.0319 - auc: 0.6591 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 172/1170 [===>..........................] - ETA: 1s - loss: 1.6353 - acc: 0.0320 - auc: 0.6602 - precision: 0.0320 - recall: 1.0000 - f1: 0.0601\n",
            " 208/1170 [====>.........................] - ETA: 1s - loss: 1.6356 - acc: 0.0312 - auc: 0.6547 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: 1.6356 - acc: 0.0306 - auc: 0.6454 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 278/1170 [======>.......................] - ETA: 1s - loss: 1.6332 - acc: 0.0301 - auc: 0.6529 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 315/1170 [=======>......................] - ETA: 1s - loss: 1.6329 - acc: 0.0299 - auc: 0.6491 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: 1.6325 - acc: 0.0303 - auc: 0.6520 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: 1.6322 - acc: 0.0306 - auc: 0.6526 - precision: 0.0306 - recall: 1.0000 - f1: 0.0577\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 1.6331 - acc: 0.0310 - auc: 0.6495 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: 1.6318 - acc: 0.0302 - auc: 0.6498 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: 1.6315 - acc: 0.0302 - auc: 0.6504 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 524/1170 [============>.................] - ETA: 0s - loss: 1.6314 - acc: 0.0299 - auc: 0.6483 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 563/1170 [=============>................] - ETA: 0s - loss: 1.6316 - acc: 0.0297 - auc: 0.6449 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 601/1170 [==============>...............] - ETA: 0s - loss: 1.6312 - acc: 0.0295 - auc: 0.6428 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 640/1170 [===============>..............] - ETA: 0s - loss: 1.6307 - acc: 0.0295 - auc: 0.6407 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 680/1170 [================>.............] - ETA: 0s - loss: 1.6317 - acc: 0.0292 - auc: 0.6396 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 719/1170 [=================>............] - ETA: 0s - loss: 1.6317 - acc: 0.0295 - auc: 0.6392 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 757/1170 [==================>...........] - ETA: 0s - loss: 1.6312 - acc: 0.0291 - auc: 0.6400 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: 1.6322 - acc: 0.0293 - auc: 0.6370 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: 1.6318 - acc: 0.0289 - auc: 0.6374 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: 1.6326 - acc: 0.0291 - auc: 0.6368 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: 1.6333 - acc: 0.0294 - auc: 0.6338 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: 1.6328 - acc: 0.0291 - auc: 0.6322 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: 1.6334 - acc: 0.0290 - auc: 0.6303 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: 1.6333 - acc: 0.0292 - auc: 0.6304 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: 1.6336 - acc: 0.0294 - auc: 0.6332 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: 1.6333 - acc: 0.0294 - auc: 0.6348 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: 1.6345 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: 1.6346 - acc: 0.0295 - auc: 0.6311 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 1.6347 - acc: 0.0296 - auc: 0.6308 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 19:42:55.152368                                                        \n",
            "2023-07-30 19:42:55.210582                                                        \n",
            "{'name': 'Adam', 'learning_rate': 0.00021982119140504425, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_4', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_8_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_8', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_8', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_8', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_8', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_9', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_9', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_9', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_9', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_4', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_16', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_12', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_17', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_13', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_14', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_19', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 80%|  | 4/5 [21:36<03:17, 197.55s/trial, best loss: -0.07274166494607925]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  26/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9663 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 19:50:59.540708                                                        \n",
            "100%|| 5/5 [21:39<00:00, 259.88s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 5 trials to 6 (+1) trials\n",
            "2023-07-30 19:50:59.604669                           \n",
            "{'name': 'Adam', 'learning_rate': 0.025785122398464748, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_5', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_10_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_10', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_10', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_10', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_10', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_11', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_11', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_11', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_11', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_5', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_20', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_15', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_16', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_22', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_17', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_23', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 0.4154 - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 83%| | 5/6 [07:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0013s). Check your callbacks.\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 0.3817 - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  72/1170 [>.............................] - ETA: 1s - loss: 0.3850 - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  98/1170 [=>............................] - ETA: 1s - loss: 0.3796 - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 124/1170 [==>...........................] - ETA: 1s - loss: 0.3804 - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 165/1170 [===>..........................] - ETA: 1s - loss: 0.3798 - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 205/1170 [====>.........................] - ETA: 1s - loss: 0.3796 - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 247/1170 [=====>........................] - ETA: 1s - loss: 0.3786 - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 288/1170 [======>.......................] - ETA: 1s - loss: 0.3783 - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 0.3783 - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 371/1170 [========>.....................] - ETA: 1s - loss: 0.3784 - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 412/1170 [=========>....................] - ETA: 1s - loss: 0.3791 - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: 0.3788 - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 473/1170 [===========>..................] - ETA: 0s - loss: 0.3785 - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 514/1170 [============>.................] - ETA: 0s - loss: 0.3781 - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 557/1170 [=============>................] - ETA: 0s - loss: 0.3778 - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 591/1170 [==============>...............] - ETA: 0s - loss: 0.3776 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 633/1170 [===============>..............] - ETA: 0s - loss: 0.3775 - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 671/1170 [================>.............] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: 0.3775 - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: 0.3773 - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: 0.3773 - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: 0.3774 - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 0.3776 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "\n",
            "2023-07-30 19:58:01.903356                           \n",
            "100%|| 6/6 [07:02<00:00, 422.36s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 6 trials to 7 (+1) trials\n",
            "2023-07-30 19:58:01.972486                           \n",
            "{'name': 'Adam', 'learning_rate': 0.0006882315854392572, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_6', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_12_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_12', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_12', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_12', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_12', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_13', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_13', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_13', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_13', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_6', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_24', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_18', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_25', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_19', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_26', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_20', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_27', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.3200 - acc: 0.0625 - auc: 0.2500 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  36/1170 [..............................] - ETA: 1s - loss: 2.1152 - acc: 0.0330 - auc: 0.6588 - precision: 0.0330 - recall: 1.0000 - f1: 0.0614 \n",
            "  68/1170 [>.............................] - ETA: 1s - loss: 2.1443 - acc: 0.0363 - auc: 0.6229 - precision: 0.0363 - recall: 1.0000 - f1: 0.0679\n",
            " 108/1170 [=>............................] - ETA: 1s - loss: 2.1284 - acc: 0.0315 - auc: 0.6318 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 135/1170 [==>...........................] - ETA: 1s - loss: 2.1271 - acc: 0.0319 - auc: 0.6504 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 159/1170 [===>..........................] - ETA: 1s - loss: 2.1279 - acc: 0.0324 - auc: 0.6509 - precision: 0.0324 - recall: 1.0000 - f1: 0.0610\n",
            " 199/1170 [====>.........................] - ETA: 1s - loss: 2.1266 - acc: 0.0311 - auc: 0.6525 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 242/1170 [=====>........................] - ETA: 1s - loss: 2.1329 - acc: 0.0306 - auc: 0.6326 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 2.1299 - acc: 0.0301 - auc: 0.6346 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 316/1170 [=======>......................] - ETA: 1s - loss: 2.1312 - acc: 0.0299 - auc: 0.6284 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: 2.1286 - acc: 0.0303 - auc: 0.6329 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: 2.1278 - acc: 0.0302 - auc: 0.6317 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: 2.1275 - acc: 0.0310 - auc: 0.6315 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: 2.1276 - acc: 0.0302 - auc: 0.6344 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 504/1170 [===========>..................] - ETA: 0s - loss: 2.1256 - acc: 0.0299 - auc: 0.6373 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 546/1170 [=============>................] - ETA: 0s - loss: 2.1255 - acc: 0.0297 - auc: 0.6349 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 589/1170 [==============>...............] - ETA: 0s - loss: 2.1266 - acc: 0.0296 - auc: 0.6331 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 631/1170 [===============>..............] - ETA: 0s - loss: 2.1251 - acc: 0.0296 - auc: 0.6323 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 672/1170 [================>.............] - ETA: 0s - loss: 2.1263 - acc: 0.0293 - auc: 0.6313 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 715/1170 [=================>............] - ETA: 0s - loss: 2.1257 - acc: 0.0294 - auc: 0.6305 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: 2.1247 - acc: 0.0291 - auc: 0.6303 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: 2.1251 - acc: 0.0292 - auc: 0.6301 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: 2.1262 - acc: 0.0291 - auc: 0.6280 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: 2.1275 - acc: 0.0291 - auc: 0.6266 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: 2.1282 - acc: 0.0295 - auc: 0.6263 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 2.1282 - acc: 0.0292 - auc: 0.6244 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: 2.1288 - acc: 0.0290 - auc: 0.6237 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: 2.1272 - acc: 0.0292 - auc: 0.6257 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: 2.1274 - acc: 0.0294 - auc: 0.6279 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: 2.1285 - acc: 0.0296 - auc: 0.6276 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: 2.1290 - acc: 0.0296 - auc: 0.6254 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.1282 - acc: 0.0296 - auc: 0.6249 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:01:22.611300                           \n",
            "100%|| 7/7 [03:20<00:00, 200.71s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 7 trials to 8 (+1) trials\n",
            "2023-07-30 20:01:22.687024                           \n",
            "{'name': 'Adam', 'learning_rate': 0.00832166518594385, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_7', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_14_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_14', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_14', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_14', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_14', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_15', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_15', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_15', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_15', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_7', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_28', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_21', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_29', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_22', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_30', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_23', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_31', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  67/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 0s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:08:05.614074                           \n",
            "100%|| 8/8 [06:42<00:00, 402.98s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 8 trials to 9 (+1) trials\n",
            "2023-07-30 20:08:05.665748                           \n",
            "{'name': 'Adam', 'learning_rate': 4.037944046128388e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_8', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_16_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_16', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_16', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_16', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_16', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_17', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_17', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_17', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_17', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_8', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_32', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_24', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_33', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_25', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_34', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_26', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_35', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.1426 - acc: 0.0625 - auc: 0.2167 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 1.9299 - acc: 0.0339 - auc: 0.6509 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 1.9424 - acc: 0.0362 - auc: 0.6232 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            " 102/1170 [=>............................] - ETA: 1s - loss: 1.9357 - acc: 0.0316 - auc: 0.6422 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 140/1170 [==>...........................] - ETA: 1s - loss: 1.9374 - acc: 0.0317 - auc: 0.6565 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 164/1170 [===>..........................] - ETA: 1s - loss: 1.9358 - acc: 0.0316 - auc: 0.6583 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 194/1170 [===>..........................] - ETA: 1s - loss: 1.9347 - acc: 0.0308 - auc: 0.6585 - precision: 0.0308 - recall: 1.0000 - f1: 0.0578\n",
            " 235/1170 [=====>........................] - ETA: 1s - loss: 1.9367 - acc: 0.0307 - auc: 0.6463 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 276/1170 [======>.......................] - ETA: 1s - loss: 1.9361 - acc: 0.0300 - auc: 0.6463 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 301/1170 [======>.......................] - ETA: 1s - loss: 1.9372 - acc: 0.0299 - auc: 0.6461 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: 1.9356 - acc: 0.0301 - auc: 0.6474 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: 1.9338 - acc: 0.0302 - auc: 0.6487 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 1.9340 - acc: 0.0310 - auc: 0.6476 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: 1.9336 - acc: 0.0304 - auc: 0.6496 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 500/1170 [===========>..................] - ETA: 0s - loss: 1.9327 - acc: 0.0301 - auc: 0.6520 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 525/1170 [============>.................] - ETA: 0s - loss: 1.9333 - acc: 0.0298 - auc: 0.6494 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 563/1170 [=============>................] - ETA: 0s - loss: 1.9337 - acc: 0.0297 - auc: 0.6455 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 604/1170 [==============>...............] - ETA: 0s - loss: 1.9337 - acc: 0.0295 - auc: 0.6424 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 645/1170 [===============>..............] - ETA: 0s - loss: 1.9326 - acc: 0.0296 - auc: 0.6417 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 688/1170 [================>.............] - ETA: 0s - loss: 1.9340 - acc: 0.0291 - auc: 0.6410 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 728/1170 [=================>............] - ETA: 0s - loss: 1.9337 - acc: 0.0291 - auc: 0.6391 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 771/1170 [==================>...........] - ETA: 0s - loss: 1.9338 - acc: 0.0291 - auc: 0.6389 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: 1.9346 - acc: 0.0293 - auc: 0.6383 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: 1.9349 - acc: 0.0289 - auc: 0.6378 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: 1.9358 - acc: 0.0291 - auc: 0.6372 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: 1.9362 - acc: 0.0293 - auc: 0.6353 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 1.9357 - acc: 0.0291 - auc: 0.6350 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: 1.9363 - acc: 0.0290 - auc: 0.6339 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 1.9366 - acc: 0.0292 - auc: 0.6345 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: 1.9363 - acc: 0.0293 - auc: 0.6370 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: 1.9360 - acc: 0.0295 - auc: 0.6401 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 1.9369 - acc: 0.0295 - auc: 0.6359 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 1.9368 - acc: 0.0296 - auc: 0.6367 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:10:16.726370                           \n",
            "100%|| 9/9 [02:11<00:00, 131.11s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 9 trials to 10 (+1) trials\n",
            "2023-07-30 20:10:16.781789                            \n",
            "{'name': 'Adam', 'learning_rate': 7.929295038158712e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_9', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_18_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_18', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_18', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_18', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_18', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_19', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_19', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_19', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_19', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_9', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_36', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_27', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_37', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_28', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_38', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_29', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_39', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.8546 - acc: 0.0625 - auc: 0.1583 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  32/1170 [..............................] - ETA: 1s - loss: 2.6180 - acc: 0.0332 - auc: 0.6120 - precision: 0.0332 - recall: 1.0000 - f1: 0.0618 \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: 2.6329 - acc: 0.0361 - auc: 0.6163 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  99/1170 [=>............................] - ETA: 1s - loss: 2.6242 - acc: 0.0312 - auc: 0.6335 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 125/1170 [==>...........................] - ETA: 1s - loss: 2.6221 - acc: 0.0320 - auc: 0.6384 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 144/1170 [==>...........................] - ETA: 1s - loss: 2.6249 - acc: 0.0326 - auc: 0.6423 - precision: 0.0326 - recall: 1.0000 - f1: 0.0613\n",
            " 183/1170 [===>..........................] - ETA: 1s - loss: 2.6291 - acc: 0.0316 - auc: 0.6300 - precision: 0.0316 - recall: 1.0000 - f1: 0.0593\n",
            " 225/1170 [====>.........................] - ETA: 1s - loss: 2.6271 - acc: 0.0306 - auc: 0.6296 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: 2.6269 - acc: 0.0305 - auc: 0.6262 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 272/1170 [=====>........................] - ETA: 1s - loss: 2.6245 - acc: 0.0303 - auc: 0.6295 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 312/1170 [=======>......................] - ETA: 1s - loss: 2.6237 - acc: 0.0295 - auc: 0.6284 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 352/1170 [========>.....................] - ETA: 1s - loss: 2.6216 - acc: 0.0303 - auc: 0.6366 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 2.6201 - acc: 0.0305 - auc: 0.6398 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: 2.6209 - acc: 0.0307 - auc: 0.6388 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: 2.6210 - acc: 0.0301 - auc: 0.6405 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: 2.6201 - acc: 0.0300 - auc: 0.6420 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 542/1170 [============>.................] - ETA: 0s - loss: 2.6212 - acc: 0.0298 - auc: 0.6372 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: 2.6221 - acc: 0.0298 - auc: 0.6311 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 625/1170 [===============>..............] - ETA: 0s - loss: 2.6209 - acc: 0.0293 - auc: 0.6308 - precision: 0.0293 - recall: 1.0000 - f1: 0.0554\n",
            " 667/1170 [================>.............] - ETA: 0s - loss: 2.6206 - acc: 0.0293 - auc: 0.6324 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 690/1170 [================>.............] - ETA: 0s - loss: 2.6212 - acc: 0.0291 - auc: 0.6314 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: 2.6215 - acc: 0.0293 - auc: 0.6293 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: 2.6212 - acc: 0.0291 - auc: 0.6296 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 2.6231 - acc: 0.0293 - auc: 0.6261 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: 2.6236 - acc: 0.0290 - auc: 0.6255 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: 2.6237 - acc: 0.0292 - auc: 0.6261 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 2.6240 - acc: 0.0294 - auc: 0.6258 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: 2.6240 - acc: 0.0291 - auc: 0.6232 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.6246 - acc: 0.0290 - auc: 0.6217 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: 2.6247 - acc: 0.0293 - auc: 0.6223 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 2.6248 - acc: 0.0293 - auc: 0.6246 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.6247 - acc: 0.0295 - auc: 0.6262 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: 2.6263 - acc: 0.0296 - auc: 0.6226 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.6260 - acc: 0.0296 - auc: 0.6225 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:12:19.579171                            \n",
            "100%|| 10/10 [02:02<00:00, 122.85s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 10 trials to 11 (+1) trials\n",
            "2023-07-30 20:12:19.632641                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3963159423976986, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_10', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_20_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_20', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_20', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_20', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_20', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_21', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_21', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_21', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_21', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_10', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_40', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_30', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_41', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_31', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_42', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_32', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_43', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:13:47.997122                             \n",
            "100%|| 11/11 [01:28<00:00, 88.41s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 11 trials to 12 (+1) trials\n",
            "2023-07-30 20:13:48.049130                             \n",
            "{'name': 'Adam', 'learning_rate': 0.44818763769596054, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_11', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_22_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_22', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_22', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_22', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_22', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_23', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_23', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_23', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_23', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_11', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_44', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_33', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_45', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_34', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_46', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_35', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_47', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:16:05.384448                             \n",
            "100%|| 12/12 [02:17<00:00, 137.39s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 12 trials to 13 (+1) trials\n",
            "2023-07-30 20:16:05.437498                             \n",
            "{'name': 'Adam', 'learning_rate': 0.10495461990291796, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_12', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_24_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_24', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_24', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_24', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_24', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_25', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_25', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_25', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_25', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_12', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_48', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_36', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_49', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_37', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_50', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_38', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_51', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:22:16.195214                             \n",
            "100%|| 13/13 [06:10<00:00, 370.81s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 13 trials to 14 (+1) trials\n",
            "2023-07-30 20:22:16.332920                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7857485657607498, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_13', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_26_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_26', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_26', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_26', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_26', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_27', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_27', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_27', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_27', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_13', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_52', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_39', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_53', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_40', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_54', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_41', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_55', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:27:36.373671                             \n",
            "100%|| 14/14 [05:20<00:00, 320.09s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 14 trials to 15 (+1) trials\n",
            "2023-07-30 20:27:36.427525                             \n",
            "{'name': 'Adam', 'learning_rate': 0.025111149518369978, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_14', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_28_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_28', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_28', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_28', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_28', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_29', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_29', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_29', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_29', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_14', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_56', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_42', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_57', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_43', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_58', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_44', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_59', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.6949 - acc: 0.0625 - auc: 0.2333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.6319 - acc: 0.0349 - auc: 0.6540 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: 2.6353 - acc: 0.0370 - auc: 0.6419 - precision: 0.0370 - recall: 1.0000 - f1: 0.0690\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: 2.6344 - acc: 0.0355 - auc: 0.6416 - precision: 0.0355 - recall: 1.0000 - f1: 0.0663\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: 2.6326 - acc: 0.0309 - auc: 0.6380 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 149/1170 [==>...........................] - ETA: 1s - loss: 2.6344 - acc: 0.0331 - auc: 0.6378 - precision: 0.0331 - recall: 1.0000 - f1: 0.0623\n",
            " 192/1170 [===>..........................] - ETA: 1s - loss: 2.6339 - acc: 0.0306 - auc: 0.6324 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 221/1170 [====>.........................] - ETA: 1s - loss: 2.6343 - acc: 0.0307 - auc: 0.6271 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 254/1170 [=====>........................] - ETA: 1s - loss: 2.6338 - acc: 0.0301 - auc: 0.6164 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 288/1170 [======>.......................] - ETA: 1s - loss: 2.6338 - acc: 0.0302 - auc: 0.6152 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: 2.6335 - acc: 0.0297 - auc: 0.6168 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 343/1170 [=======>......................] - ETA: 1s - loss: 2.6329 - acc: 0.0299 - auc: 0.6209 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.6327 - acc: 0.0303 - auc: 0.6214 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: 2.6326 - acc: 0.0310 - auc: 0.6208 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: 2.6323 - acc: 0.0303 - auc: 0.6240 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: 2.6321 - acc: 0.0302 - auc: 0.6248 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 527/1170 [============>.................] - ETA: 0s - loss: 2.6319 - acc: 0.0298 - auc: 0.6212 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 568/1170 [=============>................] - ETA: 0s - loss: 2.6318 - acc: 0.0296 - auc: 0.6209 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 610/1170 [==============>...............] - ETA: 0s - loss: 2.6318 - acc: 0.0295 - auc: 0.6162 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 636/1170 [===============>..............] - ETA: 0s - loss: 2.6316 - acc: 0.0294 - auc: 0.6144 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 670/1170 [================>.............] - ETA: 0s - loss: 2.6316 - acc: 0.0292 - auc: 0.6147 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 710/1170 [=================>............] - ETA: 0s - loss: 2.6319 - acc: 0.0294 - auc: 0.6136 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 2.6315 - acc: 0.0291 - auc: 0.6139 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 778/1170 [==================>...........] - ETA: 0s - loss: 2.6317 - acc: 0.0293 - auc: 0.6135 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: 2.6319 - acc: 0.0292 - auc: 0.6113 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: 2.6317 - acc: 0.0290 - auc: 0.6120 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.6322 - acc: 0.0293 - auc: 0.6111 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: 2.6323 - acc: 0.0293 - auc: 0.6091 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: 2.6322 - acc: 0.0291 - auc: 0.6073 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.6324 - acc: 0.0290 - auc: 0.6047 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: 2.6327 - acc: 0.0292 - auc: 0.6036 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: 2.6327 - acc: 0.0293 - auc: 0.6070 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: 2.6328 - acc: 0.0294 - auc: 0.6084 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 2.6331 - acc: 0.0295 - auc: 0.6039 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.6331 - acc: 0.0296 - auc: 0.6037 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:32:49.952866                             \n",
            "100%|| 15/15 [05:13<00:00, 313.57s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 15 trials to 16 (+1) trials\n",
            "2023-07-30 20:32:50.004461                             \n",
            "{'name': 'Adam', 'learning_rate': 1.848375226671289e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_15', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_30_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_30', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_30', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_30', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_30', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_31', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_31', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_31', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_31', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_15', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_60', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_45', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_61', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_46', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_62', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_47', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_63', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.0311 - acc: 0.4062 - auc: 0.0833 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 0.9404 - acc: 0.4969 - auc: 0.6077 - precision: 0.0444 - recall: 0.7097 - f1: 0.0746             \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: 0.9487 - acc: 0.4849 - auc: 0.6269 - precision: 0.0503 - recall: 0.7385 - f1: 0.0879\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: 0.9482 - acc: 0.4805 - auc: 0.6356 - precision: 0.0508 - recall: 0.7590 - f1: 0.0893\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: 0.9439 - acc: 0.4844 - auc: 0.6427 - precision: 0.0486 - recall: 0.7640 - f1: 0.0856\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: 0.9411 - acc: 0.4815 - auc: 0.6441 - precision: 0.0445 - recall: 0.7619 - f1: 0.0787\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: 0.9419 - acc: 0.4834 - auc: 0.6435 - precision: 0.0457 - recall: 0.7540 - f1: 0.0806\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: 0.9413 - acc: 0.4846 - auc: 0.6501 - precision: 0.0459 - recall: 0.7634 - f1: 0.0814\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: 0.9426 - acc: 0.4809 - auc: 0.6502 - precision: 0.0466 - recall: 0.7793 - f1: 0.0830\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: 0.9429 - acc: 0.4771 - auc: 0.6440 - precision: 0.0454 - recall: 0.7670 - f1: 0.0811\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: 0.9423 - acc: 0.4769 - auc: 0.6378 - precision: 0.0442 - recall: 0.7624 - f1: 0.0788\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: 0.9415 - acc: 0.4760 - auc: 0.6300 - precision: 0.0422 - recall: 0.7399 - f1: 0.0758\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: 0.9408 - acc: 0.4755 - auc: 0.6262 - precision: 0.0408 - recall: 0.7325 - f1: 0.0736\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: 0.9404 - acc: 0.4774 - auc: 0.6288 - precision: 0.0411 - recall: 0.7323 - f1: 0.0737\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 0.9399 - acc: 0.4757 - auc: 0.6260 - precision: 0.0407 - recall: 0.7415 - f1: 0.0728\n",
            " 338/1170 [=======>......................] - ETA: 1s - loss: 0.9403 - acc: 0.4780 - auc: 0.6302 - precision: 0.0419 - recall: 0.7431 - f1: 0.0749\n",
            " 367/1170 [========>.....................] - ETA: 1s - loss: 0.9402 - acc: 0.4783 - auc: 0.6306 - precision: 0.0419 - recall: 0.7416 - f1: 0.0749\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: 0.9398 - acc: 0.4794 - auc: 0.6334 - precision: 0.0422 - recall: 0.7446 - f1: 0.0754\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: 0.9405 - acc: 0.4789 - auc: 0.6319 - precision: 0.0430 - recall: 0.7449 - f1: 0.0768\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: 0.9405 - acc: 0.4782 - auc: 0.6305 - precision: 0.0429 - recall: 0.7458 - f1: 0.0769\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 0.9398 - acc: 0.4787 - auc: 0.6320 - precision: 0.0424 - recall: 0.7494 - f1: 0.0760\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: 0.9399 - acc: 0.4787 - auc: 0.6347 - precision: 0.0427 - recall: 0.7548 - f1: 0.0765\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: 0.9394 - acc: 0.4801 - auc: 0.6322 - precision: 0.0421 - recall: 0.7510 - f1: 0.0755\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: 0.9391 - acc: 0.4793 - auc: 0.6313 - precision: 0.0416 - recall: 0.7490 - f1: 0.0746\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: 0.9391 - acc: 0.4794 - auc: 0.6293 - precision: 0.0413 - recall: 0.7439 - f1: 0.0741\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: 0.9395 - acc: 0.4786 - auc: 0.6253 - precision: 0.0410 - recall: 0.7377 - f1: 0.0735\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: 0.9391 - acc: 0.4788 - auc: 0.6252 - precision: 0.0405 - recall: 0.7359 - f1: 0.0727\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: 0.9386 - acc: 0.4796 - auc: 0.6239 - precision: 0.0401 - recall: 0.7313 - f1: 0.0720\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: 0.9388 - acc: 0.4795 - auc: 0.6249 - precision: 0.0405 - recall: 0.7358 - f1: 0.0728\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 0.9388 - acc: 0.4789 - auc: 0.6249 - precision: 0.0403 - recall: 0.7382 - f1: 0.0724\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: 0.9387 - acc: 0.4795 - auc: 0.6267 - precision: 0.0405 - recall: 0.7417 - f1: 0.0729\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 0.9386 - acc: 0.4795 - auc: 0.6240 - precision: 0.0402 - recall: 0.7372 - f1: 0.0722\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: 0.9387 - acc: 0.4787 - auc: 0.6251 - precision: 0.0401 - recall: 0.7380 - f1: 0.0721\n",
            " 789/1170 [===================>..........] - ETA: 0s - loss: 0.9392 - acc: 0.4783 - auc: 0.6241 - precision: 0.0404 - recall: 0.7368 - f1: 0.0727\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 0.9390 - acc: 0.4779 - auc: 0.6235 - precision: 0.0399 - recall: 0.7342 - f1: 0.0719\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: 0.9387 - acc: 0.4781 - auc: 0.6244 - precision: 0.0397 - recall: 0.7353 - f1: 0.0715\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: 0.9390 - acc: 0.4776 - auc: 0.6231 - precision: 0.0395 - recall: 0.7312 - f1: 0.0712\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: 0.9393 - acc: 0.4775 - auc: 0.6234 - precision: 0.0399 - recall: 0.7350 - f1: 0.0719\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: 0.9398 - acc: 0.4778 - auc: 0.6222 - precision: 0.0402 - recall: 0.7314 - f1: 0.0725\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: 0.9395 - acc: 0.4779 - auc: 0.6216 - precision: 0.0399 - recall: 0.7332 - f1: 0.0719\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: 0.9394 - acc: 0.4781 - auc: 0.6199 - precision: 0.0397 - recall: 0.7299 - f1: 0.0717\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 0.9394 - acc: 0.4772 - auc: 0.6186 - precision: 0.0395 - recall: 0.7297 - f1: 0.0714\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 0.9393 - acc: 0.4772 - auc: 0.6188 - precision: 0.0396 - recall: 0.7319 - f1: 0.0716\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: 0.9391 - acc: 0.4777 - auc: 0.6188 - precision: 0.0396 - recall: 0.7328 - f1: 0.0715\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 0.9394 - acc: 0.4779 - auc: 0.6205 - precision: 0.0401 - recall: 0.7367 - f1: 0.0723\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 0.9398 - acc: 0.4772 - auc: 0.6211 - precision: 0.0404 - recall: 0.7401 - f1: 0.0728\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: 0.9399 - acc: 0.4771 - auc: 0.6225 - precision: 0.0407 - recall: 0.7416 - f1: 0.0732\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: 0.9402 - acc: 0.4761 - auc: 0.6198 - precision: 0.0404 - recall: 0.7370 - f1: 0.0726\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.4762 - auc: 0.6184 - precision: 0.0403 - recall: 0.7341 - f1: 0.0725\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.4765 - auc: 0.6194 - precision: 0.0405 - recall: 0.7359 - f1: 0.0728\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 0.9403 - acc: 0.4763 - auc: 0.6194 - precision: 0.0405 - recall: 0.7360 - f1: 0.0728\n",
            "\n",
            "2023-07-30 20:41:50.481057                             \n",
            "100%|| 16/16 [09:00<00:00, 540.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 16 trials to 17 (+1) trials\n",
            "2023-07-30 20:41:50.543093                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7604976070539027, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_16', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_32_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_32', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_32', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_32', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_32', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_33', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_33', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_33', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_33', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_16', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_64', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_48', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_65', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_49', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_66', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_50', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_67', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  25/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9663 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:49:28.126310                             \n",
            "100%|| 17/17 [07:37<00:00, 457.64s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 17 trials to 18 (+1) trials\n",
            "2023-07-30 20:49:28.184075                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0011482712432998505, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_17', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_34_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_34', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_34', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_34', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_34', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_35', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_35', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_35', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_35', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_17', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_68', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_51', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_69', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_52', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_70', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_53', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_71', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 2.7746 - acc: 0.0625 - auc: 0.2917 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            " 94%|| 17/18 [04:41<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0013s). Check your callbacks.\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.4619 - acc: 0.0349 - auc: 0.6653 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  62/1170 [>.............................] - ETA: 1s - loss: 2.4814 - acc: 0.0363 - auc: 0.6376 - precision: 0.0363 - recall: 1.0000 - f1: 0.0677\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: 2.4755 - acc: 0.0344 - auc: 0.6501 - precision: 0.0344 - recall: 1.0000 - f1: 0.0644\n",
            "  86/1170 [=>............................] - ETA: 2s - loss: 2.4717 - acc: 0.0342 - auc: 0.6484 - precision: 0.0342 - recall: 1.0000 - f1: 0.0640\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: 2.4653 - acc: 0.0312 - auc: 0.6556 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: 2.4629 - acc: 0.0320 - auc: 0.6582 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.4609 - acc: 0.0320 - auc: 0.6625 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: 2.4690 - acc: 0.0331 - auc: 0.6624 - precision: 0.0331 - recall: 1.0000 - f1: 0.0623\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: 2.4679 - acc: 0.0316 - auc: 0.6560 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: 2.4714 - acc: 0.0316 - auc: 0.6490 - precision: 0.0316 - recall: 1.0000 - f1: 0.0593\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: 2.4751 - acc: 0.0311 - auc: 0.6351 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: 2.4744 - acc: 0.0305 - auc: 0.6266 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: 2.4757 - acc: 0.0304 - auc: 0.6279 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: 2.4719 - acc: 0.0301 - auc: 0.6341 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: 2.4744 - acc: 0.0299 - auc: 0.6281 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: 2.4716 - acc: 0.0302 - auc: 0.6318 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: 2.4707 - acc: 0.0301 - auc: 0.6296 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: 2.4701 - acc: 0.0304 - auc: 0.6327 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: 2.4676 - acc: 0.0309 - auc: 0.6323 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: 2.4664 - acc: 0.0306 - auc: 0.6350 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 2.4671 - acc: 0.0304 - auc: 0.6347 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: 2.4663 - acc: 0.0303 - auc: 0.6365 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: 2.4648 - acc: 0.0301 - auc: 0.6402 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: 2.4648 - acc: 0.0298 - auc: 0.6377 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: 2.4659 - acc: 0.0297 - auc: 0.6361 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 2.4647 - acc: 0.0296 - auc: 0.6374 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: 2.4666 - acc: 0.0296 - auc: 0.6317 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: 2.4664 - acc: 0.0295 - auc: 0.6316 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: 2.4649 - acc: 0.0295 - auc: 0.6300 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: 2.4637 - acc: 0.0294 - auc: 0.6310 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: 2.4668 - acc: 0.0293 - auc: 0.6290 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: 2.4657 - acc: 0.0293 - auc: 0.6304 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: 2.4666 - acc: 0.0294 - auc: 0.6283 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: 2.4653 - acc: 0.0291 - auc: 0.6283 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 764/1170 [==================>...........] - ETA: 0s - loss: 2.4646 - acc: 0.0290 - auc: 0.6305 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 2.4655 - acc: 0.0294 - auc: 0.6285 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 2.4666 - acc: 0.0293 - auc: 0.6277 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: 2.4658 - acc: 0.0289 - auc: 0.6287 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: 2.4666 - acc: 0.0289 - auc: 0.6284 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 2.4685 - acc: 0.0292 - auc: 0.6261 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: 2.4680 - acc: 0.0293 - auc: 0.6279 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: 2.4682 - acc: 0.0293 - auc: 0.6276 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: 2.4677 - acc: 0.0291 - auc: 0.6271 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: 2.4683 - acc: 0.0290 - auc: 0.6260 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: 2.4681 - acc: 0.0290 - auc: 0.6263 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: 2.4676 - acc: 0.0290 - auc: 0.6260 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 2.4676 - acc: 0.0292 - auc: 0.6271 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 2.4680 - acc: 0.0293 - auc: 0.6287 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: 2.4675 - acc: 0.0293 - auc: 0.6290 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: 2.4681 - acc: 0.0295 - auc: 0.6292 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 2.4691 - acc: 0.0295 - auc: 0.6270 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.4685 - acc: 0.0296 - auc: 0.6275 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:54:13.251005                             \n",
            "100%|| 18/18 [04:45<00:00, 285.12s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 18 trials to 19 (+1) trials\n",
            "2023-07-30 20:54:13.400530                             \n",
            "{'name': 'Adam', 'learning_rate': 4.595802195320208e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_18', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_36_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_36', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_36', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_36', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_36', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_37', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_37', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_37', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_37', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_18', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_72', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_54', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_73', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_55', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_74', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_56', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_75', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 3.0551 - acc: 0.0625 - auc: 0.2500 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.9156 - acc: 0.0349 - auc: 0.6372 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 2.9285 - acc: 0.0362 - auc: 0.6178 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: 2.9239 - acc: 0.0339 - auc: 0.6332 - precision: 0.0339 - recall: 1.0000 - f1: 0.0634\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: 2.9223 - acc: 0.0309 - auc: 0.6320 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: 2.9203 - acc: 0.0320 - auc: 0.6383 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: 2.9239 - acc: 0.0317 - auc: 0.6424 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: 2.9245 - acc: 0.0314 - auc: 0.6365 - precision: 0.0314 - recall: 1.0000 - f1: 0.0591\n",
            " 208/1170 [====>.........................] - ETA: 1s - loss: 2.9272 - acc: 0.0312 - auc: 0.6220 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 234/1170 [=====>........................] - ETA: 1s - loss: 2.9259 - acc: 0.0304 - auc: 0.6206 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 260/1170 [=====>........................] - ETA: 1s - loss: 2.9248 - acc: 0.0304 - auc: 0.6177 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 298/1170 [======>.......................] - ETA: 1s - loss: 2.9246 - acc: 0.0299 - auc: 0.6209 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 313/1170 [=======>......................] - ETA: 1s - loss: 2.9233 - acc: 0.0297 - auc: 0.6232 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 341/1170 [=======>......................] - ETA: 1s - loss: 2.9218 - acc: 0.0301 - auc: 0.6295 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 378/1170 [========>.....................] - ETA: 1s - loss: 2.9202 - acc: 0.0304 - auc: 0.6341 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 407/1170 [=========>....................] - ETA: 1s - loss: 2.9207 - acc: 0.0309 - auc: 0.6281 - precision: 0.0309 - recall: 1.0000 - f1: 0.0583\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: 2.9189 - acc: 0.0306 - auc: 0.6331 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: 2.9196 - acc: 0.0303 - auc: 0.6333 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: 2.9176 - acc: 0.0300 - auc: 0.6380 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: 2.9185 - acc: 0.0299 - auc: 0.6345 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: 2.9188 - acc: 0.0297 - auc: 0.6315 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: 2.9193 - acc: 0.0298 - auc: 0.6291 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: 2.9187 - acc: 0.0295 - auc: 0.6287 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 644/1170 [===============>..............] - ETA: 0s - loss: 2.9181 - acc: 0.0296 - auc: 0.6285 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 663/1170 [================>.............] - ETA: 0s - loss: 2.9186 - acc: 0.0294 - auc: 0.6289 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 688/1170 [================>.............] - ETA: 0s - loss: 2.9192 - acc: 0.0291 - auc: 0.6287 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 722/1170 [=================>............] - ETA: 0s - loss: 2.9192 - acc: 0.0293 - auc: 0.6270 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: 2.9188 - acc: 0.0291 - auc: 0.6267 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 767/1170 [==================>...........] - ETA: 0s - loss: 2.9188 - acc: 0.0291 - auc: 0.6266 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: 2.9195 - acc: 0.0292 - auc: 0.6277 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: 2.9195 - acc: 0.0291 - auc: 0.6277 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: 2.9200 - acc: 0.0290 - auc: 0.6268 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: 2.9206 - acc: 0.0291 - auc: 0.6270 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: 2.9207 - acc: 0.0293 - auc: 0.6273 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: 2.9209 - acc: 0.0292 - auc: 0.6258 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: 2.9206 - acc: 0.0290 - auc: 0.6239 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: 2.9215 - acc: 0.0290 - auc: 0.6213 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: 2.9217 - acc: 0.0291 - auc: 0.6206 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: 2.9212 - acc: 0.0292 - auc: 0.6228 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: 2.9217 - acc: 0.0293 - auc: 0.6234 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.9216 - acc: 0.0294 - auc: 0.6238 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: 2.9227 - acc: 0.0295 - auc: 0.6207 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 2.9227 - acc: 0.0295 - auc: 0.6199 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.9227 - acc: 0.0296 - auc: 0.6202 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:56:28.184947                             \n",
            "100%|| 19/19 [02:14<00:00, 134.85s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 19 trials to 20 (+1) trials\n",
            "2023-07-30 20:56:28.252025                             \n",
            "{'name': 'Adam', 'learning_rate': 4.770349960026938e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_19', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_38_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_38', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_38', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_38', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_38', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_39', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_39', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_39', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_39', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_19', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_76', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_57', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_77', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_58', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_78', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_59', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_79', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 295/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:00:41.810040                             \n",
            "100%|| 20/20 [04:13<00:00, 253.61s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 20 trials to 21 (+1) trials\n",
            "2023-07-30 21:00:41.867242                             \n",
            "{'name': 'Adam', 'learning_rate': 0.02005822208047446, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_20', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_40_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_40', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_40', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_40', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_40', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_41', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_41', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_41', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_41', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_20', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_80', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_60', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_81', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_61', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_82', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_62', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_83', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.4008 - acc: 0.0625 - auc: 0.5000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  29/1170 [..............................] - ETA: 2s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0564 \n",
            "  39/1170 [>.............................] - ETA: 3s - loss: 1.3654 - acc: 0.0353 - auc: 0.5000 - precision: 0.0353 - recall: 1.0000 - f1: 0.0655\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: 1.3666 - acc: 0.0362 - auc: 0.5000 - precision: 0.0362 - recall: 1.0000 - f1: 0.0675\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: 1.3664 - acc: 0.0360 - auc: 0.5000 - precision: 0.0360 - recall: 1.0000 - f1: 0.0673\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: 1.3611 - acc: 0.0319 - auc: 0.5000 - precision: 0.0319 - recall: 1.0000 - f1: 0.0598\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: 1.3606 - acc: 0.0315 - auc: 0.5000 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: 1.3611 - acc: 0.0320 - auc: 0.5000 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: 1.3620 - acc: 0.0327 - auc: 0.5000 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: 1.3602 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0588\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: 1.3600 - acc: 0.0311 - auc: 0.5000 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: 1.3595 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: 1.3594 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 300/1170 [======>.......................] - ETA: 1s - loss: 1.3584 - acc: 0.0299 - auc: 0.5000 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 326/1170 [=======>......................] - ETA: 1s - loss: 1.3587 - acc: 0.0301 - auc: 0.5000 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 359/1170 [========>.....................] - ETA: 1s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: 1.3590 - acc: 0.0304 - auc: 0.5000 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: 1.3598 - acc: 0.0310 - auc: 0.5000 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: 1.3591 - acc: 0.0304 - auc: 0.5000 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: 1.3585 - acc: 0.0300 - auc: 0.5000 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: 1.3582 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: 1.3583 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: 1.3576 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 671/1170 [================>.............] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 697/1170 [================>.............] - ETA: 0s - loss: 1.3576 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 1.3574 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 1.3574 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: 1.3576 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: 1.3572 - acc: 0.0289 - auc: 0.5000 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: 1.3578 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: 1.3573 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: 1.3573 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: 1.3571 - acc: 0.0289 - auc: 0.5000 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.3577 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: 1.3578 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: 1.3580 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 1.3580 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:04:57.715845                             \n",
            "100%|| 21/21 [04:15<00:00, 255.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 21 trials to 22 (+1) trials\n",
            "2023-07-30 21:04:57.771782                             \n",
            "{'name': 'Adam', 'learning_rate': 4.1871180276166344e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_21', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_42_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_42', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_42', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_42', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_42', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_43', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_43', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_43', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_43', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_21', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_84', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_63', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_85', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_64', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_86', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_65', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_87', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 3.4983 - acc: 0.0625 - auc: 0.1750 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 3.3110 - acc: 0.0339 - auc: 0.6359 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: 3.3275 - acc: 0.0347 - auc: 0.6111 - precision: 0.0347 - recall: 1.0000 - f1: 0.0647\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: 3.3202 - acc: 0.0370 - auc: 0.6303 - precision: 0.0370 - recall: 1.0000 - f1: 0.0690\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: 3.3214 - acc: 0.0360 - auc: 0.6287 - precision: 0.0360 - recall: 1.0000 - f1: 0.0673\n",
            "  90/1170 [=>............................] - ETA: 3s - loss: 3.3157 - acc: 0.0326 - auc: 0.6238 - precision: 0.0326 - recall: 1.0000 - f1: 0.0611\n",
            " 106/1170 [=>............................] - ETA: 3s - loss: 3.3181 - acc: 0.0315 - auc: 0.6283 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 116/1170 [=>............................] - ETA: 3s - loss: 3.3153 - acc: 0.0315 - auc: 0.6268 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 3.3155 - acc: 0.0327 - auc: 0.6435 - precision: 0.0327 - recall: 1.0000 - f1: 0.0616\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: 3.3160 - acc: 0.0316 - auc: 0.6412 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: 3.3170 - acc: 0.0309 - auc: 0.6362 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: 3.3182 - acc: 0.0305 - auc: 0.6293 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: 3.3176 - acc: 0.0306 - auc: 0.6262 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 3.3163 - acc: 0.0307 - auc: 0.6278 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: 3.3169 - acc: 0.0302 - auc: 0.6264 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 3.3154 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: 3.3143 - acc: 0.0304 - auc: 0.6334 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: 3.3130 - acc: 0.0303 - auc: 0.6364 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 382/1170 [========>.....................] - ETA: 1s - loss: 3.3116 - acc: 0.0302 - auc: 0.6379 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: 3.3125 - acc: 0.0310 - auc: 0.6320 - precision: 0.0310 - recall: 1.0000 - f1: 0.0584\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: 3.3120 - acc: 0.0308 - auc: 0.6348 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: 3.3120 - acc: 0.0305 - auc: 0.6376 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: 3.3122 - acc: 0.0303 - auc: 0.6396 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: 3.3118 - acc: 0.0299 - auc: 0.6369 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: 3.3128 - acc: 0.0299 - auc: 0.6334 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: 3.3124 - acc: 0.0297 - auc: 0.6350 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 3.3131 - acc: 0.0295 - auc: 0.6314 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: 3.3134 - acc: 0.0296 - auc: 0.6309 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: 3.3122 - acc: 0.0294 - auc: 0.6306 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: 3.3122 - acc: 0.0294 - auc: 0.6307 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 3.3136 - acc: 0.0292 - auc: 0.6298 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 3.3133 - acc: 0.0292 - auc: 0.6298 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: 3.3129 - acc: 0.0293 - auc: 0.6292 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 3.3123 - acc: 0.0291 - auc: 0.6302 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: 3.3134 - acc: 0.0293 - auc: 0.6298 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: 3.3146 - acc: 0.0291 - auc: 0.6277 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: 3.3155 - acc: 0.0290 - auc: 0.6272 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: 3.3152 - acc: 0.0291 - auc: 0.6282 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: 3.3164 - acc: 0.0293 - auc: 0.6266 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: 3.3168 - acc: 0.0294 - auc: 0.6250 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: 3.3171 - acc: 0.0290 - auc: 0.6237 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: 3.3167 - acc: 0.0291 - auc: 0.6223 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: 3.3177 - acc: 0.0290 - auc: 0.6205 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: 3.3175 - acc: 0.0289 - auc: 0.6204 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 3.3172 - acc: 0.0292 - auc: 0.6231 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: 3.3172 - acc: 0.0293 - auc: 0.6241 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 3.3182 - acc: 0.0295 - auc: 0.6240 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: 3.3189 - acc: 0.0296 - auc: 0.6208 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 3.3187 - acc: 0.0295 - auc: 0.6207 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 3.3187 - acc: 0.0296 - auc: 0.6211 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:07:16.683478                             \n",
            "100%|| 22/22 [02:18<00:00, 138.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 22 trials to 23 (+1) trials\n",
            "2023-07-30 21:07:16.739145                             \n",
            "{'name': 'Adam', 'learning_rate': 6.502045086878626e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_22', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_44_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_44', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_44', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_44', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_44', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_45', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_45', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_45', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_45', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_22', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_88', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_66', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_89', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_67', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_90', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_68', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_91', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.3707 - acc: 0.0625 - auc: 0.1917 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 2.1581 - acc: 0.0323 - auc: 0.6266 - precision: 0.0323 - recall: 1.0000 - f1: 0.0602 \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: 2.1644 - acc: 0.0349 - auc: 0.6363 - precision: 0.0349 - recall: 1.0000 - f1: 0.0649\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: 2.1720 - acc: 0.0361 - auc: 0.6225 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  73/1170 [>.............................] - ETA: 3s - loss: 2.1666 - acc: 0.0355 - auc: 0.6385 - precision: 0.0355 - recall: 1.0000 - f1: 0.0664\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: 2.1560 - acc: 0.0316 - auc: 0.6449 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: 2.1576 - acc: 0.0315 - auc: 0.6457 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.1548 - acc: 0.0320 - auc: 0.6544 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: 2.1590 - acc: 0.0326 - auc: 0.6560 - precision: 0.0326 - recall: 1.0000 - f1: 0.0614\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: 2.1588 - acc: 0.0311 - auc: 0.6548 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: 2.1605 - acc: 0.0312 - auc: 0.6502 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: 2.1618 - acc: 0.0304 - auc: 0.6404 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: 2.1608 - acc: 0.0301 - auc: 0.6433 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: 2.1604 - acc: 0.0301 - auc: 0.6422 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: 2.1623 - acc: 0.0299 - auc: 0.6356 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: 2.1598 - acc: 0.0297 - auc: 0.6369 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: 2.1590 - acc: 0.0301 - auc: 0.6394 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 371/1170 [========>.....................] - ETA: 1s - loss: 2.1578 - acc: 0.0302 - auc: 0.6439 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: 2.1576 - acc: 0.0309 - auc: 0.6438 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: 2.1576 - acc: 0.0309 - auc: 0.6414 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 2.1573 - acc: 0.0303 - auc: 0.6432 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: 2.1572 - acc: 0.0303 - auc: 0.6426 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: 2.1568 - acc: 0.0300 - auc: 0.6449 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 2.1552 - acc: 0.0299 - auc: 0.6455 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 2.1567 - acc: 0.0297 - auc: 0.6414 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 2.1565 - acc: 0.0296 - auc: 0.6420 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: 2.1574 - acc: 0.0298 - auc: 0.6375 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: 2.1567 - acc: 0.0295 - auc: 0.6365 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 2.1553 - acc: 0.0296 - auc: 0.6363 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 2.1571 - acc: 0.0292 - auc: 0.6348 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: 2.1570 - acc: 0.0294 - auc: 0.6354 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 743/1170 [==================>...........] - ETA: 0s - loss: 2.1565 - acc: 0.0291 - auc: 0.6355 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: 2.1570 - acc: 0.0291 - auc: 0.6339 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 2.1577 - acc: 0.0294 - auc: 0.6326 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 2.1586 - acc: 0.0291 - auc: 0.6309 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: 2.1581 - acc: 0.0288 - auc: 0.6317 - precision: 0.0288 - recall: 1.0000 - f1: 0.0543\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: 2.1590 - acc: 0.0292 - auc: 0.6320 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.1595 - acc: 0.0293 - auc: 0.6322 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 2.1589 - acc: 0.0292 - auc: 0.6328 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: 2.1590 - acc: 0.0292 - auc: 0.6319 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: 2.1587 - acc: 0.0290 - auc: 0.6319 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: 2.1601 - acc: 0.0291 - auc: 0.6291 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: 2.1590 - acc: 0.0290 - auc: 0.6299 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: 2.1591 - acc: 0.0293 - auc: 0.6331 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.1586 - acc: 0.0294 - auc: 0.6358 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: 2.1598 - acc: 0.0296 - auc: 0.6331 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: 2.1602 - acc: 0.0295 - auc: 0.6307 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.1600 - acc: 0.0296 - auc: 0.6320 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:09:35.687468                             \n",
            "100%|| 23/23 [02:19<00:00, 139.00s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 23 trials to 24 (+1) trials\n",
            "2023-07-30 21:09:35.744275                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0003409035850234096, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_23', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_46_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_46', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_46', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_46', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_46', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_47', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_47', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_47', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_47', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_23', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_92', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_69', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_93', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_70', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_94', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_71', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_95', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: 0.6453 - acc: 0.9375 - auc: 0.1000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  37/1170 [..............................] - ETA: 1s - loss: 0.5459 - acc: 0.9645 - auc: 0.6781 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: 0.5501 - acc: 0.9622 - auc: 0.6706 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: 0.5497 - acc: 0.9640 - auc: 0.6533 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: 0.5425 - acc: 0.9688 - auc: 0.6599 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: 0.5427 - acc: 0.9682 - auc: 0.6586 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: 0.5422 - acc: 0.9680 - auc: 0.6682 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: 0.5450 - acc: 0.9666 - auc: 0.6627 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: 0.5428 - acc: 0.9686 - auc: 0.6591 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: 0.5425 - acc: 0.9688 - auc: 0.6576 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: 0.5421 - acc: 0.9695 - auc: 0.6491 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: 0.5420 - acc: 0.9695 - auc: 0.6397 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 0.5418 - acc: 0.9699 - auc: 0.6453 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 307/1170 [======>.......................] - ETA: 1s - loss: 0.5425 - acc: 0.9702 - auc: 0.6418 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: 0.5420 - acc: 0.9697 - auc: 0.6452 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: 0.5415 - acc: 0.9697 - auc: 0.6497 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 0.5416 - acc: 0.9695 - auc: 0.6485 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 0.5420 - acc: 0.9690 - auc: 0.6450 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 0.5413 - acc: 0.9696 - auc: 0.6458 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 0.5415 - acc: 0.9697 - auc: 0.6447 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: 0.5410 - acc: 0.9700 - auc: 0.6478 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: 0.5407 - acc: 0.9700 - auc: 0.6470 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: 0.5408 - acc: 0.9703 - auc: 0.6440 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: 0.5408 - acc: 0.9702 - auc: 0.6430 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: 0.5410 - acc: 0.9703 - auc: 0.6405 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 0.5407 - acc: 0.9705 - auc: 0.6417 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: 0.5406 - acc: 0.9707 - auc: 0.6382 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 0.5405 - acc: 0.9704 - auc: 0.6383 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: 0.5406 - acc: 0.9706 - auc: 0.6385 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: 0.5408 - acc: 0.9708 - auc: 0.6378 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: 0.5407 - acc: 0.9707 - auc: 0.6379 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 727/1170 [=================>............] - ETA: 0s - loss: 0.5407 - acc: 0.9708 - auc: 0.6368 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 757/1170 [==================>...........] - ETA: 0s - loss: 0.5406 - acc: 0.9709 - auc: 0.6369 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: 0.5409 - acc: 0.9708 - auc: 0.6362 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: 0.5411 - acc: 0.9708 - auc: 0.6354 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: 0.5411 - acc: 0.9710 - auc: 0.6347 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: 0.5413 - acc: 0.9710 - auc: 0.6348 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 0.5416 - acc: 0.9708 - auc: 0.6345 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: 0.5420 - acc: 0.9706 - auc: 0.6359 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: 0.5417 - acc: 0.9709 - auc: 0.6342 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 0.5415 - acc: 0.9709 - auc: 0.6350 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: 0.5417 - acc: 0.9710 - auc: 0.6323 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: 0.5415 - acc: 0.9710 - auc: 0.6356 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: 0.5417 - acc: 0.9708 - auc: 0.6345 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: 0.5419 - acc: 0.9707 - auc: 0.6370 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: 0.5418 - acc: 0.9707 - auc: 0.6372 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 0.5424 - acc: 0.9705 - auc: 0.6364 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.9704 - auc: 0.6342 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 0.5424 - acc: 0.9704 - auc: 0.6344 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "\n",
            "2023-07-30 21:11:30.955394                             \n",
            "100%|| 24/24 [01:55<00:00, 115.27s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 24 trials to 25 (+1) trials\n",
            "2023-07-30 21:11:31.014670                             \n",
            "{'name': 'Adam', 'learning_rate': 5.545735597680928e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_24', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_48_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_48', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_48', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_48', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_48', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_49', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_49', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_49', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_49', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_24', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_96', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_72', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_97', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_73', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_98', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_74', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_99', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 2.7473 - acc: 0.0625 - auc: 0.1083 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  37/1170 [..............................] - ETA: 1s - loss: 2.4859 - acc: 0.0355 - auc: 0.6309 - precision: 0.0355 - recall: 1.0000 - f1: 0.0657 \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: 2.4967 - acc: 0.0362 - auc: 0.6149 - precision: 0.0362 - recall: 1.0000 - f1: 0.0675\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: 2.4914 - acc: 0.0365 - auc: 0.6367 - precision: 0.0365 - recall: 1.0000 - f1: 0.0683\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: 2.4804 - acc: 0.0326 - auc: 0.6417 - precision: 0.0326 - recall: 1.0000 - f1: 0.0611\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: 2.4809 - acc: 0.0315 - auc: 0.6468 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 116/1170 [=>............................] - ETA: 2s - loss: 2.4783 - acc: 0.0315 - auc: 0.6478 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: 2.4786 - acc: 0.0319 - auc: 0.6628 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: 2.4814 - acc: 0.0320 - auc: 0.6561 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: 2.4843 - acc: 0.0314 - auc: 0.6515 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: 2.4815 - acc: 0.0311 - auc: 0.6527 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: 2.4858 - acc: 0.0307 - auc: 0.6410 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: 2.4847 - acc: 0.0304 - auc: 0.6375 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 2.4836 - acc: 0.0301 - auc: 0.6403 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: 2.4853 - acc: 0.0297 - auc: 0.6329 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 2.4849 - acc: 0.0301 - auc: 0.6371 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: 2.4828 - acc: 0.0303 - auc: 0.6388 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: 2.4829 - acc: 0.0303 - auc: 0.6362 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: 2.4823 - acc: 0.0309 - auc: 0.6361 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: 2.4817 - acc: 0.0303 - auc: 0.6395 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: 2.4824 - acc: 0.0301 - auc: 0.6401 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: 2.4815 - acc: 0.0301 - auc: 0.6420 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: 2.4796 - acc: 0.0300 - auc: 0.6419 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: 2.4804 - acc: 0.0296 - auc: 0.6392 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: 2.4811 - acc: 0.0296 - auc: 0.6379 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: 2.4825 - acc: 0.0297 - auc: 0.6362 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: 2.4813 - acc: 0.0295 - auc: 0.6369 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: 2.4810 - acc: 0.0295 - auc: 0.6340 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: 2.4803 - acc: 0.0295 - auc: 0.6348 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: 2.4828 - acc: 0.0292 - auc: 0.6345 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: 2.4821 - acc: 0.0293 - auc: 0.6338 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: 2.4821 - acc: 0.0291 - auc: 0.6330 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 774/1170 [==================>...........] - ETA: 0s - loss: 2.4828 - acc: 0.0292 - auc: 0.6322 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: 2.4851 - acc: 0.0293 - auc: 0.6296 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: 2.4844 - acc: 0.0290 - auc: 0.6307 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: 2.4855 - acc: 0.0292 - auc: 0.6296 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: 2.4855 - acc: 0.0295 - auc: 0.6297 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: 2.4856 - acc: 0.0292 - auc: 0.6291 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: 2.4856 - acc: 0.0292 - auc: 0.6277 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: 2.4857 - acc: 0.0290 - auc: 0.6283 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 2.4866 - acc: 0.0290 - auc: 0.6264 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: 2.4859 - acc: 0.0292 - auc: 0.6276 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: 2.4858 - acc: 0.0293 - auc: 0.6302 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: 2.4855 - acc: 0.0294 - auc: 0.6314 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 2.4867 - acc: 0.0295 - auc: 0.6308 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: 2.4872 - acc: 0.0296 - auc: 0.6283 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: 2.4870 - acc: 0.0295 - auc: 0.6281 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.4868 - acc: 0.0296 - auc: 0.6289 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:13:36.471941                             \n",
            "100%|| 25/25 [02:05<00:00, 125.51s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 25 trials to 26 (+1) trials\n",
            "2023-07-30 21:13:36.531085                             \n",
            "{'name': 'Adam', 'learning_rate': 4.085211163979461e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_25', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_50_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_50', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_50', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_50', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_50', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_51', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_51', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_51', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_51', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_25', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_100', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_75', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_101', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_76', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_102', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_77', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_103', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.8521 - acc: 0.0625 - auc: 0.2583 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 2.6500 - acc: 0.0331 - auc: 0.6171 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: 2.6665 - acc: 0.0370 - auc: 0.6220 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  68/1170 [>.............................] - ETA: 2s - loss: 2.6775 - acc: 0.0363 - auc: 0.6173 - precision: 0.0363 - recall: 1.0000 - f1: 0.0679\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: 2.6655 - acc: 0.0319 - auc: 0.6289 - precision: 0.0319 - recall: 1.0000 - f1: 0.0598\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: 2.6598 - acc: 0.0323 - auc: 0.6411 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: 2.6656 - acc: 0.0326 - auc: 0.6467 - precision: 0.0326 - recall: 1.0000 - f1: 0.0614\n",
            " 188/1170 [===>..........................] - ETA: 1s - loss: 2.6671 - acc: 0.0311 - auc: 0.6409 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: 2.6694 - acc: 0.0310 - auc: 0.6352 - precision: 0.0310 - recall: 1.0000 - f1: 0.0582\n",
            " 231/1170 [====>.........................] - ETA: 1s - loss: 2.6690 - acc: 0.0306 - auc: 0.6316 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 263/1170 [=====>........................] - ETA: 1s - loss: 2.6667 - acc: 0.0307 - auc: 0.6338 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 284/1170 [======>.......................] - ETA: 1s - loss: 2.6674 - acc: 0.0303 - auc: 0.6355 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 296/1170 [======>.......................] - ETA: 1s - loss: 2.6705 - acc: 0.0301 - auc: 0.6300 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 312/1170 [=======>......................] - ETA: 1s - loss: 2.6692 - acc: 0.0295 - auc: 0.6316 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 2.6679 - acc: 0.0301 - auc: 0.6333 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 354/1170 [========>.....................] - ETA: 1s - loss: 2.6667 - acc: 0.0304 - auc: 0.6329 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.6665 - acc: 0.0303 - auc: 0.6330 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: 2.6652 - acc: 0.0309 - auc: 0.6346 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: 2.6631 - acc: 0.0310 - auc: 0.6362 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: 2.6651 - acc: 0.0304 - auc: 0.6341 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: 2.6662 - acc: 0.0303 - auc: 0.6348 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: 2.6667 - acc: 0.0302 - auc: 0.6375 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: 2.6653 - acc: 0.0299 - auc: 0.6373 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: 2.6658 - acc: 0.0298 - auc: 0.6350 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: 2.6661 - acc: 0.0299 - auc: 0.6314 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: 2.6656 - acc: 0.0297 - auc: 0.6312 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: 2.6668 - acc: 0.0295 - auc: 0.6273 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 2.6668 - acc: 0.0293 - auc: 0.6262 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 2.6655 - acc: 0.0295 - auc: 0.6271 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: 2.6683 - acc: 0.0293 - auc: 0.6249 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: 2.6684 - acc: 0.0292 - auc: 0.6262 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: 2.6672 - acc: 0.0294 - auc: 0.6243 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: 2.6668 - acc: 0.0291 - auc: 0.6236 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: 2.6661 - acc: 0.0291 - auc: 0.6254 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: 2.6665 - acc: 0.0293 - auc: 0.6249 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: 2.6687 - acc: 0.0293 - auc: 0.6223 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: 2.6682 - acc: 0.0289 - auc: 0.6233 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.6704 - acc: 0.0291 - auc: 0.6219 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 2.6709 - acc: 0.0292 - auc: 0.6212 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: 2.6704 - acc: 0.0294 - auc: 0.6211 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: 2.6695 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: 2.6699 - acc: 0.0290 - auc: 0.6206 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: 2.6707 - acc: 0.0290 - auc: 0.6187 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: 2.6693 - acc: 0.0291 - auc: 0.6207 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: 2.6696 - acc: 0.0293 - auc: 0.6225 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: 2.6687 - acc: 0.0293 - auc: 0.6237 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: 2.6687 - acc: 0.0295 - auc: 0.6248 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: 2.6704 - acc: 0.0295 - auc: 0.6213 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: 2.6696 - acc: 0.0295 - auc: 0.6212 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.6697 - acc: 0.0296 - auc: 0.6212 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:16:37.407854                             \n",
            "100%|| 26/26 [03:00<00:00, 180.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 26 trials to 27 (+1) trials\n",
            "2023-07-30 21:16:37.470560                             \n",
            "{'name': 'Adam', 'learning_rate': 1.0614885222476534e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_26', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_52_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_52', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_52', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_52', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_52', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_53', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_53', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_53', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_53', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_26', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_104', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_78', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_105', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_79', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_106', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_80', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_107', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.5596 - acc: 0.9062 - auc: 0.1917 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  31/1170 [..............................] - ETA: 2s - loss: 1.4715 - acc: 0.9627 - auc: 0.5949 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: 1.4770 - acc: 0.9598 - auc: 0.6117 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  60/1170 [>.............................] - ETA: 3s - loss: 1.4801 - acc: 0.9583 - auc: 0.6361 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  71/1170 [>.............................] - ETA: 3s - loss: 1.4801 - acc: 0.9586 - auc: 0.6339 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: 1.4754 - acc: 0.9601 - auc: 0.6301 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: 1.4743 - acc: 0.9622 - auc: 0.6275 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: 1.4737 - acc: 0.9622 - auc: 0.6385 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 1.4755 - acc: 0.9617 - auc: 0.6419 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: 1.4744 - acc: 0.9622 - auc: 0.6358 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: 1.4750 - acc: 0.9632 - auc: 0.6304 - precision: 0.0263 - recall: 0.0050 - f1: 0.0025            \n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 1.4741 - acc: 0.9635 - auc: 0.6227 - precision: 0.0227 - recall: 0.0045 - f1: 0.0022\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: 1.4737 - acc: 0.9637 - auc: 0.6194 - precision: 0.0204 - recall: 0.0042 - f1: 0.0020\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: 1.4740 - acc: 0.9636 - auc: 0.6184 - precision: 0.0204 - recall: 0.0040 - f1: 0.0019\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: 1.4728 - acc: 0.9640 - auc: 0.6201 - precision: 0.0196 - recall: 0.0038 - f1: 0.0019\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: 1.4730 - acc: 0.9642 - auc: 0.6209 - precision: 0.0192 - recall: 0.0038 - f1: 0.0018\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: 1.4731 - acc: 0.9643 - auc: 0.6201 - precision: 0.0189 - recall: 0.0036 - f1: 0.0017\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 1.4721 - acc: 0.9652 - auc: 0.6233 - precision: 0.0185 - recall: 0.0034 - f1: 0.0016\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: 1.4723 - acc: 0.9649 - auc: 0.6286 - precision: 0.0351 - recall: 0.0063 - f1: 0.0030\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: 1.4722 - acc: 0.9647 - auc: 0.6282 - precision: 0.0333 - recall: 0.0059 - f1: 0.0028\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: 1.4719 - acc: 0.9648 - auc: 0.6284 - precision: 0.0317 - recall: 0.0055 - f1: 0.0027\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: 1.4719 - acc: 0.9646 - auc: 0.6307 - precision: 0.0441 - recall: 0.0078 - f1: 0.0051\n",
            " 420/1170 [=========>....................] - ETA: 2s - loss: 1.4723 - acc: 0.9644 - auc: 0.6273 - precision: 0.0435 - recall: 0.0072 - f1: 0.0048\n",
            " 441/1170 [==========>...................] - ETA: 2s - loss: 1.4719 - acc: 0.9649 - auc: 0.6289 - precision: 0.0423 - recall: 0.0070 - f1: 0.0045\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: 1.4720 - acc: 0.9651 - auc: 0.6313 - precision: 0.0526 - recall: 0.0087 - f1: 0.0063\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: 1.4714 - acc: 0.9653 - auc: 0.6312 - precision: 0.0488 - recall: 0.0084 - f1: 0.0060\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: 1.4713 - acc: 0.9654 - auc: 0.6283 - precision: 0.0449 - recall: 0.0079 - f1: 0.0057\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: 1.4716 - acc: 0.9653 - auc: 0.6252 - precision: 0.0625 - recall: 0.0115 - f1: 0.0080\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: 1.4714 - acc: 0.9656 - auc: 0.6237 - precision: 0.0619 - recall: 0.0110 - f1: 0.0077\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: 1.4712 - acc: 0.9660 - auc: 0.6210 - precision: 0.0619 - recall: 0.0107 - f1: 0.0074\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: 1.4713 - acc: 0.9660 - auc: 0.6214 - precision: 0.0612 - recall: 0.0103 - f1: 0.0072\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: 1.4708 - acc: 0.9664 - auc: 0.6211 - precision: 0.0612 - recall: 0.0103 - f1: 0.0071\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: 1.4709 - acc: 0.9662 - auc: 0.6216 - precision: 0.0606 - recall: 0.0101 - f1: 0.0070\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: 1.4710 - acc: 0.9662 - auc: 0.6225 - precision: 0.0600 - recall: 0.0098 - f1: 0.0068\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 1.4708 - acc: 0.9664 - auc: 0.6225 - precision: 0.0561 - recall: 0.0095 - f1: 0.0065\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: 1.4709 - acc: 0.9664 - auc: 0.6233 - precision: 0.0550 - recall: 0.0091 - f1: 0.0063\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: 1.4707 - acc: 0.9666 - auc: 0.6206 - precision: 0.0531 - recall: 0.0087 - f1: 0.0060\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: 1.4706 - acc: 0.9666 - auc: 0.6223 - precision: 0.0517 - recall: 0.0085 - f1: 0.0058\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: 1.4709 - acc: 0.9664 - auc: 0.6221 - precision: 0.0500 - recall: 0.0082 - f1: 0.0056\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: 1.4714 - acc: 0.9664 - auc: 0.6190 - precision: 0.0492 - recall: 0.0079 - f1: 0.0054\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: 1.4710 - acc: 0.9669 - auc: 0.6196 - precision: 0.0480 - recall: 0.0077 - f1: 0.0052\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 1.4713 - acc: 0.9668 - auc: 0.6204 - precision: 0.0551 - recall: 0.0086 - f1: 0.0056\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: 1.4718 - acc: 0.9666 - auc: 0.6191 - precision: 0.0534 - recall: 0.0083 - f1: 0.0055\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: 1.4718 - acc: 0.9666 - auc: 0.6180 - precision: 0.0530 - recall: 0.0081 - f1: 0.0054\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: 1.4715 - acc: 0.9668 - auc: 0.6175 - precision: 0.0522 - recall: 0.0080 - f1: 0.0052\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: 1.4713 - acc: 0.9670 - auc: 0.6159 - precision: 0.0519 - recall: 0.0078 - f1: 0.0051\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 1.4716 - acc: 0.9670 - auc: 0.6139 - precision: 0.0511 - recall: 0.0077 - f1: 0.0050\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: 1.4715 - acc: 0.9670 - auc: 0.6160 - precision: 0.0567 - recall: 0.0086 - f1: 0.0056\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: 1.4718 - acc: 0.9670 - auc: 0.6138 - precision: 0.0556 - recall: 0.0083 - f1: 0.0054\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: 1.4720 - acc: 0.9667 - auc: 0.6161 - precision: 0.0541 - recall: 0.0080 - f1: 0.0052\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: 1.4721 - acc: 0.9668 - auc: 0.6171 - precision: 0.0596 - recall: 0.0088 - f1: 0.0057\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: 1.4722 - acc: 0.9667 - auc: 0.6178 - precision: 0.0588 - recall: 0.0086 - f1: 0.0057\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: 1.4725 - acc: 0.9667 - auc: 0.6145 - precision: 0.0573 - recall: 0.0085 - f1: 0.0055\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.4725 - acc: 0.9666 - auc: 0.6133 - precision: 0.0549 - recall: 0.0082 - f1: 0.0054\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.4726 - acc: 0.9665 - auc: 0.6138 - precision: 0.0539 - recall: 0.0081 - f1: 0.0053\n",
            "\n",
            "2023-07-30 21:19:54.910569                             \n",
            "100%|| 27/27 [03:17<00:00, 197.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 27 trials to 28 (+1) trials\n",
            "2023-07-30 21:19:54.969055                             \n",
            "{'name': 'Adam', 'learning_rate': 3.2582018492596743e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_27', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_54_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_54', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_54', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_54', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_54', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_55', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_55', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_55', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_55', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_27', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_108', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_81', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_109', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_82', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_110', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_83', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_111', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.5025 - acc: 0.0625 - auc: 0.1833 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 2.2945 - acc: 0.0323 - auc: 0.6352 - precision: 0.0323 - recall: 1.0000 - f1: 0.0602 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.3040 - acc: 0.0353 - auc: 0.6470 - precision: 0.0353 - recall: 1.0000 - f1: 0.0658\n",
            "  64/1170 [>.............................] - ETA: 2s - loss: 2.3049 - acc: 0.0366 - auc: 0.6288 - precision: 0.0366 - recall: 1.0000 - f1: 0.0684\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: 2.3053 - acc: 0.0354 - auc: 0.6404 - precision: 0.0354 - recall: 1.0000 - f1: 0.0662\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: 2.2980 - acc: 0.0312 - auc: 0.6384 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 2.2999 - acc: 0.0323 - auc: 0.6432 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: 2.2991 - acc: 0.0317 - auc: 0.6489 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: 2.2995 - acc: 0.0329 - auc: 0.6441 - precision: 0.0329 - recall: 1.0000 - f1: 0.0618\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: 2.3014 - acc: 0.0318 - auc: 0.6429 - precision: 0.0318 - recall: 1.0000 - f1: 0.0597\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: 2.3004 - acc: 0.0311 - auc: 0.6442 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: 2.3009 - acc: 0.0312 - auc: 0.6434 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: 2.3019 - acc: 0.0306 - auc: 0.6354 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: 2.3003 - acc: 0.0307 - auc: 0.6344 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: 2.3006 - acc: 0.0301 - auc: 0.6360 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: 2.2995 - acc: 0.0297 - auc: 0.6358 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: 2.2975 - acc: 0.0300 - auc: 0.6417 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: 2.2967 - acc: 0.0302 - auc: 0.6430 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: 2.2961 - acc: 0.0302 - auc: 0.6426 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: 2.2952 - acc: 0.0310 - auc: 0.6416 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 2.2953 - acc: 0.0303 - auc: 0.6439 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: 2.2955 - acc: 0.0302 - auc: 0.6445 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 2.2939 - acc: 0.0299 - auc: 0.6456 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 2.2951 - acc: 0.0297 - auc: 0.6420 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: 2.2943 - acc: 0.0296 - auc: 0.6415 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 2.2946 - acc: 0.0295 - auc: 0.6385 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: 2.2950 - acc: 0.0293 - auc: 0.6358 - precision: 0.0293 - recall: 1.0000 - f1: 0.0554\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: 2.2942 - acc: 0.0296 - auc: 0.6361 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: 2.2948 - acc: 0.0294 - auc: 0.6365 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: 2.2960 - acc: 0.0291 - auc: 0.6355 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: 2.2955 - acc: 0.0294 - auc: 0.6359 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 726/1170 [=================>............] - ETA: 0s - loss: 2.2957 - acc: 0.0292 - auc: 0.6348 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 750/1170 [==================>...........] - ETA: 0s - loss: 2.2944 - acc: 0.0292 - auc: 0.6371 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: 2.2958 - acc: 0.0292 - auc: 0.6343 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: 2.2966 - acc: 0.0291 - auc: 0.6332 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: 2.2965 - acc: 0.0289 - auc: 0.6338 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.2978 - acc: 0.0291 - auc: 0.6326 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: 2.2976 - acc: 0.0291 - auc: 0.6330 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: 2.2978 - acc: 0.0294 - auc: 0.6338 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 2.2976 - acc: 0.0292 - auc: 0.6324 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: 2.2975 - acc: 0.0292 - auc: 0.6312 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: 2.2979 - acc: 0.0290 - auc: 0.6296 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: 2.2976 - acc: 0.0290 - auc: 0.6305 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: 2.2976 - acc: 0.0292 - auc: 0.6297 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: 2.2978 - acc: 0.0293 - auc: 0.6318 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: 2.2981 - acc: 0.0294 - auc: 0.6329 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.2977 - acc: 0.0295 - auc: 0.6343 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: 2.2990 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 2.2992 - acc: 0.0295 - auc: 0.6297 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: 2.2988 - acc: 0.0296 - auc: 0.6302 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.2990 - acc: 0.0296 - auc: 0.6300 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:26:12.093219                             \n",
            "100%|| 28/28 [06:17<00:00, 377.18s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 28 trials to 29 (+1) trials\n",
            "2023-07-30 21:26:12.151615                             \n",
            "{'name': 'Adam', 'learning_rate': 0.001416210576066268, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_28', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_56_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_56', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_56', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_56', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_56', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_57', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_57', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_57', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_57', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_28', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_112', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_84', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_113', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_85', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_114', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_86', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_115', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 1.9222 - acc: 0.0625 - auc: 0.2667 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 2s - loss: 1.7068 - acc: 0.0331 - auc: 0.6559 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  43/1170 [>.............................] - ETA: 3s - loss: 1.7224 - acc: 0.0349 - auc: 0.6312 - precision: 0.0349 - recall: 1.0000 - f1: 0.0649\n",
            "  62/1170 [>.............................] - ETA: 3s - loss: 1.7289 - acc: 0.0363 - auc: 0.6270 - precision: 0.0363 - recall: 1.0000 - f1: 0.0677\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: 1.7226 - acc: 0.0335 - auc: 0.6333 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: 1.7153 - acc: 0.0316 - auc: 0.6318 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 1.7147 - acc: 0.0323 - auc: 0.6400 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: 1.7147 - acc: 0.0319 - auc: 0.6468 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: 1.7183 - acc: 0.0329 - auc: 0.6382 - precision: 0.0329 - recall: 1.0000 - f1: 0.0618\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: 1.7159 - acc: 0.0314 - auc: 0.6379 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: 1.7153 - acc: 0.0312 - auc: 0.6349 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 1.7156 - acc: 0.0307 - auc: 0.6250 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: 1.7149 - acc: 0.0306 - auc: 0.6264 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 1.7139 - acc: 0.0307 - auc: 0.6295 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: 1.7126 - acc: 0.0302 - auc: 0.6322 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: 1.7111 - acc: 0.0295 - auc: 0.6314 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: 1.7123 - acc: 0.0302 - auc: 0.6322 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 366/1170 [========>.....................] - ETA: 1s - loss: 1.7115 - acc: 0.0304 - auc: 0.6355 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 1.7117 - acc: 0.0305 - auc: 0.6338 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: 1.7126 - acc: 0.0309 - auc: 0.6321 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 1.7109 - acc: 0.0303 - auc: 0.6352 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: 1.7114 - acc: 0.0305 - auc: 0.6353 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: 1.7099 - acc: 0.0301 - auc: 0.6366 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 1.7090 - acc: 0.0299 - auc: 0.6370 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: 1.7093 - acc: 0.0297 - auc: 0.6344 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 1.7088 - acc: 0.0296 - auc: 0.6343 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: 1.7092 - acc: 0.0297 - auc: 0.6333 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 1.7082 - acc: 0.0295 - auc: 0.6342 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: 1.7077 - acc: 0.0292 - auc: 0.6320 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: 1.7070 - acc: 0.0294 - auc: 0.6331 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: 1.7081 - acc: 0.0295 - auc: 0.6321 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 1.7090 - acc: 0.0292 - auc: 0.6309 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: 1.7086 - acc: 0.0293 - auc: 0.6323 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: 1.7085 - acc: 0.0291 - auc: 0.6309 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: 1.7084 - acc: 0.0290 - auc: 0.6314 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: 1.7093 - acc: 0.0293 - auc: 0.6313 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: 1.7095 - acc: 0.0291 - auc: 0.6295 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: 1.7094 - acc: 0.0290 - auc: 0.6284 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: 1.7098 - acc: 0.0290 - auc: 0.6275 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: 1.7100 - acc: 0.0291 - auc: 0.6270 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: 1.7104 - acc: 0.0292 - auc: 0.6270 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: 1.7107 - acc: 0.0294 - auc: 0.6275 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: 1.7101 - acc: 0.0290 - auc: 0.6263 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 1.7100 - acc: 0.0291 - auc: 0.6262 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: 1.7106 - acc: 0.0291 - auc: 0.6238 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 1.7100 - acc: 0.0290 - auc: 0.6261 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: 1.7097 - acc: 0.0291 - auc: 0.6268 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 1.7097 - acc: 0.0292 - auc: 0.6287 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 1.7102 - acc: 0.0293 - auc: 0.6295 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: 1.7106 - acc: 0.0295 - auc: 0.6309 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: 1.7116 - acc: 0.0295 - auc: 0.6286 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.7112 - acc: 0.0295 - auc: 0.6284 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.7114 - acc: 0.0296 - auc: 0.6287 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:27:50.511353                             \n",
            "100%|| 29/29 [01:38<00:00, 98.41s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 29 trials to 30 (+1) trials\n",
            "2023-07-30 21:27:50.574522                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5037537639192469, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_29', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_58_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_58', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_58', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_58', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_58', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_59', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_59', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_59', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_59', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_29', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_116', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_87', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_117', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_88', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_118', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_89', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_119', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:28:50.063158                             \n",
            "100%|| 30/30 [00:59<00:00, 59.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 30 trials to 31 (+1) trials\n",
            "2023-07-30 21:28:50.122606                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2134877090302406, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_30', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_60_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_60', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_60', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_60', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_60', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_61', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_61', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_61', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_61', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_30', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_120', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_90', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_121', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_91', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_122', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_92', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_123', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:29:47.838759                             \n",
            "100%|| 31/31 [00:57<00:00, 57.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 31 trials to 32 (+1) trials\n",
            "2023-07-30 21:29:47.987998                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00470555778189363, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_31', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_62_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_62', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_62', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_62', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_62', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_63', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_63', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_63', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_63', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_31', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_124', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_93', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_125', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_94', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_126', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_95', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_127', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.5646 - acc: 0.0625 - auc: 0.3000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 1.4511 - acc: 0.0455 - auc: 0.6812 - precision: 0.0343 - recall: 1.0000 - f1: 0.0638 \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: 1.4583 - acc: 0.0472 - auc: 0.6755 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: 1.4584 - acc: 0.0510 - auc: 0.6592 - precision: 0.0375 - recall: 1.0000 - f1: 0.0701\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: 1.4604 - acc: 0.0475 - auc: 0.6562 - precision: 0.0358 - recall: 1.0000 - f1: 0.0672\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: 1.4543 - acc: 0.0448 - auc: 0.6617 - precision: 0.0327 - recall: 1.0000 - f1: 0.0613\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: 1.4527 - acc: 0.0450 - auc: 0.6640 - precision: 0.0325 - recall: 1.0000 - f1: 0.0611\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: 1.4508 - acc: 0.0460 - auc: 0.6694 - precision: 0.0325 - recall: 1.0000 - f1: 0.0610\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 1.4548 - acc: 0.0469 - auc: 0.6635 - precision: 0.0332 - recall: 1.0000 - f1: 0.0625\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: 1.4555 - acc: 0.0460 - auc: 0.6557 - precision: 0.0324 - recall: 1.0000 - f1: 0.0609\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: 1.4542 - acc: 0.0439 - auc: 0.6522 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: 1.4566 - acc: 0.0438 - auc: 0.6448 - precision: 0.0311 - recall: 0.9953 - f1: 0.0585\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: 1.4559 - acc: 0.0435 - auc: 0.6371 - precision: 0.0307 - recall: 0.9957 - f1: 0.0579\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 1.4553 - acc: 0.0428 - auc: 0.6368 - precision: 0.0309 - recall: 0.9962 - f1: 0.0582\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 1.4549 - acc: 0.0423 - auc: 0.6372 - precision: 0.0304 - recall: 0.9963 - f1: 0.0573\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: 1.4563 - acc: 0.0417 - auc: 0.6323 - precision: 0.0303 - recall: 0.9965 - f1: 0.0570\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: 1.4552 - acc: 0.0418 - auc: 0.6348 - precision: 0.0302 - recall: 0.9968 - f1: 0.0569\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 1.4547 - acc: 0.0420 - auc: 0.6349 - precision: 0.0303 - recall: 0.9970 - f1: 0.0572\n",
            " 373/1170 [========>.....................] - ETA: 1s - loss: 1.4539 - acc: 0.0422 - auc: 0.6412 - precision: 0.0305 - recall: 0.9972 - f1: 0.0575\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: 1.4541 - acc: 0.0429 - auc: 0.6385 - precision: 0.0312 - recall: 0.9975 - f1: 0.0588\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: 1.4538 - acc: 0.0427 - auc: 0.6402 - precision: 0.0311 - recall: 0.9976 - f1: 0.0585\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: 1.4534 - acc: 0.0422 - auc: 0.6431 - precision: 0.0307 - recall: 0.9977 - f1: 0.0579\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: 1.4533 - acc: 0.0419 - auc: 0.6438 - precision: 0.0304 - recall: 0.9978 - f1: 0.0573\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: 1.4531 - acc: 0.0420 - auc: 0.6431 - precision: 0.0304 - recall: 0.9979 - f1: 0.0573\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: 1.4525 - acc: 0.0416 - auc: 0.6429 - precision: 0.0302 - recall: 0.9980 - f1: 0.0570\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: 1.4532 - acc: 0.0418 - auc: 0.6406 - precision: 0.0301 - recall: 0.9981 - f1: 0.0568\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: 1.4524 - acc: 0.0420 - auc: 0.6398 - precision: 0.0300 - recall: 0.9981 - f1: 0.0567\n",
            " 567/1170 [=============>................] - ETA: 1s - loss: 1.4522 - acc: 0.0420 - auc: 0.6413 - precision: 0.0300 - recall: 0.9981 - f1: 0.0565\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: 1.4528 - acc: 0.0420 - auc: 0.6364 - precision: 0.0299 - recall: 0.9982 - f1: 0.0565\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 1.4526 - acc: 0.0417 - auc: 0.6384 - precision: 0.0299 - recall: 0.9982 - f1: 0.0563\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 1.4521 - acc: 0.0413 - auc: 0.6355 - precision: 0.0296 - recall: 0.9983 - f1: 0.0559\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: 1.4522 - acc: 0.0415 - auc: 0.6350 - precision: 0.0298 - recall: 0.9984 - f1: 0.0562\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: 1.4525 - acc: 0.0415 - auc: 0.6354 - precision: 0.0297 - recall: 0.9984 - f1: 0.0560\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 1.4533 - acc: 0.0413 - auc: 0.6348 - precision: 0.0295 - recall: 0.9984 - f1: 0.0556\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: 1.4525 - acc: 0.0414 - auc: 0.6355 - precision: 0.0296 - recall: 0.9985 - f1: 0.0558\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: 1.4531 - acc: 0.0415 - auc: 0.6336 - precision: 0.0296 - recall: 0.9985 - f1: 0.0558\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: 1.4526 - acc: 0.0412 - auc: 0.6337 - precision: 0.0294 - recall: 0.9985 - f1: 0.0554\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: 1.4528 - acc: 0.0412 - auc: 0.6330 - precision: 0.0294 - recall: 0.9986 - f1: 0.0554\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 1.4530 - acc: 0.0414 - auc: 0.6338 - precision: 0.0297 - recall: 0.9987 - f1: 0.0560\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: 1.4535 - acc: 0.0411 - auc: 0.6320 - precision: 0.0295 - recall: 0.9974 - f1: 0.0556\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: 1.4530 - acc: 0.0407 - auc: 0.6330 - precision: 0.0292 - recall: 0.9974 - f1: 0.0550\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: 1.4540 - acc: 0.0405 - auc: 0.6308 - precision: 0.0293 - recall: 0.9962 - f1: 0.0552\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: 1.4546 - acc: 0.0408 - auc: 0.6307 - precision: 0.0294 - recall: 0.9952 - f1: 0.0554\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6332 - precision: 0.0297 - recall: 0.9953 - f1: 0.0559\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: 1.4546 - acc: 0.0411 - auc: 0.6318 - precision: 0.0295 - recall: 0.9954 - f1: 0.0556\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6307 - precision: 0.0294 - recall: 0.9955 - f1: 0.0554\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6301 - precision: 0.0293 - recall: 0.9956 - f1: 0.0552\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: 1.4551 - acc: 0.0410 - auc: 0.6286 - precision: 0.0293 - recall: 0.9956 - f1: 0.0552\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: 1.4545 - acc: 0.0410 - auc: 0.6298 - precision: 0.0292 - recall: 0.9957 - f1: 0.0550\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: 1.4545 - acc: 0.0411 - auc: 0.6297 - precision: 0.0294 - recall: 0.9959 - f1: 0.0555\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: 1.4546 - acc: 0.0413 - auc: 0.6332 - precision: 0.0296 - recall: 0.9960 - f1: 0.0558\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: 1.4545 - acc: 0.0414 - auc: 0.6340 - precision: 0.0296 - recall: 0.9961 - f1: 0.0558\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 1.4554 - acc: 0.0415 - auc: 0.6318 - precision: 0.0298 - recall: 0.9962 - f1: 0.0561\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 1.4554 - acc: 0.0415 - auc: 0.6304 - precision: 0.0298 - recall: 0.9963 - f1: 0.0560\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.4554 - acc: 0.0415 - auc: 0.6305 - precision: 0.0298 - recall: 0.9964 - f1: 0.0562\n",
            "\n",
            "2023-07-30 21:43:08.760894                             \n",
            "100%|| 32/32 [13:20<00:00, 800.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 32 trials to 33 (+1) trials\n",
            "2023-07-30 21:43:08.831484                             \n",
            "{'name': 'Adam', 'learning_rate': 0.34004878674597416, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_32', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_64_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_64', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_64', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_64', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_64', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_65', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_65', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_65', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_65', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_32', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_128', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_96', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_129', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_97', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_130', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_98', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_131', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:44:07.286538                             \n",
            "100%|| 33/33 [00:58<00:00, 58.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 33 trials to 34 (+1) trials\n",
            "2023-07-30 21:44:07.371958                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4215667018418808, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_33', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_66_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_66', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_66', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_66', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_66', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_67', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_67', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_67', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_67', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_33', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_132', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_99', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_133', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_100', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_134', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_101', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_135', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1038/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:45:14.166071                             \n",
            "100%|| 34/34 [01:06<00:00, 66.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 34 trials to 35 (+1) trials\n",
            "2023-07-30 21:45:14.227802                             \n",
            "{'name': 'Adam', 'learning_rate': 0.16260120093047567, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_34', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_68_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_68', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_68', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_68', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_68', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_69', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_69', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_69', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_69', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_34', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_136', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_102', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_137', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_103', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_138', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_104', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_139', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  38/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:46:12.605945                             \n",
            "100%|| 35/35 [00:58<00:00, 58.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 35 trials to 36 (+1) trials\n",
            "2023-07-30 21:46:12.665614                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3291661395808618, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_35', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_70_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_70', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_70', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_70', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_70', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_71', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_71', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_71', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_71', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_35', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_140', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_105', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_141', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_106', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_142', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_107', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_143', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:47:11.530855                             \n",
            "100%|| 36/36 [00:58<00:00, 58.92s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 36 trials to 37 (+1) trials\n",
            "2023-07-30 21:47:11.670530                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9086257499020481, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_36', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_72_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_72', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_72', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_72', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_72', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_73', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_73', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_73', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_73', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_36', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_144', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_108', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_145', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_109', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_146', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_110', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_147', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:48:02.708860                             \n",
            "100%|| 37/37 [00:51<00:00, 51.10s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 37 trials to 38 (+1) trials\n",
            "2023-07-30 21:48:02.768585                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5504430797123047, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_37', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_74_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_74', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_74', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_74', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_74', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_75', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_75', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_75', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_75', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_37', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_148', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_111', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_149', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_112', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_150', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_113', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_151', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 369/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:48:53.475070                             \n",
            "100%|| 38/38 [00:50<00:00, 50.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 38 trials to 39 (+1) trials\n",
            "2023-07-30 21:48:53.534346                             \n",
            "{'name': 'Adam', 'learning_rate': 0.24041289518611525, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_38', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_76_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_76', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_76', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_76', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_76', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_77', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_77', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_77', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_77', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_38', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_152', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_114', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_153', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_115', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_154', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_116', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_155', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:49:46.244282                             \n",
            "100%|| 39/39 [00:52<00:00, 52.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 39 trials to 40 (+1) trials\n",
            "2023-07-30 21:49:46.304502                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2275063984552216, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_39', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_78_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_78', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_78', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_78', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_78', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_79', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_79', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_79', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_79', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_39', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_156', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_117', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_157', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_118', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_158', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_119', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_159', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 332/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:50:44.850968                             \n",
            "100%|| 40/40 [00:58<00:00, 58.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 40 trials to 41 (+1) trials\n",
            "2023-07-30 21:50:44.908954                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5698474362113314, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_40', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_80_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_80', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_80', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_80', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_80', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_81', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_81', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_81', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_81', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_40', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_160', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_120', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_161', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_121', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_162', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_122', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_163', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  40/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:51:52.138374                             \n",
            "100%|| 41/41 [01:07<00:00, 67.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 41 trials to 42 (+1) trials\n",
            "2023-07-30 21:51:52.199739                             \n",
            "{'name': 'Adam', 'learning_rate': 0.1572789606078243, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_41', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_82_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_82', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_82', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_82', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_82', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_83', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_83', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_83', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_83', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_41', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_164', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_123', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_165', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_124', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_166', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_125', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_167', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  40/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:57:03.316648                             \n",
            "100%|| 42/42 [05:11<00:00, 311.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 42 trials to 43 (+1) trials\n",
            "2023-07-30 21:57:03.375668                             \n",
            "{'name': 'Adam', 'learning_rate': 0.48183478974461685, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_42', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_84_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_84', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_84', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_84', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_84', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_85', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_85', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_85', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_85', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_42', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_168', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_126', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_169', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_127', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_170', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_128', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_171', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  29/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:57:54.394967                             \n",
            "100%|| 43/43 [00:51<00:00, 51.07s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 43 trials to 44 (+1) trials\n",
            "2023-07-30 21:57:54.453328                             \n",
            "{'name': 'Adam', 'learning_rate': 0.21567454458132654, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_43', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_86_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_86', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_86', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_86', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_86', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_87', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_87', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_87', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_87', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_43', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_172', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_129', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_173', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_130', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_174', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_131', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_175', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:58:54.261129                             \n",
            "100%|| 44/44 [00:59<00:00, 59.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 44 trials to 45 (+1) trials\n",
            "2023-07-30 21:58:54.321039                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4220124024578253, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_44', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_88_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_88', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_88', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_88', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_88', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_89', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_89', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_89', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_89', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_44', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_176', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_132', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_177', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_133', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_178', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_134', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_179', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:04:09.801017                             \n",
            "100%|| 45/45 [05:15<00:00, 315.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 45 trials to 46 (+1) trials\n",
            "2023-07-30 22:04:09.948712                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3081750345491424, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_45', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_90_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_90', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_90', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_90', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_90', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_91', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_91', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_91', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_91', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_45', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_180', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_135', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_181', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_136', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_182', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_137', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_183', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:05:09.308877                             \n",
            "100%|| 46/46 [00:59<00:00, 59.42s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 46 trials to 47 (+1) trials\n",
            "2023-07-30 22:05:09.368198                             \n",
            "{'name': 'Adam', 'learning_rate': 0.12660174534366173, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_46', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_92_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_92', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_92', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_92', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_92', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_93', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_93', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_93', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_93', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_46', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_184', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_138', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_185', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_139', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_186', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_140', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_187', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:12:44.137043                             \n",
            "100%|| 47/47 [07:34<00:00, 454.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 47 trials to 48 (+1) trials\n",
            "2023-07-30 22:12:44.199059                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5536751854290182, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_47', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_94_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_94', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_94', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_94', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_94', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_95', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_95', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_95', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_95', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_47', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_188', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_141', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_189', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_142', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_190', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_143', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_191', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:13:37.331606                             \n",
            "100%|| 48/48 [00:53<00:00, 53.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 48 trials to 49 (+1) trials\n",
            "2023-07-30 22:13:37.389500                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00020303386505032435, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_48', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_96_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_96', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_96', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_96', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_96', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_97', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_97', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_97', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_97', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_48', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_192', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_144', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_193', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_145', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_194', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_146', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_195', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.3690 - acc: 0.0625 - auc: 0.3167 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  29/1170 [..............................] - ETA: 2s - loss: 1.2725 - acc: 0.0302 - auc: 0.6486 - precision: 0.0302 - recall: 1.0000 - f1: 0.0564 \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: 1.2850 - acc: 0.0347 - auc: 0.6273 - precision: 0.0347 - recall: 1.0000 - f1: 0.0647\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: 1.2858 - acc: 0.0369 - auc: 0.6396 - precision: 0.0369 - recall: 1.0000 - f1: 0.0688\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: 1.2849 - acc: 0.0353 - auc: 0.6485 - precision: 0.0353 - recall: 1.0000 - f1: 0.0661\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: 1.2769 - acc: 0.0309 - auc: 0.6520 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: 1.2775 - acc: 0.0320 - auc: 0.6617 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: 1.2791 - acc: 0.0317 - auc: 0.6651 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: 1.2812 - acc: 0.0329 - auc: 0.6596 - precision: 0.0329 - recall: 1.0000 - f1: 0.0619\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: 1.2805 - acc: 0.0316 - auc: 0.6492 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: 1.2791 - acc: 0.0311 - auc: 0.6484 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 215/1170 [====>.........................] - ETA: 2s - loss: 1.2801 - acc: 0.0308 - auc: 0.6395 - precision: 0.0308 - recall: 1.0000 - f1: 0.0579\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: 1.2797 - acc: 0.0306 - auc: 0.6349 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: 1.2792 - acc: 0.0304 - auc: 0.6347 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: 1.2783 - acc: 0.0301 - auc: 0.6371 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: 1.2793 - acc: 0.0301 - auc: 0.6274 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: 1.2781 - acc: 0.0296 - auc: 0.6311 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: 1.2788 - acc: 0.0302 - auc: 0.6331 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: 1.2785 - acc: 0.0302 - auc: 0.6332 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: 1.2786 - acc: 0.0303 - auc: 0.6341 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: 1.2782 - acc: 0.0302 - auc: 0.6341 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: 1.2791 - acc: 0.0310 - auc: 0.6347 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: 1.2788 - acc: 0.0307 - auc: 0.6331 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: 1.2782 - acc: 0.0304 - auc: 0.6331 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 1.2781 - acc: 0.0303 - auc: 0.6346 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: 1.2776 - acc: 0.0300 - auc: 0.6342 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: 1.2774 - acc: 0.0299 - auc: 0.6334 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: 1.2778 - acc: 0.0298 - auc: 0.6307 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: 1.2777 - acc: 0.0297 - auc: 0.6283 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: 1.2785 - acc: 0.0298 - auc: 0.6205 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: 1.2778 - acc: 0.0295 - auc: 0.6209 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: 1.2774 - acc: 0.0292 - auc: 0.6202 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: 1.2776 - acc: 0.0296 - auc: 0.6216 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: 1.2775 - acc: 0.0293 - auc: 0.6227 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: 1.2775 - acc: 0.0292 - auc: 0.6215 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: 1.2775 - acc: 0.0293 - auc: 0.6210 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: 1.2774 - acc: 0.0292 - auc: 0.6195 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: 1.2771 - acc: 0.0291 - auc: 0.6216 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: 1.2777 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: 1.2778 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: 1.2782 - acc: 0.0292 - auc: 0.6194 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: 1.2776 - acc: 0.0289 - auc: 0.6208 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 1.2782 - acc: 0.0290 - auc: 0.6191 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: 1.2782 - acc: 0.0291 - auc: 0.6195 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6189 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: 1.2789 - acc: 0.0294 - auc: 0.6188 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6183 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6183 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 1.2783 - acc: 0.0291 - auc: 0.6175 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: 1.2782 - acc: 0.0290 - auc: 0.6181 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 1.2786 - acc: 0.0290 - auc: 0.6156 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: 1.2783 - acc: 0.0289 - auc: 0.6160 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: 1.2787 - acc: 0.0292 - auc: 0.6169 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.2790 - acc: 0.0293 - auc: 0.6179 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: 1.2788 - acc: 0.0293 - auc: 0.6196 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: 1.2791 - acc: 0.0295 - auc: 0.6213 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 1.2793 - acc: 0.0295 - auc: 0.6199 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.0295 - auc: 0.6186 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.0295 - auc: 0.6194 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 1.2795 - acc: 0.0296 - auc: 0.6200 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 22:15:02.242784                             \n",
            "100%|| 49/49 [01:24<00:00, 84.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 49 trials to 50 (+1) trials\n",
            "2023-07-30 22:15:02.305599                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6991370578489954, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_49', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_98_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_98', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_98', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_98', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_98', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_99', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_99', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_99', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_99', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_49', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_196', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_147', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_197', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_148', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_198', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_149', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_199', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 28s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  17/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9596 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  32/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 4s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  40/1170 [>.............................] - ETA: 6s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  42/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  52/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 6s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 9s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 10s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 10s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            " 125/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 9s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 8s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 8s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 9s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 9s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 7s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 7s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 7s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 7s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 6s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 9s 8ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:22:12.407335                             \n",
            "100%|| 50/50 [07:10<00:00, 430.21s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 50 trials to 51 (+1) trials\n",
            "2023-07-30 22:22:12.545049                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6278383296275578, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_50', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_100_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_100', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_100', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_100', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_100', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_101', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_101', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_101', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_101', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_50', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_200', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_150', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_201', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_151', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_202', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_152', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_203', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 50/51 [08:43<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:31:01.334511                             \n",
            "100%|| 51/51 [08:48<00:00, 528.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 51 trials to 52 (+1) trials\n",
            "2023-07-30 22:31:01.406436                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5428541232560751, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_51', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_102_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_102', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_102', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_102', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_102', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_103', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_103', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_103', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_103', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_51', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_204', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_153', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_205', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_154', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_206', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_155', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_207', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 980/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:32:04.932893                             \n",
            "100%|| 52/52 [01:03<00:00, 63.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 52 trials to 53 (+1) trials\n",
            "2023-07-30 22:32:05.004607                             \n",
            "{'name': 'Adam', 'learning_rate': 0.006077755955858521, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_52', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_104_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_104', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_104', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_104', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_104', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_105', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_105', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_105', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_105', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_52', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_208', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_156', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_209', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_157', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_210', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_158', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_211', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 1.0519 - acc: 0.0625 - auc: 0.1333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  28/1170 [..............................] - ETA: 2s - loss: 0.9565 - acc: 0.0312 - auc: 0.6174 - precision: 0.0312 - recall: 1.0000 - f1: 0.0584 \n",
            "  36/1170 [..............................] - ETA: 3s - loss: 0.9603 - acc: 0.0330 - auc: 0.6261 - precision: 0.0330 - recall: 1.0000 - f1: 0.0614\n",
            "  49/1170 [>.............................] - ETA: 3s - loss: 0.9670 - acc: 0.0370 - auc: 0.6289 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  63/1170 [>.............................] - ETA: 3s - loss: 0.9653 - acc: 0.0362 - auc: 0.6281 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  77/1170 [>.............................] - ETA: 3s - loss: 0.9653 - acc: 0.0353 - auc: 0.6416 - precision: 0.0353 - recall: 1.0000 - f1: 0.0661\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: 0.9619 - acc: 0.0330 - auc: 0.6290 - precision: 0.0330 - recall: 1.0000 - f1: 0.0618\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: 0.9614 - acc: 0.0309 - auc: 0.6333 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 0.9610 - acc: 0.0320 - auc: 0.6371 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: 0.9623 - acc: 0.0323 - auc: 0.6407 - precision: 0.0323 - recall: 1.0000 - f1: 0.0609\n",
            " 161/1170 [===>..........................] - ETA: 3s - loss: 0.9627 - acc: 0.0320 - auc: 0.6331 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: 0.9621 - acc: 0.0316 - auc: 0.6358 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 189/1170 [===>..........................] - ETA: 3s - loss: 0.9626 - acc: 0.0309 - auc: 0.6303 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 203/1170 [====>.........................] - ETA: 3s - loss: 0.9629 - acc: 0.0311 - auc: 0.6309 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 215/1170 [====>.........................] - ETA: 3s - loss: 0.9635 - acc: 0.0308 - auc: 0.6249 - precision: 0.0308 - recall: 1.0000 - f1: 0.0579\n",
            " 226/1170 [====>.........................] - ETA: 3s - loss: 0.9621 - acc: 0.0306 - auc: 0.6284 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: 0.9625 - acc: 0.0306 - auc: 0.6205 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: 0.9619 - acc: 0.0301 - auc: 0.6182 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 266/1170 [=====>........................] - ETA: 3s - loss: 0.9618 - acc: 0.0307 - auc: 0.6164 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: 0.9613 - acc: 0.0301 - auc: 0.6189 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: 0.9618 - acc: 0.0302 - auc: 0.6174 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 299/1170 [======>.......................] - ETA: 3s - loss: 0.9617 - acc: 0.0300 - auc: 0.6181 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 312/1170 [=======>......................] - ETA: 3s - loss: 0.9607 - acc: 0.0295 - auc: 0.6206 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 319/1170 [=======>......................] - ETA: 3s - loss: 0.9606 - acc: 0.0296 - auc: 0.6210 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: 0.9612 - acc: 0.0302 - auc: 0.6230 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 354/1170 [========>.....................] - ETA: 3s - loss: 0.9611 - acc: 0.0304 - auc: 0.6239 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 369/1170 [========>.....................] - ETA: 2s - loss: 0.9608 - acc: 0.0303 - auc: 0.6258 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: 0.9600 - acc: 0.0300 - auc: 0.6265 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: 0.9610 - acc: 0.0309 - auc: 0.6249 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: 0.9610 - acc: 0.0308 - auc: 0.6242 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: 0.9609 - acc: 0.0308 - auc: 0.6256 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: 0.9607 - acc: 0.0306 - auc: 0.6268 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: 0.9602 - acc: 0.0304 - auc: 0.6271 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 472/1170 [===========>..................] - ETA: 2s - loss: 0.9603 - acc: 0.0303 - auc: 0.6271 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: 0.9596 - acc: 0.0300 - auc: 0.6282 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 519/1170 [============>.................] - ETA: 2s - loss: 0.9597 - acc: 0.0299 - auc: 0.6242 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 531/1170 [============>.................] - ETA: 2s - loss: 0.9595 - acc: 0.0297 - auc: 0.6257 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 540/1170 [============>.................] - ETA: 2s - loss: 0.9598 - acc: 0.0297 - auc: 0.6242 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: 0.9598 - acc: 0.0298 - auc: 0.6221 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 568/1170 [=============>................] - ETA: 2s - loss: 0.9594 - acc: 0.0296 - auc: 0.6232 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 584/1170 [=============>................] - ETA: 2s - loss: 0.9599 - acc: 0.0298 - auc: 0.6188 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: 0.9594 - acc: 0.0295 - auc: 0.6200 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: 0.9594 - acc: 0.0294 - auc: 0.6192 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: 0.9590 - acc: 0.0293 - auc: 0.6192 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: 0.9592 - acc: 0.0296 - auc: 0.6204 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: 0.9591 - acc: 0.0294 - auc: 0.6206 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: 0.9592 - acc: 0.0294 - auc: 0.6203 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: 0.9590 - acc: 0.0293 - auc: 0.6211 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: 0.9588 - acc: 0.0291 - auc: 0.6212 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: 0.9589 - acc: 0.0292 - auc: 0.6228 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: 0.9593 - acc: 0.0294 - auc: 0.6212 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: 0.9589 - acc: 0.0291 - auc: 0.6198 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: 0.9588 - acc: 0.0291 - auc: 0.6208 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: 0.9590 - acc: 0.0290 - auc: 0.6196 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 0.9591 - acc: 0.0292 - auc: 0.6205 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: 0.9593 - acc: 0.0292 - auc: 0.6188 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: 0.9594 - acc: 0.0292 - auc: 0.6180 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 828/1170 [====================>.........] - ETA: 1s - loss: 0.9590 - acc: 0.0289 - auc: 0.6189 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            " 851/1170 [====================>.........] - ETA: 1s - loss: 0.9593 - acc: 0.0290 - auc: 0.6176 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 865/1170 [=====================>........] - ETA: 1s - loss: 0.9595 - acc: 0.0291 - auc: 0.6171 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 878/1170 [=====================>........] - ETA: 1s - loss: 0.9596 - acc: 0.0291 - auc: 0.6184 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: 0.9599 - acc: 0.0293 - auc: 0.6176 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: 0.9600 - acc: 0.0294 - auc: 0.6177 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 0.9600 - acc: 0.0294 - auc: 0.6166 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: 0.9598 - acc: 0.0292 - auc: 0.6159 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 0.9598 - acc: 0.0292 - auc: 0.6139 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: 0.9595 - acc: 0.0290 - auc: 0.6143 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 0.9599 - acc: 0.0290 - auc: 0.6121 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 0.9599 - acc: 0.0290 - auc: 0.6131 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: 0.9598 - acc: 0.0290 - auc: 0.6133 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: 0.9601 - acc: 0.0292 - auc: 0.6131 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 0.9602 - acc: 0.0292 - auc: 0.6128 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: 0.9601 - acc: 0.0292 - auc: 0.6145 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: 0.9602 - acc: 0.0292 - auc: 0.6155 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 0.9604 - acc: 0.0293 - auc: 0.6149 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: 0.9603 - acc: 0.0293 - auc: 0.6153 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: 0.9606 - acc: 0.0295 - auc: 0.6163 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 0.9611 - acc: 0.0295 - auc: 0.6133 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.0295 - auc: 0.6121 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.0296 - auc: 0.6121 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: 0.9611 - acc: 0.0295 - auc: 0.6114 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: 0.9611 - acc: 0.0296 - auc: 0.6122 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 22:33:19.136767                             \n",
            "100%|| 53/53 [01:14<00:00, 74.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 53 trials to 54 (+1) trials\n",
            "2023-07-30 22:33:19.196213                             \n",
            "{'name': 'Adam', 'learning_rate': 0.19767535212585802, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_53', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_106_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_106', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_106', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_106', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_106', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_107', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_107', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_107', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_107', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_53', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_212', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_159', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_213', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_160', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_214', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_161', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_215', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 509/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:38:45.202832                             \n",
            "100%|| 54/54 [05:26<00:00, 326.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 54 trials to 55 (+1) trials\n",
            "2023-07-30 22:38:45.278056                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5943576615756214, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_54', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_108_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_108', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_108', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_108', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_108', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_109', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_109', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_109', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_109', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_54', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_216', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_162', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_217', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_163', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_218', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_164', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_219', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:46:27.884960                             \n",
            "100%|| 55/55 [07:42<00:00, 462.66s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 55 trials to 56 (+1) trials\n",
            "2023-07-30 22:46:27.945282                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5478990863457, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_55', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_110_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_110', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_110', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_110', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_110', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_111', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_111', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_111', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_111', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_55', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_220', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_165', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_221', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_166', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_222', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_167', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_223', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:51:56.970992                             \n",
            "100%|| 56/56 [05:29<00:00, 329.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 56 trials to 57 (+1) trials\n",
            "2023-07-30 22:51:57.035398                             \n",
            "{'name': 'Adam', 'learning_rate': 0.003842335918990532, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_56', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_112_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_112', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_112', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_112', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_112', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_113', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_113', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_113', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_113', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_56', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_224', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_168', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_225', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_169', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_226', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_170', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_227', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: 1.2810 - acc: 0.2500 - auc: 0.3333 - precision: 0.0769 - recall: 1.0000 - f1: 0.1429\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 1.2249 - acc: 0.1866 - auc: 0.6174 - precision: 0.0371 - recall: 0.9429 - f1: 0.0676 \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: 1.2374 - acc: 0.1857 - auc: 0.6014 - precision: 0.0392 - recall: 0.9153 - f1: 0.0720\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: 1.2355 - acc: 0.1870 - auc: 0.6148 - precision: 0.0393 - recall: 0.9221 - f1: 0.0724\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: 1.2333 - acc: 0.1843 - auc: 0.6258 - precision: 0.0370 - recall: 0.9362 - f1: 0.0684\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: 1.2292 - acc: 0.1840 - auc: 0.6305 - precision: 0.0348 - recall: 0.9388 - f1: 0.0643\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: 1.2282 - acc: 0.1859 - auc: 0.6379 - precision: 0.0361 - recall: 0.9504 - f1: 0.0668\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: 1.2286 - acc: 0.1834 - auc: 0.6442 - precision: 0.0361 - recall: 0.9552 - f1: 0.0670\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: 1.2310 - acc: 0.1860 - auc: 0.6336 - precision: 0.0361 - recall: 0.9400 - f1: 0.0671\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: 1.2289 - acc: 0.1875 - auc: 0.6319 - precision: 0.0362 - recall: 0.9455 - f1: 0.0673\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: 1.2305 - acc: 0.1855 - auc: 0.6306 - precision: 0.0356 - recall: 0.9438 - f1: 0.0663\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: 1.2296 - acc: 0.1853 - auc: 0.6291 - precision: 0.0348 - recall: 0.9490 - f1: 0.0649\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: 1.2309 - acc: 0.1859 - auc: 0.6164 - precision: 0.0343 - recall: 0.9401 - f1: 0.0641\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: 1.2308 - acc: 0.1851 - auc: 0.6136 - precision: 0.0341 - recall: 0.9381 - f1: 0.0637\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: 1.2301 - acc: 0.1860 - auc: 0.6119 - precision: 0.0343 - recall: 0.9397 - f1: 0.0642\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: 1.2299 - acc: 0.1853 - auc: 0.6082 - precision: 0.0337 - recall: 0.9383 - f1: 0.0632\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: 1.2305 - acc: 0.1846 - auc: 0.6099 - precision: 0.0339 - recall: 0.9365 - f1: 0.0635\n",
            " 261/1170 [=====>........................] - ETA: 3s - loss: 1.2303 - acc: 0.1846 - auc: 0.6112 - precision: 0.0341 - recall: 0.9375 - f1: 0.0638\n",
            " 265/1170 [=====>........................] - ETA: 3s - loss: 1.2302 - acc: 0.1848 - auc: 0.6131 - precision: 0.0342 - recall: 0.9385 - f1: 0.0639\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: 1.2295 - acc: 0.1851 - auc: 0.6119 - precision: 0.0335 - recall: 0.9363 - f1: 0.0627\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: 1.2304 - acc: 0.1833 - auc: 0.6054 - precision: 0.0330 - recall: 0.9286 - f1: 0.0618\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: 1.2293 - acc: 0.1842 - auc: 0.6099 - precision: 0.0330 - recall: 0.9302 - f1: 0.0619\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: 1.2292 - acc: 0.1848 - auc: 0.6121 - precision: 0.0333 - recall: 0.9308 - f1: 0.0625\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 1.2289 - acc: 0.1847 - auc: 0.6106 - precision: 0.0332 - recall: 0.9299 - f1: 0.0623\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: 1.2283 - acc: 0.1851 - auc: 0.6117 - precision: 0.0336 - recall: 0.9331 - f1: 0.0628\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: 1.2279 - acc: 0.1846 - auc: 0.6136 - precision: 0.0338 - recall: 0.9352 - f1: 0.0633\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: 1.2274 - acc: 0.1845 - auc: 0.6134 - precision: 0.0336 - recall: 0.9313 - f1: 0.0629\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: 1.2276 - acc: 0.1838 - auc: 0.6120 - precision: 0.0334 - recall: 0.9326 - f1: 0.0627\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: 1.2277 - acc: 0.1839 - auc: 0.6106 - precision: 0.0336 - recall: 0.9340 - f1: 0.0630\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: 1.2275 - acc: 0.1857 - auc: 0.6093 - precision: 0.0344 - recall: 0.9355 - f1: 0.0645\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: 1.2263 - acc: 0.1849 - auc: 0.6148 - precision: 0.0343 - recall: 0.9351 - f1: 0.0643\n",
            " 440/1170 [==========>...................] - ETA: 2s - loss: 1.2262 - acc: 0.1839 - auc: 0.6159 - precision: 0.0340 - recall: 0.9372 - f1: 0.0637\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: 1.2264 - acc: 0.1833 - auc: 0.6162 - precision: 0.0337 - recall: 0.9361 - f1: 0.0632\n",
            " 464/1170 [==========>...................] - ETA: 2s - loss: 1.2259 - acc: 0.1831 - auc: 0.6185 - precision: 0.0335 - recall: 0.9375 - f1: 0.0629\n",
            " 468/1170 [===========>..................] - ETA: 2s - loss: 1.2262 - acc: 0.1827 - auc: 0.6182 - precision: 0.0336 - recall: 0.9360 - f1: 0.0630\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: 1.2261 - acc: 0.1829 - auc: 0.6181 - precision: 0.0337 - recall: 0.9365 - f1: 0.0632\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: 1.2264 - acc: 0.1826 - auc: 0.6207 - precision: 0.0339 - recall: 0.9376 - f1: 0.0636\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: 1.2259 - acc: 0.1826 - auc: 0.6197 - precision: 0.0335 - recall: 0.9382 - f1: 0.0628\n",
            " 504/1170 [===========>..................] - ETA: 2s - loss: 1.2251 - acc: 0.1826 - auc: 0.6202 - precision: 0.0332 - recall: 0.9378 - f1: 0.0622\n",
            " 516/1170 [============>.................] - ETA: 2s - loss: 1.2253 - acc: 0.1828 - auc: 0.6181 - precision: 0.0332 - recall: 0.9371 - f1: 0.0622\n",
            " 534/1170 [============>.................] - ETA: 2s - loss: 1.2255 - acc: 0.1823 - auc: 0.6155 - precision: 0.0329 - recall: 0.9387 - f1: 0.0618\n",
            " 546/1170 [=============>................] - ETA: 2s - loss: 1.2258 - acc: 0.1818 - auc: 0.6173 - precision: 0.0330 - recall: 0.9383 - f1: 0.0619\n",
            " 557/1170 [=============>................] - ETA: 2s - loss: 1.2258 - acc: 0.1819 - auc: 0.6151 - precision: 0.0330 - recall: 0.9358 - f1: 0.0618\n",
            " 569/1170 [=============>................] - ETA: 2s - loss: 1.2256 - acc: 0.1818 - auc: 0.6180 - precision: 0.0330 - recall: 0.9354 - f1: 0.0618\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: 1.2259 - acc: 0.1815 - auc: 0.6164 - precision: 0.0329 - recall: 0.9331 - f1: 0.0617\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: 1.2259 - acc: 0.1814 - auc: 0.6156 - precision: 0.0328 - recall: 0.9319 - f1: 0.0616\n",
            " 590/1170 [==============>...............] - ETA: 2s - loss: 1.2256 - acc: 0.1813 - auc: 0.6155 - precision: 0.0327 - recall: 0.9320 - f1: 0.0613\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: 1.2255 - acc: 0.1812 - auc: 0.6155 - precision: 0.0326 - recall: 0.9320 - f1: 0.0612\n",
            " 598/1170 [==============>...............] - ETA: 2s - loss: 1.2253 - acc: 0.1811 - auc: 0.6161 - precision: 0.0325 - recall: 0.9325 - f1: 0.0610\n",
            " 612/1170 [==============>...............] - ETA: 2s - loss: 1.2254 - acc: 0.1809 - auc: 0.6164 - precision: 0.0325 - recall: 0.9308 - f1: 0.0611\n",
            " 626/1170 [===============>..............] - ETA: 2s - loss: 1.2248 - acc: 0.1811 - auc: 0.6161 - precision: 0.0323 - recall: 0.9319 - f1: 0.0607\n",
            " 633/1170 [===============>..............] - ETA: 2s - loss: 1.2247 - acc: 0.1814 - auc: 0.6160 - precision: 0.0325 - recall: 0.9313 - f1: 0.0610\n",
            " 637/1170 [===============>..............] - ETA: 2s - loss: 1.2243 - acc: 0.1816 - auc: 0.6167 - precision: 0.0324 - recall: 0.9316 - f1: 0.0608\n",
            " 650/1170 [===============>..............] - ETA: 2s - loss: 1.2245 - acc: 0.1815 - auc: 0.6181 - precision: 0.0325 - recall: 0.9315 - f1: 0.0610\n",
            " 661/1170 [===============>..............] - ETA: 2s - loss: 1.2250 - acc: 0.1810 - auc: 0.6177 - precision: 0.0325 - recall: 0.9325 - f1: 0.0609\n",
            " 666/1170 [================>.............] - ETA: 2s - loss: 1.2250 - acc: 0.1808 - auc: 0.6185 - precision: 0.0324 - recall: 0.9329 - f1: 0.0608\n",
            " 681/1170 [================>.............] - ETA: 2s - loss: 1.2263 - acc: 0.1795 - auc: 0.6157 - precision: 0.0322 - recall: 0.9310 - f1: 0.0605\n",
            " 700/1170 [================>.............] - ETA: 2s - loss: 1.2262 - acc: 0.1792 - auc: 0.6163 - precision: 0.0322 - recall: 0.9313 - f1: 0.0604\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: 1.2261 - acc: 0.1799 - auc: 0.6158 - precision: 0.0324 - recall: 0.9316 - f1: 0.0608\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: 1.2262 - acc: 0.1796 - auc: 0.6150 - precision: 0.0322 - recall: 0.9323 - f1: 0.0605\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: 1.2262 - acc: 0.1790 - auc: 0.6138 - precision: 0.0321 - recall: 0.9333 - f1: 0.0602\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: 1.2258 - acc: 0.1794 - auc: 0.6155 - precision: 0.0321 - recall: 0.9330 - f1: 0.0604\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: 1.2261 - acc: 0.1794 - auc: 0.6151 - precision: 0.0321 - recall: 0.9336 - f1: 0.0604\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: 1.2262 - acc: 0.1793 - auc: 0.6138 - precision: 0.0321 - recall: 0.9328 - f1: 0.0603\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 1.2258 - acc: 0.1798 - auc: 0.6154 - precision: 0.0323 - recall: 0.9342 - f1: 0.0606\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 1.2265 - acc: 0.1795 - auc: 0.6124 - precision: 0.0323 - recall: 0.9330 - f1: 0.0606\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: 1.2267 - acc: 0.1794 - auc: 0.6116 - precision: 0.0322 - recall: 0.9329 - f1: 0.0605\n",
            " 828/1170 [====================>.........] - ETA: 1s - loss: 1.2263 - acc: 0.1792 - auc: 0.6126 - precision: 0.0319 - recall: 0.9335 - f1: 0.0599\n",
            " 847/1170 [====================>.........] - ETA: 1s - loss: 1.2266 - acc: 0.1793 - auc: 0.6108 - precision: 0.0319 - recall: 0.9325 - f1: 0.0599\n",
            " 869/1170 [=====================>........] - ETA: 1s - loss: 1.2268 - acc: 0.1794 - auc: 0.6107 - precision: 0.0322 - recall: 0.9334 - f1: 0.0603\n",
            " 886/1170 [=====================>........] - ETA: 1s - loss: 1.2272 - acc: 0.1795 - auc: 0.6094 - precision: 0.0322 - recall: 0.9335 - f1: 0.0604\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: 1.2275 - acc: 0.1798 - auc: 0.6096 - precision: 0.0325 - recall: 0.9352 - f1: 0.0610\n",
            " 914/1170 [======================>.......] - ETA: 1s - loss: 1.2275 - acc: 0.1796 - auc: 0.6092 - precision: 0.0325 - recall: 0.9360 - f1: 0.0609\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: 1.2275 - acc: 0.1790 - auc: 0.6081 - precision: 0.0320 - recall: 0.9344 - f1: 0.0601\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: 1.2274 - acc: 0.1794 - auc: 0.6078 - precision: 0.0321 - recall: 0.9349 - f1: 0.0603\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: 1.2279 - acc: 0.1787 - auc: 0.6070 - precision: 0.0320 - recall: 0.9327 - f1: 0.0601\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 1.2279 - acc: 0.1789 - auc: 0.6073 - precision: 0.0320 - recall: 0.9329 - f1: 0.0601\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: 1.2274 - acc: 0.1792 - auc: 0.6071 - precision: 0.0320 - recall: 0.9340 - f1: 0.0600\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: 1.2274 - acc: 0.1795 - auc: 0.6084 - precision: 0.0322 - recall: 0.9348 - f1: 0.0605\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: 1.2272 - acc: 0.1797 - auc: 0.6105 - precision: 0.0323 - recall: 0.9357 - f1: 0.0606\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 1.2275 - acc: 0.1796 - auc: 0.6104 - precision: 0.0324 - recall: 0.9366 - f1: 0.0609\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: 1.2277 - acc: 0.1794 - auc: 0.6105 - precision: 0.0326 - recall: 0.9380 - f1: 0.0612\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: 1.2277 - acc: 0.1795 - auc: 0.6118 - precision: 0.0327 - recall: 0.9374 - f1: 0.0612\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 1.2281 - acc: 0.1792 - auc: 0.6103 - precision: 0.0328 - recall: 0.9386 - f1: 0.0614\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: 1.2284 - acc: 0.1791 - auc: 0.6104 - precision: 0.0328 - recall: 0.9387 - f1: 0.0614\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: 1.2279 - acc: 0.1794 - auc: 0.6108 - precision: 0.0327 - recall: 0.9393 - f1: 0.0613\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.1796 - auc: 0.6107 - precision: 0.0328 - recall: 0.9400 - f1: 0.0614\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: 1.2278 - acc: 0.1797 - auc: 0.6114 - precision: 0.0328 - recall: 0.9403 - f1: 0.0616\n",
            "\n",
            "2023-07-30 23:05:53.733325                             \n",
            "100%|| 57/57 [13:56<00:00, 836.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 57 trials to 58 (+1) trials\n",
            "2023-07-30 23:05:53.949459                             \n",
            "{'name': 'Adam', 'learning_rate': 0.29801146033562625, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_57', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_114_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_114', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_114', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_114', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_114', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_115', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_115', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_115', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_115', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_57', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_228', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_171', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_229', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_172', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_230', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_173', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_231', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 57/58 [04:32<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0022s). Check your callbacks.\n",
            "  22/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  41/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:10:32.215625                             \n",
            "100%|| 58/58 [04:38<00:00, 278.39s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 58 trials to 59 (+1) trials\n",
            "2023-07-30 23:10:32.409469                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6036989399345184, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_58', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_116_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_116', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_116', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_116', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_116', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_117', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_117', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_117', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_117', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_58', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_232', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_174', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_233', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_175', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_234', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_176', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_235', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 58/59 [04:32<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0017s). Check your callbacks.\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  62/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:15:10.924379                             \n",
            "100%|| 59/59 [04:38<00:00, 278.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 59 trials to 60 (+1) trials\n",
            "2023-07-30 23:15:11.298830                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7184720653373794, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_59', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_118_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_118', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_118', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_118', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_118', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_119', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_119', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_119', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_119', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_59', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_236', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_177', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_237', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_178', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_238', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_179', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_239', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:23:15.300598                             \n",
            "100%|| 60/60 [08:04<00:00, 484.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 60 trials to 61 (+1) trials\n",
            "2023-07-30 23:23:15.496788                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9444304103657876, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_60', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_120_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_120', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_120', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_120', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_120', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_121', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_121', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_121', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_121', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_60', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_240', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_180', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_241', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_181', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_242', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_182', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_243', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:31:08.571484                             \n",
            "100%|| 61/61 [07:53<00:00, 473.14s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 61 trials to 62 (+1) trials\n",
            "2023-07-30 23:31:08.725461                             \n",
            "{'name': 'Adam', 'learning_rate': 0.953368986716328, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_61', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_122_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_122', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_122', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_122', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_122', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_123', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_123', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_123', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_123', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_61', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_244', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_183', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_245', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_184', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_246', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_185', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_247', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 215/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:39:06.784438                             \n",
            "100%|| 62/62 [07:58<00:00, 478.12s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 62 trials to 63 (+1) trials\n",
            "2023-07-30 23:39:06.855106                             \n",
            "{'name': 'Adam', 'learning_rate': 0.13229624691303804, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_62', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_124_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_124', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_124', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_124', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_124', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_125', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_125', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_125', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_125', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_62', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_248', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_186', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_249', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_187', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_250', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_188', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_251', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 62/63 [04:21<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  63/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:43:34.297034                             \n",
            "100%|| 63/63 [04:27<00:00, 267.51s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 63 trials to 64 (+1) trials\n",
            "2023-07-30 23:43:34.458570                             \n",
            "{'name': 'Adam', 'learning_rate': 0.21959673885547523, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_63', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_126_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_126', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_126', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_126', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_126', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_127', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_127', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_127', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_127', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_63', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_252', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_189', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_253', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_190', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_254', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_191', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_255', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  69/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:48:39.195663                             \n",
            "100%|| 64/64 [05:04<00:00, 304.81s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 64 trials to 65 (+1) trials\n",
            "2023-07-30 23:48:39.270916                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5768952264140849, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_64', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_128_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_128', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_128', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_128', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_128', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_129', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_129', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_129', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_129', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_64', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_256', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_192', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_257', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_193', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_258', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_194', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_259', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  69/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:52:30.683291                             \n",
            "100%|| 65/65 [03:51<00:00, 231.47s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 65 trials to 66 (+1) trials\n",
            "2023-07-30 23:52:30.747024                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8324671365502878, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_65', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_130_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_130', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_130', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_130', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_130', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_131', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_131', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_131', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_131', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_65', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_260', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_195', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_261', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_196', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_262', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_197', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_263', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:00:46.983595                             \n",
            "100%|| 66/66 [08:16<00:00, 496.34s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 66 trials to 67 (+1) trials\n",
            "2023-07-31 00:00:47.113493                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3579129518848078, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_66', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_132_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_132', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_132', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_132', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_132', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_133', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_133', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_133', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_133', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_66', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_264', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_198', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_265', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_199', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_266', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_200', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_267', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 23s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:09:47.263398                             \n",
            "100%|| 67/67 [09:00<00:00, 540.24s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 67 trials to 68 (+1) trials\n",
            "2023-07-31 00:09:47.360996                             \n",
            "{'name': 'Adam', 'learning_rate': 0.398611709446185, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_67', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_134_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_134', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_134', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_134', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_134', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_135', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_135', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_135', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_135', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_67', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_268', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_201', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_269', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_202', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_270', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_203', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_271', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:14:15.667855                             \n",
            "100%|| 68/68 [04:28<00:00, 268.39s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 68 trials to 69 (+1) trials\n",
            "2023-07-31 00:14:15.756130                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00020438666294315516, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_68', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_136_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_136', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_136', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_136', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_136', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_137', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_137', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_137', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_137', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_68', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_272', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_204', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_273', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_205', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_274', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_206', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_275', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.8549 - acc: 0.0625 - auc: 0.2250 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            " 99%|| 68/69 [04:56<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
            "  24/1170 [..............................] - ETA: 2s - loss: 1.6928 - acc: 0.0339 - auc: 0.6389 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  40/1170 [>.............................] - ETA: 3s - loss: 1.6974 - acc: 0.0344 - auc: 0.6233 - precision: 0.0344 - recall: 1.0000 - f1: 0.0638\n",
            "  52/1170 [>.............................] - ETA: 3s - loss: 1.7036 - acc: 0.0361 - auc: 0.6236 - precision: 0.0361 - recall: 1.0000 - f1: 0.0671\n",
            "  61/1170 [>.............................] - ETA: 3s - loss: 1.7022 - acc: 0.0369 - auc: 0.6147 - precision: 0.0369 - recall: 1.0000 - f1: 0.0688\n",
            "  71/1170 [>.............................] - ETA: 4s - loss: 1.7038 - acc: 0.0365 - auc: 0.6206 - precision: 0.0365 - recall: 1.0000 - f1: 0.0683\n",
            "  75/1170 [>.............................] - ETA: 4s - loss: 1.7022 - acc: 0.0354 - auc: 0.6241 - precision: 0.0354 - recall: 1.0000 - f1: 0.0662\n",
            "  83/1170 [=>............................] - ETA: 5s - loss: 1.6966 - acc: 0.0335 - auc: 0.6299 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            "  92/1170 [=>............................] - ETA: 5s - loss: 1.6916 - acc: 0.0323 - auc: 0.6302 - precision: 0.0323 - recall: 1.0000 - f1: 0.0605\n",
            " 105/1170 [=>............................] - ETA: 5s - loss: 1.6917 - acc: 0.0312 - auc: 0.6372 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 124/1170 [==>...........................] - ETA: 4s - loss: 1.6913 - acc: 0.0320 - auc: 0.6436 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 134/1170 [==>...........................] - ETA: 4s - loss: 1.6926 - acc: 0.0319 - auc: 0.6513 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 148/1170 [==>...........................] - ETA: 4s - loss: 1.6958 - acc: 0.0334 - auc: 0.6465 - precision: 0.0334 - recall: 1.0000 - f1: 0.0627\n",
            " 164/1170 [===>..........................] - ETA: 4s - loss: 1.6933 - acc: 0.0316 - auc: 0.6455 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 178/1170 [===>..........................] - ETA: 4s - loss: 1.6956 - acc: 0.0316 - auc: 0.6466 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 191/1170 [===>..........................] - ETA: 4s - loss: 1.6937 - acc: 0.0308 - auc: 0.6497 - precision: 0.0308 - recall: 1.0000 - f1: 0.0578\n",
            " 201/1170 [====>.........................] - ETA: 4s - loss: 1.6924 - acc: 0.0309 - auc: 0.6497 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 212/1170 [====>.........................] - ETA: 4s - loss: 1.6951 - acc: 0.0311 - auc: 0.6376 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 226/1170 [====>.........................] - ETA: 4s - loss: 1.6947 - acc: 0.0306 - auc: 0.6335 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 238/1170 [=====>........................] - ETA: 4s - loss: 1.6941 - acc: 0.0305 - auc: 0.6306 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: 1.6943 - acc: 0.0301 - auc: 0.6286 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 265/1170 [=====>........................] - ETA: 3s - loss: 1.6943 - acc: 0.0307 - auc: 0.6292 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 279/1170 [======>.......................] - ETA: 3s - loss: 1.6932 - acc: 0.0301 - auc: 0.6296 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: 1.6949 - acc: 0.0302 - auc: 0.6210 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 310/1170 [======>.......................] - ETA: 3s - loss: 1.6943 - acc: 0.0296 - auc: 0.6203 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 321/1170 [=======>......................] - ETA: 3s - loss: 1.6936 - acc: 0.0295 - auc: 0.6228 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: 1.6940 - acc: 0.0302 - auc: 0.6259 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 342/1170 [=======>......................] - ETA: 3s - loss: 1.6932 - acc: 0.0300 - auc: 0.6251 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 354/1170 [========>.....................] - ETA: 3s - loss: 1.6934 - acc: 0.0304 - auc: 0.6259 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 365/1170 [========>.....................] - ETA: 3s - loss: 1.6929 - acc: 0.0304 - auc: 0.6259 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 377/1170 [========>.....................] - ETA: 3s - loss: 1.6921 - acc: 0.0303 - auc: 0.6247 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 402/1170 [=========>....................] - ETA: 3s - loss: 1.6938 - acc: 0.0310 - auc: 0.6207 - precision: 0.0310 - recall: 1.0000 - f1: 0.0584\n",
            " 425/1170 [=========>....................] - ETA: 3s - loss: 1.6929 - acc: 0.0309 - auc: 0.6233 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 435/1170 [==========>...................] - ETA: 3s - loss: 1.6923 - acc: 0.0307 - auc: 0.6253 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: 1.6915 - acc: 0.0303 - auc: 0.6262 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: 1.6919 - acc: 0.0304 - auc: 0.6280 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: 1.6916 - acc: 0.0302 - auc: 0.6282 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 480/1170 [===========>..................] - ETA: 2s - loss: 1.6914 - acc: 0.0303 - auc: 0.6284 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 493/1170 [===========>..................] - ETA: 2s - loss: 1.6908 - acc: 0.0300 - auc: 0.6289 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 505/1170 [===========>..................] - ETA: 2s - loss: 1.6900 - acc: 0.0300 - auc: 0.6291 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 514/1170 [============>.................] - ETA: 2s - loss: 1.6895 - acc: 0.0300 - auc: 0.6307 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 523/1170 [============>.................] - ETA: 2s - loss: 1.6902 - acc: 0.0299 - auc: 0.6284 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 535/1170 [============>.................] - ETA: 2s - loss: 1.6903 - acc: 0.0296 - auc: 0.6266 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 549/1170 [=============>................] - ETA: 2s - loss: 1.6909 - acc: 0.0299 - auc: 0.6249 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 563/1170 [=============>................] - ETA: 2s - loss: 1.6908 - acc: 0.0297 - auc: 0.6262 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 572/1170 [=============>................] - ETA: 2s - loss: 1.6907 - acc: 0.0298 - auc: 0.6260 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: 1.6912 - acc: 0.0298 - auc: 0.6240 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: 1.6905 - acc: 0.0296 - auc: 0.6235 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: 1.6902 - acc: 0.0295 - auc: 0.6241 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 616/1170 [==============>...............] - ETA: 2s - loss: 1.6905 - acc: 0.0294 - auc: 0.6229 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 638/1170 [===============>..............] - ETA: 2s - loss: 1.6892 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 660/1170 [===============>..............] - ETA: 2s - loss: 1.6900 - acc: 0.0295 - auc: 0.6230 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 671/1170 [================>.............] - ETA: 2s - loss: 1.6903 - acc: 0.0292 - auc: 0.6227 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 684/1170 [================>.............] - ETA: 2s - loss: 1.6909 - acc: 0.0292 - auc: 0.6221 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 1.6908 - acc: 0.0292 - auc: 0.6221 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: 1.6910 - acc: 0.0295 - auc: 0.6211 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: 1.6911 - acc: 0.0293 - auc: 0.6207 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: 1.6908 - acc: 0.0292 - auc: 0.6206 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: 1.6906 - acc: 0.0291 - auc: 0.6203 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: 1.6902 - acc: 0.0292 - auc: 0.6230 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: 1.6904 - acc: 0.0291 - auc: 0.6217 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: 1.6911 - acc: 0.0293 - auc: 0.6219 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: 1.6911 - acc: 0.0292 - auc: 0.6212 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 1.6917 - acc: 0.0293 - auc: 0.6200 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 810/1170 [===================>..........] - ETA: 1s - loss: 1.6927 - acc: 0.0293 - auc: 0.6193 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 823/1170 [====================>.........] - ETA: 1s - loss: 1.6921 - acc: 0.0290 - auc: 0.6197 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 830/1170 [====================>.........] - ETA: 1s - loss: 1.6919 - acc: 0.0289 - auc: 0.6205 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: 1.6918 - acc: 0.0289 - auc: 0.6206 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 843/1170 [====================>.........] - ETA: 1s - loss: 1.6926 - acc: 0.0290 - auc: 0.6201 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 853/1170 [====================>.........] - ETA: 1s - loss: 1.6925 - acc: 0.0290 - auc: 0.6203 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: 1.6927 - acc: 0.0291 - auc: 0.6199 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 879/1170 [=====================>........] - ETA: 1s - loss: 1.6932 - acc: 0.0291 - auc: 0.6199 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: 1.6930 - acc: 0.0292 - auc: 0.6208 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 900/1170 [======================>.......] - ETA: 1s - loss: 1.6931 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 911/1170 [======================>.......] - ETA: 1s - loss: 1.6933 - acc: 0.0294 - auc: 0.6213 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 919/1170 [======================>.......] - ETA: 1s - loss: 1.6930 - acc: 0.0293 - auc: 0.6208 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 931/1170 [======================>.......] - ETA: 1s - loss: 1.6930 - acc: 0.0291 - auc: 0.6202 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 1.6931 - acc: 0.0292 - auc: 0.6197 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6208 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: 1.6928 - acc: 0.0291 - auc: 0.6212 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: 1.6931 - acc: 0.0290 - auc: 0.6200 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: 1.6935 - acc: 0.0290 - auc: 0.6185 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: 1.6930 - acc: 0.0290 - auc: 0.6190 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: 1.6929 - acc: 0.0290 - auc: 0.6190 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6205 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6207 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 1.6926 - acc: 0.0292 - auc: 0.6214 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: 1.6929 - acc: 0.0293 - auc: 0.6214 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.6930 - acc: 0.0293 - auc: 0.6213 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: 1.6925 - acc: 0.0294 - auc: 0.6247 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 1.6935 - acc: 0.0295 - auc: 0.6229 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 1.6937 - acc: 0.0295 - auc: 0.6219 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 1.6940 - acc: 0.0295 - auc: 0.6211 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.6935 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: 1.6938 - acc: 0.0296 - auc: 0.6223 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 00:19:18.245401                             \n",
            "100%|| 69/69 [05:02<00:00, 302.57s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 69 trials to 70 (+1) trials\n",
            "2023-07-31 00:19:18.348550                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7478430445712738, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_69', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_138_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_138', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_138', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_138', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_138', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_139', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_139', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_139', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_139', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_69', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_276', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_207', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_277', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_208', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_278', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_209', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_279', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  61/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:28:39.897339                             \n",
            "100%|| 70/70 [09:21<00:00, 561.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 70 trials to 71 (+1) trials\n",
            "2023-07-31 00:28:40.212711                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6645233154570755, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_70', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_140_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_140', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_140', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_140', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_140', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_141', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_141', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_141', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_141', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_70', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_280', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_210', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_281', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_211', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_282', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_212', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_283', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 70/71 [09:25<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:38:11.708763                             \n",
            "100%|| 71/71 [09:31<00:00, 571.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 71 trials to 72 (+1) trials\n",
            "2023-07-31 00:38:11.867400                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6772052987280605, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_71', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_142_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_142', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_142', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_142', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_142', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_143', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_143', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_143', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_143', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_71', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_284', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_213', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_285', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_214', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_286', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_215', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_287', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 24s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  19/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  36/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  46/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  64/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 4s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 4s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 4s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 4s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:42:57.751566                             \n",
            "100%|| 72/72 [04:46<00:00, 286.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 72 trials to 73 (+1) trials\n",
            "2023-07-31 00:42:57.970976                             \n",
            "{'name': 'Adam', 'learning_rate': 0.33104385724238167, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_72', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_144_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_144', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_144', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_144', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_144', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_145', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_145', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_145', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_145', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_72', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_288', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_216', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_289', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_217', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_290', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_218', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_291', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 72/73 [08:36<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0017s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:51:40.307012                             \n",
            "100%|| 73/73 [08:42<00:00, 522.44s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 73 trials to 74 (+1) trials\n",
            "2023-07-31 00:51:40.426397                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3090876398904634, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_73', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_146_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_146', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_146', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_146', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_146', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_147', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_147', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_147', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_147', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_73', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_292', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_219', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_293', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_220', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_294', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_221', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_295', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:00:19.442277                             \n",
            "100%|| 74/74 [08:39<00:00, 519.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 74 trials to 75 (+1) trials\n",
            "2023-07-31 01:00:19.526390                             \n",
            "{'name': 'Adam', 'learning_rate': 0.09873581962166446, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_74', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_148_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_148', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_148', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_148', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_148', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_149', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_149', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_149', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_149', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_74', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_296', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_222', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_297', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_223', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_298', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_224', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_299', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 74/75 [04:38<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:05:04.392253                             \n",
            "100%|| 75/75 [04:44<00:00, 284.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 75 trials to 76 (+1) trials\n",
            "2023-07-31 01:05:04.484346                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3530785775140963, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_75', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_150_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_150', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_150', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_150', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_150', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_151', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_151', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_151', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_151', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_75', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_300', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_225', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_301', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_226', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_302', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_227', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_303', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 75/76 [04:29<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:09:38.583007                             \n",
            "100%|| 76/76 [04:34<00:00, 274.20s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 76 trials to 77 (+1) trials\n",
            "2023-07-31 01:09:38.689083                             \n",
            "{'name': 'Adam', 'learning_rate': 0.38523016696577345, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_76', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_152_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_152', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_152', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_152', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_152', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_153', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_153', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_153', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_153', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_76', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_304', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_228', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_305', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_229', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_306', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_230', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_307', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  17/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9596 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:18:10.195919                             \n",
            "100%|| 77/77 [08:31<00:00, 511.61s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 77 trials to 78 (+1) trials\n",
            "2023-07-31 01:18:10.312405                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7565104372977938, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_77', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_154_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_154', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_154', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_154', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_154', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_155', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_155', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_155', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_155', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_77', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_308', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_231', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_309', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_232', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_310', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_233', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_311', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  64/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1038/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:22:53.762254                             \n",
            "100%|| 78/78 [04:43<00:00, 283.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 78 trials to 79 (+1) trials\n",
            "2023-07-31 01:22:53.860975                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8719978625410024, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_78', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_156_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_156', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_156', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_156', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_156', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_157', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_157', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_157', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_157', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_78', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_312', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_234', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_313', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_235', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_314', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_236', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_315', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 78/79 [04:13<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0027s). Check your callbacks.\n",
            "  29/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 295/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:27:11.802797                             \n",
            "100%|| 79/79 [04:18<00:00, 258.04s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 79 trials to 80 (+1) trials\n",
            "2023-07-31 01:27:11.908512                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4130029240297524, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_79', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_158_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_158', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_158', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_158', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_158', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_159', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_159', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_159', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_159', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_79', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_316', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_237', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_317', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_238', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_318', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_239', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_319', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:31:31.724769                             \n",
            "100%|| 80/80 [04:19<00:00, 259.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 80 trials to 81 (+1) trials\n",
            "2023-07-31 01:31:31.814017                             \n",
            "{'name': 'Adam', 'learning_rate': 1.1681975858502103e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_80', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_160_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_160', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_160', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_160', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_160', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_161', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_161', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_161', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_161', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_80', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_320', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_240', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_321', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_241', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_322', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_242', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_323', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 27s - loss: 2.5851 - acc: 0.1250 - auc: 0.3000 - precision: 0.0667 - recall: 1.0000 - f1: 0.1250\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.3936 - acc: 0.0423 - auc: 0.6024 - precision: 0.0343 - recall: 0.9737 - f1: 0.0640 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.3963 - acc: 0.0435 - auc: 0.6043 - precision: 0.0350 - recall: 0.9808 - f1: 0.0653\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: 2.4036 - acc: 0.0457 - auc: 0.5960 - precision: 0.0364 - recall: 0.9868 - f1: 0.0682\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: 2.3863 - acc: 0.0441 - auc: 0.6236 - precision: 0.0340 - recall: 0.9885 - f1: 0.0637\n",
            "  91/1170 [=>............................] - ETA: 3s - loss: 2.3759 - acc: 0.0422 - auc: 0.6149 - precision: 0.0323 - recall: 0.9894 - f1: 0.0605\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: 2.3711 - acc: 0.0426 - auc: 0.6204 - precision: 0.0316 - recall: 0.9910 - f1: 0.0594\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: 2.3746 - acc: 0.0434 - auc: 0.6127 - precision: 0.0322 - recall: 0.9917 - f1: 0.0604\n",
            " 123/1170 [==>...........................] - ETA: 3s - loss: 2.3755 - acc: 0.0440 - auc: 0.6203 - precision: 0.0324 - recall: 0.9921 - f1: 0.0609\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: 2.3719 - acc: 0.0424 - auc: 0.6280 - precision: 0.0318 - recall: 0.9928 - f1: 0.0599\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: 2.3765 - acc: 0.0427 - auc: 0.6141 - precision: 0.0323 - recall: 0.9867 - f1: 0.0607\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: 2.3787 - acc: 0.0435 - auc: 0.6114 - precision: 0.0325 - recall: 0.9812 - f1: 0.0611\n",
            " 164/1170 [===>..........................] - ETA: 3s - loss: 2.3723 - acc: 0.0423 - auc: 0.6055 - precision: 0.0314 - recall: 0.9819 - f1: 0.0591\n",
            " 178/1170 [===>..........................] - ETA: 3s - loss: 2.3725 - acc: 0.0421 - auc: 0.6005 - precision: 0.0314 - recall: 0.9833 - f1: 0.0591\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: 2.3684 - acc: 0.0414 - auc: 0.5994 - precision: 0.0308 - recall: 0.9846 - f1: 0.0580\n",
            " 217/1170 [====>.........................] - ETA: 3s - loss: 2.3670 - acc: 0.0416 - auc: 0.5930 - precision: 0.0306 - recall: 0.9859 - f1: 0.0576\n",
            " 237/1170 [=====>........................] - ETA: 3s - loss: 2.3666 - acc: 0.0417 - auc: 0.5861 - precision: 0.0306 - recall: 0.9871 - f1: 0.0575\n",
            " 257/1170 [=====>........................] - ETA: 3s - loss: 2.3671 - acc: 0.0415 - auc: 0.5836 - precision: 0.0306 - recall: 0.9881 - f1: 0.0576\n",
            " 282/1170 [======>.......................] - ETA: 3s - loss: 2.3639 - acc: 0.0407 - auc: 0.5839 - precision: 0.0301 - recall: 0.9890 - f1: 0.0568\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: 2.3635 - acc: 0.0401 - auc: 0.5754 - precision: 0.0298 - recall: 0.9825 - f1: 0.0561\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: 2.3618 - acc: 0.0397 - auc: 0.5765 - precision: 0.0295 - recall: 0.9829 - f1: 0.0557\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: 2.3626 - acc: 0.0397 - auc: 0.5843 - precision: 0.0297 - recall: 0.9839 - f1: 0.0560\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: 2.3641 - acc: 0.0403 - auc: 0.5871 - precision: 0.0301 - recall: 0.9851 - f1: 0.0566\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: 2.3651 - acc: 0.0405 - auc: 0.5869 - precision: 0.0301 - recall: 0.9805 - f1: 0.0567\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: 2.3631 - acc: 0.0404 - auc: 0.5896 - precision: 0.0298 - recall: 0.9811 - f1: 0.0562\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: 2.3687 - acc: 0.0412 - auc: 0.5885 - precision: 0.0307 - recall: 0.9823 - f1: 0.0578\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: 2.3670 - acc: 0.0407 - auc: 0.5866 - precision: 0.0304 - recall: 0.9826 - f1: 0.0573\n",
            " 418/1170 [=========>....................] - ETA: 2s - loss: 2.3693 - acc: 0.0412 - auc: 0.5886 - precision: 0.0308 - recall: 0.9807 - f1: 0.0579\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: 2.3673 - acc: 0.0408 - auc: 0.5872 - precision: 0.0304 - recall: 0.9790 - f1: 0.0572\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: 2.3653 - acc: 0.0406 - auc: 0.5858 - precision: 0.0301 - recall: 0.9795 - f1: 0.0566\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: 2.3650 - acc: 0.0409 - auc: 0.5894 - precision: 0.0301 - recall: 0.9804 - f1: 0.0567\n",
            " 495/1170 [===========>..................] - ETA: 2s - loss: 2.3629 - acc: 0.0403 - auc: 0.5896 - precision: 0.0297 - recall: 0.9789 - f1: 0.0560\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: 2.3625 - acc: 0.0399 - auc: 0.5878 - precision: 0.0296 - recall: 0.9797 - f1: 0.0559\n",
            " 533/1170 [============>.................] - ETA: 2s - loss: 2.3610 - acc: 0.0399 - auc: 0.5869 - precision: 0.0294 - recall: 0.9802 - f1: 0.0555\n",
            " 544/1170 [============>.................] - ETA: 2s - loss: 2.3613 - acc: 0.0400 - auc: 0.5850 - precision: 0.0295 - recall: 0.9807 - f1: 0.0556\n",
            " 550/1170 [=============>................] - ETA: 2s - loss: 2.3626 - acc: 0.0402 - auc: 0.5818 - precision: 0.0297 - recall: 0.9810 - f1: 0.0560\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: 2.3621 - acc: 0.0400 - auc: 0.5775 - precision: 0.0296 - recall: 0.9817 - f1: 0.0558\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 2.3605 - acc: 0.0397 - auc: 0.5754 - precision: 0.0293 - recall: 0.9804 - f1: 0.0552\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: 2.3604 - acc: 0.0396 - auc: 0.5771 - precision: 0.0293 - recall: 0.9810 - f1: 0.0553\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: 2.3608 - acc: 0.0395 - auc: 0.5758 - precision: 0.0293 - recall: 0.9816 - f1: 0.0553\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: 2.3602 - acc: 0.0394 - auc: 0.5769 - precision: 0.0293 - recall: 0.9821 - f1: 0.0552\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: 2.3597 - acc: 0.0394 - auc: 0.5800 - precision: 0.0292 - recall: 0.9824 - f1: 0.0551\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: 2.3576 - acc: 0.0392 - auc: 0.5780 - precision: 0.0289 - recall: 0.9828 - f1: 0.0545\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: 2.3594 - acc: 0.0396 - auc: 0.5770 - precision: 0.0292 - recall: 0.9838 - f1: 0.0550\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: 2.3576 - acc: 0.0391 - auc: 0.5797 - precision: 0.0290 - recall: 0.9843 - f1: 0.0546\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: 2.3592 - acc: 0.0390 - auc: 0.5768 - precision: 0.0292 - recall: 0.9851 - f1: 0.0550\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: 2.3592 - acc: 0.0390 - auc: 0.5767 - precision: 0.0292 - recall: 0.9854 - f1: 0.0550\n",
            " 817/1170 [===================>..........] - ETA: 1s - loss: 2.3577 - acc: 0.0387 - auc: 0.5773 - precision: 0.0290 - recall: 0.9855 - f1: 0.0546\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: 2.3563 - acc: 0.0384 - auc: 0.5789 - precision: 0.0287 - recall: 0.9857 - f1: 0.0542\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.3577 - acc: 0.0385 - auc: 0.5787 - precision: 0.0289 - recall: 0.9849 - f1: 0.0545\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: 2.3584 - acc: 0.0387 - auc: 0.5786 - precision: 0.0290 - recall: 0.9853 - f1: 0.0547\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.3592 - acc: 0.0389 - auc: 0.5762 - precision: 0.0292 - recall: 0.9856 - f1: 0.0549\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 2.3599 - acc: 0.0391 - auc: 0.5751 - precision: 0.0293 - recall: 0.9860 - f1: 0.0552\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 2.3586 - acc: 0.0390 - auc: 0.5743 - precision: 0.0291 - recall: 0.9864 - f1: 0.0548\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: 2.3577 - acc: 0.0389 - auc: 0.5749 - precision: 0.0290 - recall: 0.9868 - f1: 0.0546\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.3576 - acc: 0.0388 - auc: 0.5728 - precision: 0.0289 - recall: 0.9859 - f1: 0.0545\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: 2.3577 - acc: 0.0388 - auc: 0.5734 - precision: 0.0289 - recall: 0.9861 - f1: 0.0545\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: 2.3577 - acc: 0.0389 - auc: 0.5711 - precision: 0.0289 - recall: 0.9863 - f1: 0.0545\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: 2.3590 - acc: 0.0392 - auc: 0.5714 - precision: 0.0291 - recall: 0.9865 - f1: 0.0549\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: 2.3586 - acc: 0.0390 - auc: 0.5731 - precision: 0.0291 - recall: 0.9867 - f1: 0.0548\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: 2.3597 - acc: 0.0393 - auc: 0.5738 - precision: 0.0293 - recall: 0.9870 - f1: 0.0552\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.3598 - acc: 0.0393 - auc: 0.5758 - precision: 0.0293 - recall: 0.9873 - f1: 0.0552\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: 2.3602 - acc: 0.0394 - auc: 0.5765 - precision: 0.0294 - recall: 0.9876 - f1: 0.0554\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 2.3604 - acc: 0.0393 - auc: 0.5753 - precision: 0.0294 - recall: 0.9878 - f1: 0.0554\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 2.3609 - acc: 0.0394 - auc: 0.5753 - precision: 0.0295 - recall: 0.9880 - f1: 0.0556\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: 2.3605 - acc: 0.0393 - auc: 0.5753 - precision: 0.0295 - recall: 0.9881 - f1: 0.0555\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.3609 - acc: 0.0393 - auc: 0.5762 - precision: 0.0295 - recall: 0.9882 - f1: 0.0556\n",
            "\n",
            "2023-07-31 01:32:56.498271                             \n",
            "100%|| 81/81 [01:24<00:00, 84.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 81 trials to 82 (+1) trials\n",
            "2023-07-31 01:32:56.576356                             \n",
            "{'name': 'Adam', 'learning_rate': 0.07252457999446249, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_81', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_162_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_162', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_162', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_162', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_162', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_163', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_163', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_163', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_163', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_81', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_324', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_243', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_325', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_244', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_326', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_245', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_327', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  64/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:37:06.821744                             \n",
            "100%|| 82/82 [04:10<00:00, 250.32s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 82 trials to 83 (+1) trials\n",
            "2023-07-31 01:37:07.008827                             \n",
            "{'name': 'Adam', 'learning_rate': 0.37673140677437306, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_82', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_164_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_164', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_164', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_164', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_164', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_165', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_165', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_165', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_165', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_82', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_328', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_246', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_329', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_247', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_330', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_248', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_331', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 313/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:44:53.906677                             \n",
            "100%|| 83/83 [07:46<00:00, 466.98s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 83 trials to 84 (+1) trials\n",
            "2023-07-31 01:44:54.069166                             \n",
            "{'name': 'Adam', 'learning_rate': 0.878876693571428, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_83', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_166_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_166', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_166', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_166', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_166', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_167', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_167', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_167', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_167', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_83', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_332', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_249', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_333', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_250', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_334', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_251', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_335', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:48:00.550149                             \n",
            "100%|| 84/84 [03:06<00:00, 186.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 84 trials to 85 (+1) trials\n",
            "2023-07-31 01:48:00.625999                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6979557003572269, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_84', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_168_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_168', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_168', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_168', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_168', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_169', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_169', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_169', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_169', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_84', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_336', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_252', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_337', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_253', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_338', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_254', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_339', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:52:12.051337                             \n",
            "100%|| 85/85 [04:11<00:00, 251.50s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 85 trials to 86 (+1) trials\n",
            "2023-07-31 01:52:12.237753                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0031117218755322743, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_85', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_170_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_170', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_170', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_170', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_170', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_171', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_171', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_171', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_171', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_85', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_340', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_255', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_341', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_256', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_342', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_257', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_343', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:00:06.593802                             \n",
            "100%|| 86/86 [07:54<00:00, 474.47s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 86 trials to 87 (+1) trials\n",
            "2023-07-31 02:00:06.717584                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9115174049288475, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_86', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_172_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_172', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_172', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_172', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_172', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_173', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_173', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_173', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_173', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_86', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_344', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_258', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_345', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_259', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_346', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_260', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_347', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:04:18.168489                             \n",
            "100%|| 87/87 [04:11<00:00, 251.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 87 trials to 88 (+1) trials\n",
            "2023-07-31 02:04:18.339809                             \n",
            "{'name': 'Adam', 'learning_rate': 0.1942831107538497, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_87', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_174_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_174', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_174', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_174', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_174', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_175', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_175', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_175', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_175', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_87', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_348', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_261', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_349', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_262', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_350', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_263', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_351', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:08:27.181482                             \n",
            "100%|| 88/88 [04:08<00:00, 248.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 88 trials to 89 (+1) trials\n",
            "2023-07-31 02:08:27.339240                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0027993636104718246, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_88', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_176_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_176', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_176', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_176', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_176', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_177', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_177', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_177', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_177', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_88', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_352', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_264', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_353', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_265', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_354', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_266', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_355', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.6962 - acc: 0.0625 - auc: 0.3083 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  32/1170 [..............................] - ETA: 1s - loss: 2.4498 - acc: 0.0332 - auc: 0.6749 - precision: 0.0332 - recall: 1.0000 - f1: 0.0618 \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: 2.4686 - acc: 0.0360 - auc: 0.6604 - precision: 0.0360 - recall: 1.0000 - f1: 0.0672\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: 2.4758 - acc: 0.0371 - auc: 0.6582 - precision: 0.0371 - recall: 1.0000 - f1: 0.0692\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: 2.4679 - acc: 0.0338 - auc: 0.6574 - precision: 0.0338 - recall: 1.0000 - f1: 0.0633\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: 2.4690 - acc: 0.0316 - auc: 0.6580 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: 2.4732 - acc: 0.0309 - auc: 0.6577 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.4696 - acc: 0.0320 - auc: 0.6631 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: 2.4760 - acc: 0.0321 - auc: 0.6637 - precision: 0.0321 - recall: 1.0000 - f1: 0.0605\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: 2.4742 - acc: 0.0327 - auc: 0.6652 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: 2.4804 - acc: 0.0318 - auc: 0.6541 - precision: 0.0318 - recall: 1.0000 - f1: 0.0597\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: 2.4831 - acc: 0.0311 - auc: 0.6468 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: 2.4802 - acc: 0.0305 - auc: 0.6391 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: 2.4816 - acc: 0.0304 - auc: 0.6380 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: 2.4798 - acc: 0.0307 - auc: 0.6392 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 2.4783 - acc: 0.0301 - auc: 0.6431 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: 2.4817 - acc: 0.0300 - auc: 0.6394 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: 2.4788 - acc: 0.0295 - auc: 0.6438 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 2.4774 - acc: 0.0301 - auc: 0.6440 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: 2.4762 - acc: 0.0304 - auc: 0.6452 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: 2.4747 - acc: 0.0303 - auc: 0.6445 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: 2.4731 - acc: 0.0305 - auc: 0.6450 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: 2.4752 - acc: 0.0308 - auc: 0.6394 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: 2.4729 - acc: 0.0310 - auc: 0.6438 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: 2.4743 - acc: 0.0307 - auc: 0.6440 - precision: 0.0307 - recall: 1.0000 - f1: 0.0579\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: 2.4750 - acc: 0.0305 - auc: 0.6452 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: 2.4751 - acc: 0.0304 - auc: 0.6460 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: 2.4730 - acc: 0.0300 - auc: 0.6457 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: 2.4740 - acc: 0.0297 - auc: 0.6435 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: 2.4727 - acc: 0.0297 - auc: 0.6436 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: 2.4746 - acc: 0.0297 - auc: 0.6402 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: 2.4739 - acc: 0.0296 - auc: 0.6398 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: 2.4739 - acc: 0.0295 - auc: 0.6407 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: 2.4747 - acc: 0.0293 - auc: 0.6378 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 2.4733 - acc: 0.0296 - auc: 0.6369 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: 2.4746 - acc: 0.0293 - auc: 0.6374 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: 2.4766 - acc: 0.0292 - auc: 0.6362 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: 2.4765 - acc: 0.0292 - auc: 0.6371 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 2.4758 - acc: 0.0292 - auc: 0.6372 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: 2.4756 - acc: 0.0294 - auc: 0.6362 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: 2.4755 - acc: 0.0291 - auc: 0.6345 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: 2.4754 - acc: 0.0290 - auc: 0.6353 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 2.4751 - acc: 0.0292 - auc: 0.6363 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 2.4759 - acc: 0.0293 - auc: 0.6347 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: 2.4764 - acc: 0.0291 - auc: 0.6344 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: 2.4765 - acc: 0.0289 - auc: 0.6350 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 2.4774 - acc: 0.0290 - auc: 0.6327 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: 2.4777 - acc: 0.0291 - auc: 0.6317 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: 2.4789 - acc: 0.0292 - auc: 0.6315 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: 2.4782 - acc: 0.0295 - auc: 0.6329 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: 2.4787 - acc: 0.0292 - auc: 0.6318 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: 2.4785 - acc: 0.0291 - auc: 0.6302 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: 2.4785 - acc: 0.0291 - auc: 0.6296 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: 2.4798 - acc: 0.0290 - auc: 0.6282 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: 2.4803 - acc: 0.0290 - auc: 0.6272 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: 2.4792 - acc: 0.0290 - auc: 0.6301 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: 2.4790 - acc: 0.0290 - auc: 0.6288 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: 2.4791 - acc: 0.0291 - auc: 0.6284 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: 2.4789 - acc: 0.0292 - auc: 0.6304 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: 2.4790 - acc: 0.0294 - auc: 0.6311 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: 2.4791 - acc: 0.0295 - auc: 0.6325 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: 2.4800 - acc: 0.0295 - auc: 0.6309 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: 2.4805 - acc: 0.0295 - auc: 0.6300 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: 2.4801 - acc: 0.0296 - auc: 0.6295 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.4802 - acc: 0.0296 - auc: 0.6295 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 02:12:51.012003                             \n",
            "100%|| 89/89 [04:23<00:00, 263.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 89 trials to 90 (+1) trials\n",
            "2023-07-31 02:12:51.190083                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8944920446616569, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_89', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_178_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_178', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_178', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_178', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_178', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_179', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_179', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_179', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_179', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_89', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_356', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_267', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_357', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_268', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_358', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_269', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_359', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:20:39.466286                             \n",
            "100%|| 90/90 [07:48<00:00, 468.35s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 90 trials to 91 (+1) trials\n",
            "2023-07-31 02:20:39.549186                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2828772225492993, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_90', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_180_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_180', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_180', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_180', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_180', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_181', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_181', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_181', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_181', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_90', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_360', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_270', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_361', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_271', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_362', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_272', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_363', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:23:56.428166                             \n",
            "100%|| 91/91 [03:16<00:00, 196.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 91 trials to 92 (+1) trials\n",
            "2023-07-31 02:23:56.588463                             \n",
            "{'name': 'Adam', 'learning_rate': 0.12494711923816655, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_91', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_182_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_182', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_182', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_182', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_182', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_183', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_183', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_183', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_183', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_91', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_364', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_273', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_365', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_274', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_366', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_275', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_367', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 385/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:29:30.801066                             \n",
            "100%|| 92/92 [05:34<00:00, 334.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 92 trials to 93 (+1) trials\n",
            "2023-07-31 02:29:30.881192                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4779636574447378, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_92', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_184_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_184', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_184', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_184', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_184', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_185', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_185', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_185', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_185', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_92', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_368', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_276', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_369', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_277', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_370', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_278', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_371', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1075/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:37:13.374198                             \n",
            "100%|| 93/93 [07:42<00:00, 462.56s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 93 trials to 94 (+1) trials\n",
            "2023-07-31 02:37:13.519540                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9016314767543332, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_93', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_186_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_186', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_186', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_186', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_186', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_187', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_187', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_187', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_187', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_93', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_372', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_279', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_373', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_280', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_374', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_281', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_375', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:45:20.127697                             \n",
            "100%|| 94/94 [08:06<00:00, 486.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 94 trials to 95 (+1) trials\n",
            "2023-07-31 02:45:20.195780                             \n",
            "{'name': 'Adam', 'learning_rate': 0.11889646878078022, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_94', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_188_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_188', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_188', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_188', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_188', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_189', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_189', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_189', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_189', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_94', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_376', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_282', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_377', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_283', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_378', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_284', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_379', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:53:12.098944                             \n",
            "100%|| 95/95 [07:51<00:00, 471.97s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 95 trials to 96 (+1) trials\n",
            "2023-07-31 02:53:12.263048                             \n",
            "{'name': 'Adam', 'learning_rate': 0.39236795258078544, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_95', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_190_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_190', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_190', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_190', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_190', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_191', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_191', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_191', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_191', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_95', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_380', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_285', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_381', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_286', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_382', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_287', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_383', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:56:13.962318                             \n",
            "100%|| 96/96 [03:01<00:00, 181.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 96 trials to 97 (+1) trials\n",
            "2023-07-31 02:56:14.046131                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3017889324799028, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_96', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_192_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_192', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_192', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_192', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_192', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_193', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_193', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_193', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_193', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_96', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_384', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_288', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_385', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_289', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_386', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_290', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_387', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:59:24.141754                             \n",
            "100%|| 97/97 [03:10<00:00, 190.16s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 97 trials to 98 (+1) trials\n",
            "2023-07-31 02:59:24.217485                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5399404399508391, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_97', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_194_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_194', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_194', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_194', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_194', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_195', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_195', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_195', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_195', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_97', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_388', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_291', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_389', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_292', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_390', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_293', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_391', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:07:32.685690                             \n",
            "100%|| 98/98 [08:08<00:00, 488.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 98 trials to 99 (+1) trials\n",
            "2023-07-31 03:07:32.871898                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7967569710679964, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_98', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_196_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_196', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_196', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_196', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_196', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_197', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_197', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_197', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_197', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_98', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_392', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_294', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_393', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_295', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_394', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_296', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_395', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:10:49.254990                             \n",
            "100%|| 99/99 [03:16<00:00, 196.45s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 99 trials to 100 (+1) trials\n",
            "2023-07-31 03:10:49.326396                              \n",
            "{'name': 'Adam', 'learning_rate': 0.6861134019645244, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_99', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_198_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_198', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_198', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_198', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_198', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_199', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_199', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_199', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_199', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_99', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_396', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_297', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_397', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_298', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_398', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_299', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_399', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:18:42.955118                              \n",
            "100%|| 100/100 [07:53<00:00, 473.70s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 100 trials to 101 (+1) trials\n",
            "2023-07-31 03:18:43.111069                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7667599474229218, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_100', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_200_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_200', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_200', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_200', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_200', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_201', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_201', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_201', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_201', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_100', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_400', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_300', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_401', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_301', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_402', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_302', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_403', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:22:00.381169                               \n",
            "100%|| 101/101 [03:17<00:00, 197.35s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 101 trials to 102 (+1) trials\n",
            "2023-07-31 03:22:00.477208                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3486892825076603, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_101', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_202_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_202', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_202', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_202', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_202', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_203', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_203', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_203', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_203', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_101', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_404', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_303', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_405', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_304', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_406', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_305', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_407', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:27:14.713552                               \n",
            "100%|| 102/102 [05:14<00:00, 314.31s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 102 trials to 103 (+1) trials\n",
            "2023-07-31 03:27:14.800222                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9392503619423661, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_102', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_204_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_204', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_204', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_204', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_204', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_205', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_205', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_205', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_205', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_102', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_408', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_306', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_409', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_307', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_410', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_308', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_411', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:35:10.916444                               \n",
            "100%|| 103/103 [07:56<00:00, 476.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 103 trials to 104 (+1) trials\n",
            "2023-07-31 03:35:11.002710                               \n",
            "{'name': 'Adam', 'learning_rate': 0.6991660246432225, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_103', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_206_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_206', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_206', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_206', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_206', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_207', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_207', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_207', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_207', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_103', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_412', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_309', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_413', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_310', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_414', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_311', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_415', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 21s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:40:28.754689                               \n",
            "100%|| 104/104 [05:17<00:00, 317.84s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 104 trials to 105 (+1) trials\n",
            "2023-07-31 03:40:28.946847                               \n",
            "{'name': 'Adam', 'learning_rate': 0.14100621278052225, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_104', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_208_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_208', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_208', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_208', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_208', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_209', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_209', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_209', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_209', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_104', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_416', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_312', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_417', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_313', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_418', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_314', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_419', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:43:44.421912                               \n",
            "100%|| 105/105 [03:15<00:00, 195.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 105 trials to 106 (+1) trials\n",
            "2023-07-31 03:43:44.513608                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1395947794568499, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_105', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_210_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_210', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_210', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_210', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_210', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_211', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_211', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_211', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_211', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_105', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_420', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_315', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_421', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_316', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_422', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_317', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_423', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:49:20.706747                               \n",
            "100%|| 106/106 [05:36<00:00, 336.27s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 106 trials to 107 (+1) trials\n",
            "2023-07-31 03:49:20.880808                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5725308147608181, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_106', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_212_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_212', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_212', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_212', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_212', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_213', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_213', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_213', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_213', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_106', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_424', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_318', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_425', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_319', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_426', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_320', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_427', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:52:37.498248                               \n",
            "100%|| 107/107 [03:16<00:00, 196.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 107 trials to 108 (+1) trials\n",
            "2023-07-31 03:52:37.569889                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1305476129192773, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_107', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_214_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_214', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_214', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_214', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_214', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_215', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_215', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_215', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_215', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_107', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_428', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_321', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_429', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_322', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_430', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_323', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_431', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:00:33.458029                               \n",
            "100%|| 108/108 [07:55<00:00, 475.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 108 trials to 109 (+1) trials\n",
            "2023-07-31 04:00:33.540546                               \n",
            "{'name': 'Adam', 'learning_rate': 0.582004298361701, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_108', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_216_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_216', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_216', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_216', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_216', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_217', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_217', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_217', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_217', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_108', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_432', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_324', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_433', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_325', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_434', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_326', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_435', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:08:37.261391                               \n",
            "100%|| 109/109 [08:03<00:00, 483.80s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 109 trials to 110 (+1) trials\n",
            "2023-07-31 04:08:37.354093                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3520405344522889, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_109', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_218_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_218', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_218', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_218', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_218', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_219', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_219', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_219', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_219', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_109', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_436', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_327', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_437', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_328', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_438', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_329', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_439', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:16:15.179155                               \n",
            "100%|| 110/110 [07:37<00:00, 457.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 110 trials to 111 (+1) trials\n",
            "2023-07-31 04:16:15.351619                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9610395331932369, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_110', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_220_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_220', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_220', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_220', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_220', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_221', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_221', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_221', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_221', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_110', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_440', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_330', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_441', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_331', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_442', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_332', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_443', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:21:18.868778                               \n",
            "100%|| 111/111 [05:03<00:00, 303.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 111 trials to 112 (+1) trials\n",
            "2023-07-31 04:21:18.947853                               \n",
            "{'name': 'Adam', 'learning_rate': 0.35221170078001734, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_111', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_222_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_222', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_222', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_222', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_222', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_223', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_223', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_223', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_223', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_111', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_444', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_333', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_445', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_334', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_446', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_335', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_447', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:24:18.659430                               \n",
            "100%|| 112/112 [02:59<00:00, 179.79s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 112 trials to 113 (+1) trials\n",
            "2023-07-31 04:24:18.744936                               \n",
            "{'name': 'Adam', 'learning_rate': 0.10317751681800515, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_112', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_224_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_224', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_224', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_224', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_224', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_225', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_225', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_225', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_225', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_112', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_448', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_336', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_449', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_337', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_450', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_338', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_451', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:32:06.607046                               \n",
            "100%|| 113/113 [07:47<00:00, 467.93s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 113 trials to 114 (+1) trials\n",
            "2023-07-31 04:32:06.681009                               \n",
            "{'name': 'Adam', 'learning_rate': 0.795454837013674, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_113', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_226_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_226', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_226', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_226', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_226', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_227', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_227', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_227', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_227', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_113', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_452', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_339', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_453', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_340', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_454', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_341', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_455', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:39:41.571444                               \n",
            "100%|| 114/114 [07:34<00:00, 454.97s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 114 trials to 115 (+1) trials\n",
            "2023-07-31 04:39:41.740757                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9858889385962811, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_114', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_228_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_228', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_228', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_228', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_228', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_229', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_229', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_229', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_229', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_114', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_456', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_342', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_457', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_343', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_458', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_344', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_459', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 332/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:47:28.751844                               \n",
            "100%|| 115/115 [07:47<00:00, 467.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 115 trials to 116 (+1) trials\n",
            "2023-07-31 04:47:28.833662                               \n",
            "{'name': 'Adam', 'learning_rate': 0.42467851342662793, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_115', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_230_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_230', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_230', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_230', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_230', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_231', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_231', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_231', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_231', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_115', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_460', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_345', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_461', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_346', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_462', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_347', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_463', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:52:45.550570                               \n",
            "100%|| 116/116 [05:16<00:00, 316.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 116 trials to 117 (+1) trials\n",
            "2023-07-31 04:52:45.704135                               \n",
            "{'name': 'Adam', 'learning_rate': 0.834045910594486, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_116', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_232_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_232', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_232', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_232', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_232', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_233', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_233', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_233', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_233', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_116', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_464', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_348', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_465', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_349', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_466', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_350', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_467', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:55:49.799368                               \n",
            "100%|| 117/117 [03:04<00:00, 184.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 117 trials to 118 (+1) trials\n",
            "2023-07-31 04:55:49.878714                               \n",
            "{'name': 'Adam', 'learning_rate': 0.855118276704653, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_117', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_234_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_234', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_234', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_234', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_234', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_235', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_235', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_235', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_235', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_117', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_468', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_351', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_469', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_352', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_470', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_353', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_471', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:03:41.663390                               \n",
            "100%|| 118/118 [07:51<00:00, 471.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 118 trials to 119 (+1) trials\n",
            "2023-07-31 05:03:41.831377                               \n",
            "{'name': 'Adam', 'learning_rate': 0.46840495620850564, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_118', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_236_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_236', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_236', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_236', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_236', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_237', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_237', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_237', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_237', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_118', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_472', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_354', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_473', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_355', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_474', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_356', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_475', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:06:43.476476                               \n",
            "100%|| 119/119 [03:01<00:00, 181.72s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 119 trials to 120 (+1) trials\n",
            "2023-07-31 05:06:43.559794                               \n",
            "{'name': 'Adam', 'learning_rate': 0.009498559702984662, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_119', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_238_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_238', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_238', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_238', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_238', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_239', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_239', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_239', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_239', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_119', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_476', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_357', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_477', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_358', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_478', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_359', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_479', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: 2.1760 - acc: 0.0625 - auc: 0.1833 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 2.0700 - acc: 0.0331 - auc: 0.6193 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.0763 - acc: 0.0353 - auc: 0.6341 - precision: 0.0353 - recall: 1.0000 - f1: 0.0658\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: 2.0787 - acc: 0.0371 - auc: 0.6273 - precision: 0.0371 - recall: 1.0000 - f1: 0.0692\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: 2.0760 - acc: 0.0338 - auc: 0.6270 - precision: 0.0338 - recall: 1.0000 - f1: 0.0634\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: 2.0773 - acc: 0.0312 - auc: 0.6287 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 2.0767 - acc: 0.0323 - auc: 0.6289 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: 2.0765 - acc: 0.0320 - auc: 0.6377 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: 2.0779 - acc: 0.0327 - auc: 0.6382 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: 2.0800 - acc: 0.0320 - auc: 0.6289 - precision: 0.0320 - recall: 1.0000 - f1: 0.0600\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: 2.0800 - acc: 0.0306 - auc: 0.6251 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: 2.0807 - acc: 0.0314 - auc: 0.6191 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 2.0803 - acc: 0.0307 - auc: 0.6164 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: 2.0800 - acc: 0.0304 - auc: 0.6160 - precision: 0.0304 - recall: 1.0000 - f1: 0.0571\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: 2.0796 - acc: 0.0307 - auc: 0.6102 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: 2.0804 - acc: 0.0302 - auc: 0.6097 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: 2.0795 - acc: 0.0295 - auc: 0.6140 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: 2.0786 - acc: 0.0303 - auc: 0.6168 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.0778 - acc: 0.0303 - auc: 0.6162 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: 2.0775 - acc: 0.0301 - auc: 0.6161 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: 2.0782 - acc: 0.0306 - auc: 0.6126 - precision: 0.0306 - recall: 1.0000 - f1: 0.0577\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: 2.0776 - acc: 0.0307 - auc: 0.6153 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: 2.0772 - acc: 0.0304 - auc: 0.6165 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: 2.0774 - acc: 0.0304 - auc: 0.6175 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 2.0774 - acc: 0.0303 - auc: 0.6163 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: 2.0766 - acc: 0.0300 - auc: 0.6197 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: 2.0760 - acc: 0.0300 - auc: 0.6190 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: 2.0767 - acc: 0.0298 - auc: 0.6172 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: 2.0765 - acc: 0.0297 - auc: 0.6163 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: 2.0768 - acc: 0.0298 - auc: 0.6138 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: 2.0770 - acc: 0.0298 - auc: 0.6120 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: 2.0771 - acc: 0.0295 - auc: 0.6106 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: 2.0770 - acc: 0.0295 - auc: 0.6101 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 2.0770 - acc: 0.0293 - auc: 0.6088 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 2.0766 - acc: 0.0295 - auc: 0.6109 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: 2.0770 - acc: 0.0292 - auc: 0.6104 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: 2.0774 - acc: 0.0293 - auc: 0.6098 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: 2.0772 - acc: 0.0292 - auc: 0.6108 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: 2.0767 - acc: 0.0293 - auc: 0.6114 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: 2.0770 - acc: 0.0291 - auc: 0.6097 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: 2.0768 - acc: 0.0291 - auc: 0.6105 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: 2.0774 - acc: 0.0291 - auc: 0.6075 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: 2.0775 - acc: 0.0292 - auc: 0.6081 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: 2.0774 - acc: 0.0294 - auc: 0.6087 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: 2.0780 - acc: 0.0293 - auc: 0.6074 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: 2.0777 - acc: 0.0289 - auc: 0.6090 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: 2.0779 - acc: 0.0290 - auc: 0.6066 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: 2.0785 - acc: 0.0291 - auc: 0.6070 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: 2.0786 - acc: 0.0294 - auc: 0.6055 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: 2.0787 - acc: 0.0292 - auc: 0.6038 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: 2.0788 - acc: 0.0291 - auc: 0.6035 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 2.0793 - acc: 0.0290 - auc: 0.6014 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: 2.0790 - acc: 0.0290 - auc: 0.6026 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: 2.0794 - acc: 0.0293 - auc: 0.6011 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 2.0792 - acc: 0.0293 - auc: 0.6040 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: 2.0795 - acc: 0.0294 - auc: 0.6035 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.0795 - acc: 0.0295 - auc: 0.6051 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: 2.0801 - acc: 0.0296 - auc: 0.6037 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 2.0803 - acc: 0.0296 - auc: 0.6016 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: 2.0801 - acc: 0.0295 - auc: 0.6018 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.0802 - acc: 0.0296 - auc: 0.6021 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 05:09:58.535887                               \n",
            "100%|| 120/120 [03:15<00:00, 195.05s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 120 trials to 121 (+1) trials\n",
            "2023-07-31 05:09:58.627861                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5413406918394045, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_120', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_240_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_240', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_240', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_240', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_240', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_241', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_241', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_241', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_241', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_120', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_480', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_360', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_481', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_361', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_482', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_362', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_483', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:17:51.081370                               \n",
            "100%|| 121/121 [07:52<00:00, 472.56s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 121 trials to 122 (+1) trials\n",
            "2023-07-31 05:17:51.283140                               \n",
            "{'name': 'Adam', 'learning_rate': 0.821771187159131, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_121', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_242_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_242', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_242', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_242', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_242', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_243', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_243', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_243', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_243', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_121', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_484', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_363', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_485', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_364', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_486', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_365', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_487', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:22:54.663254                               \n",
            "100%|| 122/122 [05:03<00:00, 303.46s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 122 trials to 123 (+1) trials\n",
            "2023-07-31 05:22:54.746975                               \n",
            "{'name': 'Adam', 'learning_rate': 0.40925238898628585, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_122', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_244_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_244', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_244', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_244', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_244', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_245', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_245', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_245', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_245', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_122', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_488', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_366', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_489', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_367', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_490', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_368', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_491', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:28:09.206512                               \n",
            "100%|| 123/123 [05:14<00:00, 314.52s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 123 trials to 124 (+1) trials\n",
            "2023-07-31 05:28:09.270734                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1142522359237881, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_123', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_246_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_246', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_246', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_246', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_246', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_247', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_247', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_247', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_247', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_123', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_492', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_369', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_493', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_370', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_494', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_371', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_495', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:33:00.649779                               \n",
            "100%|| 124/124 [04:51<00:00, 291.46s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 124 trials to 125 (+1) trials\n",
            "2023-07-31 05:33:00.832396                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4367761070540499, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_124', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_248_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_248', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_248', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_248', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_248', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_249', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_249', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_249', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_249', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_124', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_496', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_372', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_497', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_373', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_498', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_374', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_499', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:38:11.011235                               \n",
            "100%|| 125/125 [05:10<00:00, 310.26s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 125 trials to 126 (+1) trials\n",
            "2023-07-31 05:38:11.191002                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5824987694628025, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_125', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_250_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_250', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_250', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_250', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_250', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_251', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_251', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_251', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_251', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_125', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_500', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_375', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_501', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_376', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_502', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_377', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_503', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:43:17.851482                               \n",
            "100%|| 126/126 [05:06<00:00, 306.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 126 trials to 127 (+1) trials\n",
            "2023-07-31 05:43:17.931188                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7234946987313463, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_126', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_252_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_252', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_252', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_252', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_252', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_253', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_253', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_253', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_253', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_126', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_504', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_378', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_505', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_379', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_506', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_380', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_507', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:48:11.614391                               \n",
            "100%|| 127/127 [04:53<00:00, 293.75s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 127 trials to 128 (+1) trials\n",
            "2023-07-31 05:48:11.694065                               \n",
            "{'name': 'Adam', 'learning_rate': 0.31867133078351506, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_127', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_254_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_254', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_254', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_254', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_254', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_255', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_255', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_255', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_255', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_127', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_508', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_381', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_509', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_382', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_510', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_383', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_511', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:53:16.697999                               \n",
            "100%|| 128/128 [05:05<00:00, 305.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 128 trials to 129 (+1) trials\n",
            "2023-07-31 05:53:16.869209                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3317816372728631, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_128', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_256_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_256', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_256', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_256', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_256', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_257', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_257', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_257', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_257', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_128', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_512', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_384', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_513', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_385', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_514', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_386', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_515', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:00:50.822551                               \n",
            "100%|| 129/129 [07:34<00:00, 454.02s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 129 trials to 130 (+1) trials\n",
            "2023-07-31 06:00:50.981265                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5168269938007773, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_129', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_258_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_258', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_258', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_258', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_258', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_259', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_259', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_259', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_259', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_129', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_516', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_387', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_517', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_388', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_518', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_389', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_519', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:08:33.655715                               \n",
            "100%|| 130/130 [07:42<00:00, 462.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 130 trials to 131 (+1) trials\n",
            "2023-07-31 06:08:33.831874                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5225799648099021, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_130', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_260_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_260', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_260', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_260', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_260', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_261', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_261', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_261', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_261', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_130', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_520', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_390', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_521', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_391', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_522', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_392', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_523', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:13:54.928375                               \n",
            "100%|| 131/131 [05:21<00:00, 321.16s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 131 trials to 132 (+1) trials\n",
            "2023-07-31 06:13:54.998657                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8986164409876551, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_131', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_262_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_262', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_262', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_262', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_262', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_263', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_263', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_263', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_263', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_131', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_524', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_393', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_525', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_394', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_526', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_395', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_527', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:18:55.361645                               \n",
            "100%|| 132/132 [05:00<00:00, 300.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 132 trials to 133 (+1) trials\n",
            "2023-07-31 06:18:55.533716                               \n",
            "{'name': 'Adam', 'learning_rate': 0.625991535565348, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_132', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_264_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_264', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_264', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_264', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_264', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_265', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_265', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_265', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_265', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_132', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_528', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_396', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_529', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_397', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_530', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_398', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_531', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:24:09.526675                               \n",
            "100%|| 133/133 [05:14<00:00, 314.06s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 133 trials to 134 (+1) trials\n",
            "2023-07-31 06:24:09.599461                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4920692726310007, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_133', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_266_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_266', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_266', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_266', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_266', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_267', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_267', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_267', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_267', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_133', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_532', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_399', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_533', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_400', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_534', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_401', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_535', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:31:31.088799                               \n",
            "100%|| 134/134 [07:21<00:00, 441.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 134 trials to 135 (+1) trials\n",
            "2023-07-31 06:31:31.157188                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1686555010616247, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_134', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_268_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_268', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_268', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_268', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_268', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_269', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_269', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_269', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_269', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_134', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_536', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_402', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_537', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_403', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_538', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_404', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_539', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:36:40.571238                               \n",
            "100%|| 135/135 [05:09<00:00, 309.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 135 trials to 136 (+1) trials\n",
            "2023-07-31 06:36:40.650583                               \n",
            "{'name': 'Adam', 'learning_rate': 0.22095108060854524, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_135', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_270_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_270', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_270', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_270', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_270', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_271', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_271', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_271', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_271', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_135', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_540', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_405', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_541', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_406', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_542', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_407', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_543', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 215/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:41:44.289351                               \n",
            "100%|| 136/136 [05:03<00:00, 303.72s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 136 trials to 137 (+1) trials\n",
            "2023-07-31 06:41:44.461515                               \n",
            "{'name': 'Adam', 'learning_rate': 0.06625963243435189, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_136', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_272_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_272', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_272', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_272', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_272', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_273', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_273', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_273', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_273', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_136', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_544', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_408', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_545', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_409', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_546', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_410', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_547', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:49:11.103781                               \n",
            "100%|| 137/137 [07:26<00:00, 446.71s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 137 trials to 138 (+1) trials\n",
            "2023-07-31 06:49:11.173650                               \n",
            "{'name': 'Adam', 'learning_rate': 0.08376672400009921, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_137', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_274_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_274', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_274', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_274', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_274', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_275', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_275', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_275', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_275', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_137', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_548', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_411', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_549', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_412', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_550', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_413', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_551', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:57:03.318452                               \n",
            "100%|| 138/138 [07:52<00:00, 472.21s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 138 trials to 139 (+1) trials\n",
            "2023-07-31 06:57:03.391437                               \n",
            "{'name': 'Adam', 'learning_rate': 0.24863451029421096, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_138', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_276_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_276', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_276', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_276', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_276', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_277', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_277', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_277', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_277', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_138', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_552', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_414', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_553', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_415', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_554', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_416', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_555', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:00:54.012672                               \n",
            "100%|| 139/139 [03:50<00:00, 230.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 139 trials to 140 (+1) trials\n",
            "2023-07-31 07:00:54.077371                               \n",
            "{'name': 'Adam', 'learning_rate': 0.306481092344722, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_139', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_278_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_278', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_278', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_278', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_278', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_279', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_279', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_279', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_279', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_139', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_556', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_417', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_557', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_418', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_558', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_419', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_559', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 313/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:08:53.063261                               \n",
            "100%|| 140/140 [07:59<00:00, 479.06s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 140 trials to 141 (+1) trials\n",
            "2023-07-31 07:08:53.232145                               \n",
            "{'name': 'Adam', 'learning_rate': 0.27011196068049326, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_140', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_280_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_280', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_280', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_280', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_280', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_281', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_281', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_281', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_281', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_140', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_560', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_420', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_561', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_421', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_562', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_422', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_563', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:13:57.539405                               \n",
            "100%|| 141/141 [05:04<00:00, 304.37s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 141 trials to 142 (+1) trials\n",
            "2023-07-31 07:13:57.610385                               \n",
            "{'name': 'Adam', 'learning_rate': 0.40135687185279567, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_141', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_282_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_282', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_282', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_282', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_282', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_283', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_283', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_283', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_283', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_141', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_564', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_423', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_565', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_424', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_566', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_425', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_567', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 141/142 [07:42<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0015s). Check your callbacks.\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:21:43.708694                               \n",
            "100%|| 142/142 [07:46<00:00, 466.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 142 trials to 143 (+1) trials\n",
            "2023-07-31 07:21:43.868857                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5801427165648934, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_142', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_284_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_284', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_284', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_284', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_284', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_285', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_285', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_285', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_285', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_142', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_568', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_426', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_569', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_427', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_570', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_428', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_571', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 21s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:29:09.293730                               \n",
            "100%|| 143/143 [07:25<00:00, 445.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 143 trials to 144 (+1) trials\n",
            "2023-07-31 07:29:09.618390                               \n",
            "{'name': 'Adam', 'learning_rate': 0.17732123564908345, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_143', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_286_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_286', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_286', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_286', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_286', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_287', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_287', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_287', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_287', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_143', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_572', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_429', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_573', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_430', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_574', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_431', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_575', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:37:02.040191                               \n",
            "100%|| 144/144 [07:52<00:00, 472.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 144 trials to 145 (+1) trials\n",
            "2023-07-31 07:37:02.193546                               \n",
            "{'name': 'Adam', 'learning_rate': 0.29239509830475546, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_144', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_288_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_288', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_288', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_288', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_288', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_289', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_289', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_289', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_289', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_144', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_576', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_432', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_577', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_433', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_578', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_434', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_579', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:45:01.392554                               \n",
            "100%|| 145/145 [07:59<00:00, 479.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 145 trials to 146 (+1) trials\n",
            "2023-07-31 07:45:01.571003                               \n",
            "{'name': 'Adam', 'learning_rate': 0.09948120220181846, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_145', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_290_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_290', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_290', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_290', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_290', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_291', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_291', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_291', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_291', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_145', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_580', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_435', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_581', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_436', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_582', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_437', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_583', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:50:09.169793                               \n",
            "100%|| 146/146 [05:07<00:00, 307.67s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 146 trials to 147 (+1) trials\n",
            "2023-07-31 07:50:09.343621                               \n",
            "{'name': 'Adam', 'learning_rate': 0.846820160647481, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_146', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_292_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_292', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_292', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_292', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_292', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_293', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_293', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_293', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_293', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_146', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_584', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_438', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_585', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_439', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_586', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_440', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_587', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:55:18.100978                               \n",
            "100%|| 147/147 [05:08<00:00, 308.85s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 147 trials to 148 (+1) trials\n",
            "2023-07-31 07:55:18.287294                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3745277053816565, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_147', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_294_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_294', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_294', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_294', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_294', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_295', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_295', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_295', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_295', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_147', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_588', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_441', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_589', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_442', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_590', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_443', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_591', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:00:48.883026                               \n",
            "100%|| 148/148 [05:30<00:00, 330.67s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 148 trials to 149 (+1) trials\n",
            "2023-07-31 08:00:48.965826                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3006440316370092, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_148', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_296_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_296', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_296', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_296', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_296', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_297', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_297', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_297', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_297', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_148', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_592', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_444', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_593', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_445', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_594', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_446', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_595', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:08:24.652775                               \n",
            "100%|| 149/149 [07:35<00:00, 455.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 149 trials to 150 (+1) trials\n",
            "2023-07-31 08:08:24.841206                               \n",
            "{'name': 'Adam', 'learning_rate': 0.23348164532472845, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_149', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_298_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_298', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_298', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_298', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_298', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_299', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_299', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_299', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_299', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_149', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_596', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_447', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_597', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_448', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_598', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_449', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_599', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:13:29.259201                               \n",
            "100%|| 150/150 [05:04<00:00, 304.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 150 trials to 151 (+1) trials\n",
            "2023-07-31 08:13:29.337324                               \n",
            "{'name': 'Adam', 'learning_rate': 0.31352032970254423, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_150', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_300_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_300', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_300', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_300', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_300', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_301', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_301', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_301', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_301', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_150', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_600', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_450', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_601', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_451', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_602', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_452', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_603', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:18:32.305290                               \n",
            "100%|| 151/151 [05:03<00:00, 303.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 151 trials to 152 (+1) trials\n",
            "2023-07-31 08:18:32.452505                               \n",
            "{'name': 'Adam', 'learning_rate': 0.07687536399162423, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_151', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_302_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_302', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_302', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_302', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_302', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_303', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_303', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_303', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_303', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_151', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_604', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_453', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_605', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_454', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_606', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_455', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_607', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:25:56.654437                               \n",
            "100%|| 152/152 [07:24<00:00, 444.28s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 152 trials to 153 (+1) trials\n",
            "2023-07-31 08:25:56.737914                               \n",
            "{'name': 'Adam', 'learning_rate': 0.2847777760406176, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_152', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_304_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_304', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_304', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_304', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_304', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_305', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_305', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_305', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_305', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_152', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_608', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_456', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_609', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_457', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_610', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_458', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_611', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:31:03.982243                               \n",
            "100%|| 153/153 [05:07<00:00, 307.33s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 153 trials to 154 (+1) trials\n",
            "2023-07-31 08:31:04.170813                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8036399433912818, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_153', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_306_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_306', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_306', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_306', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_306', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_307', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_307', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_307', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_307', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_153', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_612', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_459', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_613', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_460', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_614', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_461', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_615', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 509/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:36:15.657217                               \n",
            "100%|| 154/154 [05:11<00:00, 311.57s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 154 trials to 155 (+1) trials\n",
            "2023-07-31 08:36:15.762106                               \n",
            "{'name': 'Adam', 'learning_rate': 0.19581596078991026, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_154', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_308_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_308', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_308', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_308', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_308', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_309', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_309', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_309', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_309', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_154', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_616', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_462', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_617', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_463', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_618', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_464', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_619', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 25s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:44:00.048654                               \n",
            "100%|| 155/155 [07:44<00:00, 464.41s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 155 trials to 156 (+1) trials\n",
            "2023-07-31 08:44:00.181317                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1935959205551956, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_155', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_310_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_310', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_310', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_310', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_310', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_311', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_311', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_311', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_311', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_155', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_620', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_465', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_621', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_466', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_622', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_467', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_623', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:51:55.188215                               \n",
            "100%|| 156/156 [07:55<00:00, 475.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 156 trials to 157 (+1) trials\n",
            "2023-07-31 08:51:55.358587                               \n",
            "{'name': 'Adam', 'learning_rate': 0.6443787098651779, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_156', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_312_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_312', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_312', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_312', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_312', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_313', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_313', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_313', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_313', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_156', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_624', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_468', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_625', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_469', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_626', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_470', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_627', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:57:09.702067                               \n",
            "100%|| 157/157 [05:14<00:00, 314.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 157 trials to 158 (+1) trials\n",
            "2023-07-31 08:57:09.879277                               \n",
            "{'name': 'Adam', 'learning_rate': 0.24697331057966157, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_157', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_314_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_314', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_314', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_314', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_314', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_315', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_315', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_315', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_315', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_157', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_628', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_471', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_629', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_472', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_630', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_473', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_631', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:01:13.625989                               \n",
            "100%|| 158/158 [04:03<00:00, 243.82s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 158 trials to 159 (+1) trials\n",
            "2023-07-31 09:01:13.727729                               \n",
            "{'name': 'Adam', 'learning_rate': 0.20520517075346406, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_158', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_316_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_316', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_316', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_316', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_316', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_317', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_317', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_317', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_317', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_158', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_632', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_474', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_633', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_475', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_634', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_476', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_635', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:06:18.209861                               \n",
            "100%|| 159/159 [05:04<00:00, 304.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 159 trials to 160 (+1) trials\n",
            "2023-07-31 09:06:18.377850                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4584487512512802, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_159', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_318_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_318', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_318', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_318', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_318', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_319', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_319', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_319', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_319', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_159', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_636', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_477', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_637', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_478', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_638', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_479', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_639', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:11:33.118188                               \n",
            "100%|| 160/160 [05:14<00:00, 314.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 160 trials to 161 (+1) trials\n",
            "2023-07-31 09:11:33.278567                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8084590558529352, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_160', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_320_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_320', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_320', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_320', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_320', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_321', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_321', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_321', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_321', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_160', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_640', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_480', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_641', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_481', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_642', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_482', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_643', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:19:01.444823                               \n",
            "100%|| 161/161 [07:28<00:00, 448.23s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 161 trials to 162 (+1) trials\n",
            "2023-07-31 09:19:01.609806                               \n",
            "{'name': 'Adam', 'learning_rate': 0.25693698799879666, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_161', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_322_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_322', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_322', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_322', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_322', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_323', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_323', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_323', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_323', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_161', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_644', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_483', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_645', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_484', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_646', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_485', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_647', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:24:11.572070                               \n",
            "100%|| 162/162 [05:10<00:00, 310.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 162 trials to 163 (+1) trials\n",
            "2023-07-31 09:24:11.644223                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3817600873667501, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_162', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_324_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_324', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_324', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_324', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_324', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_325', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_325', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_325', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_325', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_162', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_648', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_486', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_649', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_487', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_650', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_488', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_651', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:29:34.385753                               \n",
            "100%|| 163/163 [05:22<00:00, 322.81s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 163 trials to 164 (+1) trials\n",
            "2023-07-31 09:29:34.547427                               \n",
            "{'name': 'Adam', 'learning_rate': 0.12284198520300191, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_163', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_326_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_326', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_326', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_326', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_326', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_327', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_327', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_327', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_327', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_163', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_652', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_489', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_653', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_490', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_654', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_491', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_655', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 980/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:37:32.288263                               \n",
            "100%|| 164/164 [07:57<00:00, 477.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 164 trials to 165 (+1) trials\n",
            "2023-07-31 09:37:32.381538                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5778208462619084, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_164', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_328_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_328', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_328', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_328', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_328', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_329', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_329', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_329', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_329', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_164', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_656', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_492', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_657', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_493', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_658', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_494', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_659', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:42:41.956966                               \n",
            "100%|| 165/165 [05:09<00:00, 309.64s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 165 trials to 166 (+1) trials\n",
            "2023-07-31 09:42:42.041419                               \n",
            "{'name': 'Adam', 'learning_rate': 0.0004972729685774701, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_165', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_330_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_330', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_330', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_330', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_330', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_331', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_331', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_331', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_331', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_165', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_660', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_495', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_661', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_496', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_662', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_497', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_663', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:47:48.713186                               \n",
            "100%|| 166/166 [05:06<00:00, 306.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 166 trials to 167 (+1) trials\n",
            "2023-07-31 09:47:48.812408                               \n",
            "{'name': 'Adam', 'learning_rate': 0.28821341475605244, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_166', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_332_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_332', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_332', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_332', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_332', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_333', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_333', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_333', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_333', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_166', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_664', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_498', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_665', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_499', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_666', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_500', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_667', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:52:54.329677                               \n",
            "100%|| 167/167 [05:05<00:00, 305.59s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 167 trials to 168 (+1) trials\n",
            "2023-07-31 09:52:54.515984                               \n",
            "{'name': 'Adam', 'learning_rate': 0.25296596518615905, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_167', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_334_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_334', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_334', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_334', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_334', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_335', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_335', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_335', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_335', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_167', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_668', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_501', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_669', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_502', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_670', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_503', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_671', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 638/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:57:59.376075                               \n",
            "100%|| 168/168 [05:04<00:00, 304.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 168 trials to 169 (+1) trials\n",
            "2023-07-31 09:57:59.457604                               \n",
            "{'name': 'Adam', 'learning_rate': 0.29718313897509513, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_168', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_336_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_336', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_336', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_336', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_336', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_337', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_337', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_337', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_337', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_168', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_672', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_504', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_673', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_505', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_674', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_506', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_675', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:03:12.904703                               \n",
            "100%|| 169/169 [05:13<00:00, 313.52s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 169 trials to 170 (+1) trials\n",
            "2023-07-31 10:03:12.986742                               \n",
            "{'name': 'Adam', 'learning_rate': 0.22785515075978435, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_169', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_338_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_338', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_338', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_338', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_338', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_339', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_339', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_339', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_339', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_169', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_676', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_507', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_677', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_508', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_678', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_509', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_679', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 385/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:10:37.799967                               \n",
            "100%|| 170/170 [07:24<00:00, 444.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 170 trials to 171 (+1) trials\n",
            "2023-07-31 10:10:37.989923                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7875690868006465, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_170', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_340_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_340', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_340', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_340', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_340', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_341', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_341', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_341', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_341', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_170', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_680', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_510', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_681', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_511', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_682', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_512', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_683', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  63/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1075/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:15:52.024902                               \n",
            "100%|| 171/171 [05:14<00:00, 314.11s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 171 trials to 172 (+1) trials\n",
            "2023-07-31 10:15:52.103282                               \n",
            "{'name': 'Adam', 'learning_rate': 0.21092334483875513, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_171', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_342_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_342', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_342', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_342', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_342', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_343', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_343', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_343', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_343', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_171', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_684', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_513', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_685', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_514', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_686', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_515', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_687', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:23:33.695957                               \n",
            "100%|| 172/172 [07:41<00:00, 461.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 172 trials to 173 (+1) trials\n",
            "2023-07-31 10:23:33.793515                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3662888333345001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_172', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_344_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_344', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_344', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_344', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_344', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_345', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_345', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_345', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_345', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_172', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_688', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_516', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_689', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_517', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_690', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_518', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_691', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:31:01.691173                               \n",
            "100%|| 173/173 [07:27<00:00, 447.98s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 173 trials to 174 (+1) trials\n",
            "2023-07-31 10:31:01.781280                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1352301354989742, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_173', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_346_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_346', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_346', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_346', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_346', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_347', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_347', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_347', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_347', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_173', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_692', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_519', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_693', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_520', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_694', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_521', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_695', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 6s 5ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:38:59.586526                               \n",
            "100%|| 174/174 [07:58<00:00, 478.18s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 174 trials to 175 (+1) trials\n",
            "2023-07-31 10:38:59.997254                               \n",
            "{'name': 'Adam', 'learning_rate': 0.18790750631772393, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_174', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_348_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_348', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_348', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_348', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_348', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_349', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_349', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_349', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_349', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_174', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_696', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_522', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_697', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_523', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_698', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_524', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_699', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            " 99%|| 174/175 [01:30<?, ?trial/s, best loss=?]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# loop indefinitely and stop whenever you like\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     run_trials()\n\u001b[1;32m     16\u001b[0m \u001b[39m# best_param = fmin(objective_function,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#                   param_hyperopt,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#                   algo=tpe.suggest,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# loss = [x['result']['loss'] for x in trials.trials]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# best_param_values = [x for x in best_param.values()]\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[20], line 52\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# create a new trials object and start searching\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> 52\u001b[0m best_param \u001b[39m=\u001b[39m fmin(objective_function,\n\u001b[1;32m     53\u001b[0m               param_hyperopt,\n\u001b[1;32m     54\u001b[0m               algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[1;32m     55\u001b[0m               max_evals\u001b[39m=\u001b[39;49mmax_trials,\n\u001b[1;32m     56\u001b[0m               trials\u001b[39m=\u001b[39;49mtrials)\n\u001b[1;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest:\u001b[39m\u001b[39m\"\u001b[39m, best_param)\n\u001b[1;32m     60\u001b[0m \u001b[39m# save the trials object\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
            "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective_function\u001b[39m(params):\n\u001b[1;32m     23\u001b[0m   adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m---> 25\u001b[0m   _, _, result \u001b[39m=\u001b[39m run_nn(num_feature\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconvolution\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     26\u001b[0m           loss\u001b[39m=\u001b[39;49mwbce_custom(loss_weight), optimizer\u001b[39m=\u001b[39;49mAdam(learning_rate\u001b[39m=\u001b[39;49madam_learning_rate), dropout\u001b[39m=\u001b[39;49mdropout, patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m     27\u001b[0m           existing_model \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m'\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, train_pairs \u001b[39m=\u001b[39;49m time_site_pairs_train, test_pairs \u001b[39m=\u001b[39;49m time_site_pairs_valid)\n\u001b[1;32m     29\u001b[0m   f1_score \u001b[39m=\u001b[39m result[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m   \u001b[39mif\u001b[39;00m f1_score \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mnan:\n",
            "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mrun_nn\u001b[0;34m(num_feature, model_type, batch_size, epochs, loss, optimizer, existing_model, metrics, dropout, patience, verbose, train_pairs, test_pairs)\u001b[0m\n\u001b[1;32m     26\u001b[0m xy_data \u001b[39m=\u001b[39m get_train_test_val_nn(input_data_,\n\u001b[1;32m     27\u001b[0m                       train_pairs,\n\u001b[1;32m     28\u001b[0m                       test_pairs)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Get history and result\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m model_, history, result \u001b[39m=\u001b[39m fit_nn(xy_data, model_type, existing_model\u001b[39m=\u001b[39;49mexisting_model,\n\u001b[1;32m     32\u001b[0m                                  batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m     33\u001b[0m                                  optimizer\u001b[39m=\u001b[39;49moptimizer, dropout\u001b[39m=\u001b[39;49mdropout, patience\u001b[39m=\u001b[39;49mpatience, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Plot\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m (existing_model \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m&\u001b[39m (verbose \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m&\u001b[39m (verbose \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n",
            "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mfit_nn\u001b[0;34m(xy_data, model_type, existing_model, metrics, loss, optimizer, batch_size, epochs, dropout, patience, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m es \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_f1\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39mpatience)\n\u001b[1;32m     92\u001b[0m \u001b[39m# callback for model checkpoint\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/CapstoneProject/Models/Checkpoints',\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m#                                                                save_weights_only=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[39m# Fit Model\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m[GarbageCollectorCallback(), es], verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    102\u001b[0m \u001b[39m# Evaluate Model\u001b[39;00m\n\u001b[1;32m    103\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test, callbacks\u001b[39m=\u001b[39m[GarbageCollectorCallback()])\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "param_hyperopt = {\n",
        "    'loss_weight': hp.uniform('loss_weight', 1, 50),\n",
        "    'adam_learning_rate': hp.loguniform('learning_rate', np.log(0.000001), np.log(1)),\n",
        "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
        "    'patience': hp.uniform('patience', 3, 50),\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64\n",
        "}\n",
        "\n",
        "# trials=Trials()\n",
        "\n",
        "# loop indefinitely and stop whenever you like\n",
        "while True:\n",
        "    run_trials()\n",
        "\n",
        "# best_param = fmin(objective_function,\n",
        "#                   param_hyperopt,\n",
        "#                   algo=tpe.suggest,\n",
        "#                   max_evals=5,\n",
        "#                   trials=trials)\n",
        "\n",
        "# loss = [x['result']['loss'] for x in trials.trials]\n",
        "# best_param_values = [x for x in best_param.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-31 10:48:08.620015\n",
            "{'name': 'Adam', 'learning_rate': 0.49621807449983946, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_175', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_350_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_350', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_350', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_350', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_350', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_351', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_351', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_351', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_351', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_175', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_700', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_525', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_701', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_526', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_702', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_527', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_703', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9724 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "2023-07-31 10:52:45.827179\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[nan, 0.9724233746528625, 0.5, 0.0, 0.0, nan]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
        "\n",
        "adam_learning_rate, dropout, loss_weight, patience = best_params.values()\n",
        "epochs, batch_size = 500, 64\n",
        "\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "        loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "        existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_test)\n",
        "\n",
        "\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 2s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# precision-recall curve\n",
        "\n",
        "# Getting X_test and y_test\n",
        "xy_data = get_train_test_val_nn(sites_data,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[nan],\n",
              "       [nan],\n",
              "       [nan],\n",
              "       ...,\n",
              "       [nan],\n",
              "       [nan],\n",
              "       [nan]], dtype=float32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mhYydRcPVt3",
        "outputId": "cc62a475-4c3b-436d-8f1a-90b5314b1ab5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_param' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_param\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_param' is not defined"
          ]
        }
      ],
      "source": [
        "best_param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters to trial\n",
        "adam_learning_rates = [0.00005]\n",
        "loss_weights = [3.5]\n",
        "dropouts = [0.075]\n",
        "patiences = [30]\n",
        "\n",
        "# Fixed Hyperparameters\n",
        "epochs = 500\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 1/1\n",
            "2023-08-20 18:42:55.623847\n"
          ]
        }
      ],
      "source": [
        "# Loop\n",
        "histories = []\n",
        "results = []\n",
        "model_list = []\n",
        "f1_scores = []\n",
        "run_times = []\n",
        "print(datetime.now())\n",
        "i = 0\n",
        "\n",
        "for adam_learning_rate, loss_weight, dropout, patience in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences):\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # fit model\n",
        "    model_, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "          loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "          existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)\n",
        "\n",
        "    # f1 score\n",
        "    f1_score = result[-1]\n",
        "\n",
        "    if f1_score == np.nan:\n",
        "        f1_score = 0\n",
        "\n",
        "\n",
        "    # run times\n",
        "    end_time = datetime.now()\n",
        "    run_time = end_time - start_time\n",
        "\n",
        "    # save model, history, result\n",
        "    model_list.append(model_)\n",
        "    histories.append(history)\n",
        "    results.append(result)\n",
        "    f1_scores.append(f1_score)\n",
        "    run_times.append(run_time)\n",
        "    \n",
        "    i += 1\n",
        "    clear_output(wait=True)\n",
        "    print(f'Progress: {i}/{len(adam_learning_rates)*len(loss_weights)*len(dropouts)*len(patiences)}')\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07549760490655899\n",
            "0\n",
            "(5e-05, 3.5, 0.075, 30)\n"
          ]
        }
      ],
      "source": [
        "# Best Loss \n",
        "f1_scores = [result[1] for result in results]\n",
        "print(max(f1_scores))\n",
        "\n",
        "# Best Param\n",
        "best_index = f1_scores.index(max(f1_scores))\n",
        "print(best_index)\n",
        "print([i for i in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences)][best_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save hyperparamter tuning results\n",
        "\n",
        "df1 = pd.DataFrame([i for i in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences)], columns=['adam_learning_rate', 'loss_weight', 'dropout', 'patience'])\n",
        "df2 = pd.DataFrame(results, columns=['loss', 'f1', 'Precision','Recall', 'PR-AUC','AUC','Accuracy'])\n",
        "results_df_nn = pd.concat([df1, df2], axis=1)\n",
        "results_df_nn.to_csv('~data/nn_hyperparameter_zero_4.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adam_learning_rate</th>\n",
              "      <th>loss_weight</th>\n",
              "      <th>dropout</th>\n",
              "      <th>patience</th>\n",
              "      <th>loss</th>\n",
              "      <th>f1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>PR-AUC</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.500</td>\n",
              "      <td>30</td>\n",
              "      <td>0.714413</td>\n",
              "      <td>0.075666</td>\n",
              "      <td>0.042491</td>\n",
              "      <td>0.687161</td>\n",
              "      <td>0.040975</td>\n",
              "      <td>0.621024</td>\n",
              "      <td>0.533109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>30</td>\n",
              "      <td>0.711131</td>\n",
              "      <td>0.076002</td>\n",
              "      <td>0.042542</td>\n",
              "      <td>0.684448</td>\n",
              "      <td>0.043491</td>\n",
              "      <td>0.629501</td>\n",
              "      <td>0.535407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.500</td>\n",
              "      <td>30</td>\n",
              "      <td>0.780218</td>\n",
              "      <td>0.075085</td>\n",
              "      <td>0.041526</td>\n",
              "      <td>0.735986</td>\n",
              "      <td>0.042422</td>\n",
              "      <td>0.623275</td>\n",
              "      <td>0.490139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00005</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.075</td>\n",
              "      <td>30</td>\n",
              "      <td>0.771419</td>\n",
              "      <td>0.074300</td>\n",
              "      <td>0.041644</td>\n",
              "      <td>0.680832</td>\n",
              "      <td>0.042170</td>\n",
              "      <td>0.619260</td>\n",
              "      <td>0.527497</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   adam_learning_rate  loss_weight  dropout  patience      loss        f1   \n",
              "0             0.00005          3.0    0.500        30  0.714413  0.075666  \\\n",
              "1             0.00005          3.0    0.075        30  0.711131  0.076002   \n",
              "2             0.00005          3.5    0.500        30  0.780218  0.075085   \n",
              "3             0.00005          3.5    0.075        30  0.771419  0.074300   \n",
              "\n",
              "   Precision    Recall    PR-AUC       AUC  Accuracy  \n",
              "0   0.042491  0.687161  0.040975  0.621024  0.533109  \n",
              "1   0.042542  0.684448  0.043491  0.629501  0.535407  \n",
              "2   0.041526  0.735986  0.042422  0.623275  0.490139  \n",
              "3   0.041644  0.680832  0.042170  0.619260  0.527497  "
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Best Model\n",
        "model_list[best_index].save(f'cnn_fneg10_best_1845.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = wbce_custom(3)\n",
        "metric = f1\n",
        "best_model_f0_ = tf.keras.models.load_model('cnn_f0_best.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})\n",
        "best_model_f0 = tf.keras.models.load_model('cnn_f0_best.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})\n",
        "best_model_fneg10 = tf.keras.models.load_model('cnn_fneg10_best_1845.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data_ = sites_data\n",
        "\n",
        "# Getting \"test\" data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 2s 1ms/step\n",
            "1170/1170 [==============================] - 2s 1ms/step\n",
            "1170/1170 [==============================] - 1s 970us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.06107</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06107</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.07663</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.668</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.627</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        f1  precision  recall  pr_auc  roc_auc  accuracy\n",
              "0  0.06107      0.032   0.865   0.032    0.563     0.266\n",
              "1  0.06107      0.032   0.865   0.032    0.563     0.266\n",
              "2  0.07663      0.041   0.668   0.040    0.627     0.556"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, average_precision_score, f1_score\n",
        "\n",
        "metrics = []\n",
        "\n",
        "for model in [best_model_f0_, best_model_f0, best_model_fneg10]:\n",
        "\n",
        "    pred_prob = model.predict(X_test)\n",
        "    pred_class = [1 if i>0.5 else 0 for i in pred_prob]\n",
        "    \n",
        "    f1_ = round(f1_score(y_test, pred_class), 5)\n",
        "    precision = round(precision_score(y_test, pred_class), 3)\n",
        "    recall = round(recall_score(y_test, pred_class), 3)\n",
        "    pr_auc = round(average_precision_score(y_test, pred_prob), 3)\n",
        "    roc_auc = round(roc_auc_score(y_test, pred_prob), 3)   \n",
        "    accuracy = round(accuracy_score(y_test, pred_class), 3)\n",
        "\n",
        "    metrics.append([f1_,precision,recall,pr_auc,roc_auc,accuracy])\n",
        "\n",
        "pd.DataFrame(metrics, columns=['f1','precision','recall','pr_auc','roc_auc','accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data_ = sites_data\n",
        "\n",
        "# Getting \"test\" data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.027960379812231664"
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.merge(time_site_pairs_train, sites_data, on=['time', 'site'])\n",
        "baseline_val = sum(train['riskLevelLabel'])/len(train)\n",
        "baseline_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc_pr_graphs(model):\n",
        "    # predictions from best model on validation data\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
        "    roc_auc = roc_auc_score(y_test, predictions)\n",
        "\n",
        "    # PR curve and AP\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
        "    average_precision = average_precision_score(y_test, predictions)\n",
        "\n",
        "    # plots\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, marker='.',label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate', fontsize=15)\n",
        "    plt.ylabel('True Positive Rate', fontsize=15)\n",
        "    plt.title('Receiver Operating Characteristic (ROC)', fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"lower right\", fontsize=15)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, marker='.', label='PR curve (AP = %0.4f)' % average_precision)\n",
        "    plt.axhline(y=baseline_val, color=\"gray\", linestyle='--', label='Baseline')\n",
        "    plt.xlabel('Recall', fontsize=15)\n",
        "    plt.ylabel('Precision', fontsize=15)\n",
        "    plt.title('Precision-Recall (PR) Curve', fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"upper right\", fontsize=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 945us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_roc_pr_graphs(best_model_f0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 966us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "confusion_matrix_plot(best_model_fneg10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 959us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_roc_pr_graphs(best_model_fneg10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Strength and Weakness of Best Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load site dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "site_with_region = pd.read_csv('~data/site.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get % positive in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020: 3.291110608459624\n",
            "2021: 3.0794248986861463\n",
            "2022: 3.291110608459624\n",
            "May: 1.5554983260365698\n",
            "Jun: 2.258435922416084\n",
            "Jul: 1.8845450291402561\n",
            "Aug: 3.3907034636679683\n",
            "Sep: 4.620255987156045\n",
            "South West: 2.455636207572167\n",
            "South East: 2.5582128541188127\n",
            "East of England: 2.1468276787425724\n",
            "North East: 2.9177139110479726\n",
            "North West: 5.660134089736977\n",
            "Yorkshire: 4.209988068859724\n",
            "East Midlands: 1.9761029411764706\n",
            "London: 3.2169117647058822\n"
          ]
        }
      ],
      "source": [
        "train = pd.merge(time_site_pairs_train, sites_data, on=['time', 'site'])\n",
        "train = train.merge(site_with_region[['EUBWID', 'region']], how='left', left_on='site', right_on='EUBWID')\n",
        "train['time'] = pd.to_datetime(train['time'])\n",
        "\n",
        "def get_percent_pos_train(df):\n",
        "    y_actual = df['riskLevelLabel']\n",
        "    return sum(y_actual) / len(df) * 100\n",
        "\n",
        "# % pos train samples by year\n",
        "print(f\"2020: {get_percent_pos_train(train[train['time'] <= '2020-12-31'])}\")\n",
        "print(f\"2021: {get_percent_pos_train(train[(train['time'] <= '2021-12-31') & (train['time'] >= '2021-01-01')])}\")\n",
        "print(f\"2022: {get_percent_pos_train(train[train['time'] <= '2020-12-31'])}\")\n",
        "\n",
        "# % pos train samples by month\n",
        "print(f\"May: {get_percent_pos_train(train[train['time'].dt.month==5])}\")\n",
        "print(f\"Jun: {get_percent_pos_train(train[train['time'].dt.month==6])}\")\n",
        "print(f\"Jul: {get_percent_pos_train(train[train['time'].dt.month==7])}\")\n",
        "print(f\"Aug: {get_percent_pos_train(train[train['time'].dt.month==8])}\")\n",
        "print(f\"Sep: {get_percent_pos_train(train[train['time'].dt.month==9])}\")\n",
        "\n",
        "# % pos train samples by region\n",
        "print(f\"South West: {get_percent_pos_train(train[train['region'] == 'South West'])}\")\n",
        "print(f\"South East: {get_percent_pos_train(train[train['region'] == 'South East'])}\")\n",
        "print(f\"East of England: {get_percent_pos_train(train[train['region'] == 'East of England'])}\")\n",
        "print(f\"North East: {get_percent_pos_train(train[train['region'] == 'North East'])}\")\n",
        "print(f\"North West: {get_percent_pos_train(train[train['region'] == 'North West'])}\")\n",
        "print(f\"Yorkshire: {get_percent_pos_train(train[train['region'] == 'Yorkshire and The Humber'])}\")\n",
        "print(f\"East Midlands: {get_percent_pos_train(train[train['region'] == 'East Midlands'])}\")\n",
        "print(f\"London: {get_percent_pos_train(train[train['region'] == 'London'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get % positive in test data + other metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, average_precision_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_metrics_from_test_df(df):\n",
        "    y_pred = df['predictions']\n",
        "    y_actual = df['riskLevelLabel']\n",
        "\n",
        "    f1_ = round(f1_score(y_actual, y_pred), 3)\n",
        "    precision = round(precision_score(y_actual, y_pred), 3)\n",
        "    recall = round(recall_score(y_actual, y_pred), 3)\n",
        "    pr_auc = round(average_precision_score(y_actual, y_pred), 3)\n",
        "    roc_auc = round(roc_auc_score(y_actual, y_pred), 3)   \n",
        "    accuracy = round(accuracy_score(y_actual, y_pred), 3)\n",
        "    \n",
        "    metrics_list = [len(df), f1_, precision, recall, pr_auc, roc_auc, accuracy]\n",
        "    return metrics_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# best model from fill neg10\n",
        "\n",
        "pred_prob = best_model_fneg10.predict(X_test)\n",
        "pred_class = [1 if i>0.5 else 0 for i in pred_prob]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020: [11766, 0.087, 0.046, 0.702, 0.042, 0.621, 0.545]\n",
            "2021: [12894, 0.079, 0.042, 0.639, 0.038, 0.587, 0.538]\n",
            "2022: [12763, 0.063, 0.033, 0.663, 0.029, 0.623, 0.586]\n",
            "May: [6430, 0.053, 0.027, 0.717, 0.024, 0.66, 0.604]\n",
            "Jun: [7690, 0.059, 0.031, 0.704, 0.028, 0.619, 0.537]\n",
            "Jul: [7829, 0.055, 0.028, 0.671, 0.026, 0.604, 0.54]\n",
            "Aug: [7683, 0.095, 0.051, 0.664, 0.045, 0.615, 0.569]\n",
            "Sep: [7429, 0.113, 0.062, 0.645, 0.056, 0.592, 0.544]\n",
            "South West: [17926, 0.071, 0.037, 0.65, 0.033, 0.613, 0.578]\n",
            "South East: [7603, 0.08, 0.042, 0.665, 0.037, 0.632, 0.602]\n",
            "East of England: [3510, 0.044, 0.023, 0.569, 0.021, 0.558, 0.547]\n",
            "North East: [2953, 0.101, 0.054, 0.815, 0.049, 0.676, 0.546]\n",
            "North West: [2557, 0.12, 0.065, 0.748, 0.062, 0.585, 0.438]\n",
            "Yorkshire and The Humber: [1832, 0.074, 0.04, 0.5, 0.039, 0.512, 0.523]\n",
            "East Midlands: [696, 0.047, 0.025, 0.55, 0.026, 0.453, 0.362]\n",
            "London: [346, 0.078, 0.04, 1.0, 0.04, 0.5, 0.04]\n"
          ]
        }
      ],
      "source": [
        "# Merge predictions onto test dataset\n",
        "test = pd.merge(time_site_pairs_test, sites_data, on=['time', 'site'])\n",
        "test['time'] = pd.to_datetime(test['time'])\n",
        "test['predictions'] = pred_class\n",
        "\n",
        "# grouping predictions by year\n",
        "print(f\"2020: {get_metrics_from_test_df(test[test['time'] <= '2020-12-31'])}\")\n",
        "print(f\"2021: {get_metrics_from_test_df(test[(test['time'] <= '2021-12-31') & (test['time'] >= '2021-01-01')])}\")\n",
        "print(f\"2022: {get_metrics_from_test_df(test[test['time'] >= '2022-01-01'])}\")\n",
        "\n",
        "# grouping predictions by month\n",
        "print(f\"May: {get_metrics_from_test_df(test[test['time'].dt.month==5])}\")\n",
        "print(f\"Jun: {get_metrics_from_test_df(test[test['time'].dt.month==6])}\")\n",
        "print(f\"Jul: {get_metrics_from_test_df(test[test['time'].dt.month==7])}\")\n",
        "print(f\"Aug: {get_metrics_from_test_df(test[test['time'].dt.month==8])}\")\n",
        "print(f\"Sep: {get_metrics_from_test_df(test[test['time'].dt.month==9])}\")\n",
        "\n",
        "# grouping predictions by regions\n",
        "test = test.merge(site_with_region[['EUBWID', 'region']], how='left', left_on='site', right_on='EUBWID')\n",
        "print(f\"South West: {get_metrics_from_test_df(test[test['region'] == 'South West'])}\")\n",
        "print(f\"South East: {get_metrics_from_test_df(test[test['region'] == 'South East'])}\")\n",
        "print(f\"East of England: {get_metrics_from_test_df(test[test['region'] == 'East of England'])}\")\n",
        "print(f\"North East: {get_metrics_from_test_df(test[test['region'] == 'North East'])}\")\n",
        "print(f\"North West: {get_metrics_from_test_df(test[test['region'] == 'North West'])}\")\n",
        "print(f\"Yorkshire and The Humber: {get_metrics_from_test_df(test[test['region'] == 'Yorkshire and The Humber'])}\")\n",
        "print(f\"East Midlands: {get_metrics_from_test_df(test[test['region'] == 'East Midlands'])}\")\n",
        "print(f\"London: {get_metrics_from_test_df(test[test['region'] == 'London'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S2 and S3 Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_site_pairs_train_s2s3 = pd.read_csv(\"~data/time_site_pairs_train_s2s3.csv\")\n",
        "time_site_pairs_test_s2s3 = pd.read_csv(\"~data/time_site_pairs_test_s2s3.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'nn_51x51.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m\"\u001b[39;49m\u001b[39mnn_51x51.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    180\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    181\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    182\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    183\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    184\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    185\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    186\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nn_51x51.pkl'"
          ]
        }
      ],
      "source": [
        "sites_data = pd.read_pickle(\"~data/nn_51x51.pkl\")\n",
        "\n",
        "model_, history, result = run_nn(num_feature=3, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "            loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "            existing_model = None, verbose=0, train_pairs = time_site_pairs_train_s2s3, test_pairs = time_site_pairs_test_s2s3,\n",
        "            dim = dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sites_data = pd.read_pickle(\"~data/nn_15x15_6_features_na_mean_zero_no_norm.pkl\")\n",
        "\n",
        "model_, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "            loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "            existing_model = None, verbose=0, train_pairs = time_site_pairs_train_s2s3, test_pairs = time_site_pairs_test_s2s3,\n",
        "            dim = dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhySSdKwM30l"
      },
      "source": [
        "# Window Size Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best params from previous trials\n",
        "adam_learning_rate = \n",
        "dropout = \n",
        "loss_weight =\n",
        "patience =\n",
        "batch_size = 64\n",
        "epochs = 500\n",
        "\n",
        "# storing results\n",
        "histories = []\n",
        "results = []\n",
        "model_list = []\n",
        "f1_scores = []\n",
        "run_times = []\n",
        "print(datetime.now())\n",
        "i = 0\n",
        "\n",
        "for dim in [1, 3, 5, 7, 9, 11, 13, 15:]\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # fit model\n",
        "    model_, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "            loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "            existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_test,\n",
        "            dim = dim)\n",
        "\n",
        "    # f1 score\n",
        "    f1_score = result[-1]\n",
        "\n",
        "    if f1_score == np.nan:\n",
        "        f1_score = 0\n",
        "\n",
        "    # run times\n",
        "    end_time = datetime.now()\n",
        "    run_time = end_time - start_time\n",
        "\n",
        "    # save model, history, result\n",
        "    model_list.append(model_)\n",
        "    histories.append(history)\n",
        "    results.append(result)\n",
        "    f1_scores.append(f1_score)\n",
        "    run_times.append(run_time)\n",
        "    \n",
        "    i += 1\n",
        "    clear_output(wait=True)\n",
        "    print(f'Progress: {i}/8')\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIVoNZRCM30l"
      },
      "outputs": [],
      "source": [
        "f1_scores = [i[5] for i in results]\n",
        "plt.plot([1,3,5,7,9,11], f1_scores)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Window Size')\n",
        "plt.title('F1 Score with different window size, for Convolutional Neural Network, 3 feature, No Oversampling')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1vIrrlI5qYYy",
        "WCq5OTnjM30d",
        "fCm5MtmEM30d",
        "Inlpz-2fyRS-",
        "CQ2WRerRdX5E",
        "7_MflnfHQXCz",
        "qbDjSSIaoPmH",
        "um8esTkeoRAe"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local-venv-kernel",
      "language": "python",
      "name": "local-venv-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
