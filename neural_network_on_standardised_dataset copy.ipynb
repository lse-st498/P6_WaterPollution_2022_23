{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrR-uN_IM30X"
      },
      "source": [
        "**Models**:\n",
        "- Logistic Regression (LR)\n",
        "- Random Forest (RF)\n",
        "- Neural Network (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vIrrlI5qYYy"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "bs7kFeNpqVMd"
      },
      "source": [
        ">[Table of Contents](#scrollTo=1vIrrlI5qYYy)\n",
        "\n",
        ">[Load Packages](#scrollTo=7kYT-Av0M30Y)\n",
        "\n",
        ">[Load Datasets and Basic Data Cleaning](#scrollTo=W8j2Yk93M30a)\n",
        "\n",
        ">>[BC & RF: features dataset](#scrollTo=ogBM3dkMM30a)\n",
        "\n",
        ">>[NN: sites_data.csv](#scrollTo=XsGpGZJoM30b)\n",
        "\n",
        ">>[All: Pollution Data](#scrollTo=iAjdKpWAM30b)\n",
        "\n",
        ">[Data Manipulation and Further Data Cleaning](#scrollTo=7jfpVEdhM30b)\n",
        "\n",
        ">>>[BC & RF: Replace NaNs with mean](#scrollTo=2tNYFCXyM30b)\n",
        "\n",
        ">>>[BC & RF: Replace NaNs with -10](#scrollTo=w2nMfJ_MM30c)\n",
        "\n",
        ">[Train-Test Split](#scrollTo=k5Rh25eDM30d)\n",
        "\n",
        ">>[5-fold CV on Training Data](#scrollTo=NVfVZtslVDgN)\n",
        "\n",
        ">[Train & Test Models](#scrollTo=LqVCHBwqM30d)\n",
        "\n",
        ">>[Baseline](#scrollTo=WCq5OTnjM30d)\n",
        "\n",
        ">>[BC](#scrollTo=fCm5MtmEM30d)\n",
        "\n",
        ">>>[df_merged_mean](#scrollTo=sk2ahJhvM30d)\n",
        "\n",
        ">>>>[ROC](#scrollTo=AUtGhFBxM30f)\n",
        "\n",
        ">>>[df_merged_neg](#scrollTo=LbH3oq34M30f)\n",
        "\n",
        ">>[RF](#scrollTo=tX_m83LwM30g)\n",
        "\n",
        ">>>[df_merged_mean](#scrollTo=WwjWWDddM30g)\n",
        "\n",
        ">>>>[Importance of Feature Aggregation](#scrollTo=HSd3THghM30h)\n",
        "\n",
        ">>>>[PCA](#scrollTo=ce8UQqnCM30h)\n",
        "\n",
        ">>>[df_merged_neg](#scrollTo=yNtOjtXHM30h)\n",
        "\n",
        ">>>>[Importance of Feature Aggregation](#scrollTo=d85180_iM30i)\n",
        "\n",
        ">>>>[PCA](#scrollTo=3iz1OD5WM30i)\n",
        "\n",
        ">>[NN](#scrollTo=HqmPxsTQM30j)\n",
        "\n",
        ">>>[Helper Functions](#scrollTo=kweB7fnOM30j)\n",
        "\n",
        ">>>[Training Models](#scrollTo=42b7DOTtM30k)\n",
        "\n",
        ">>>>[Training From Scratch](#scrollTo=orGBi9j_QSm6)\n",
        "\n",
        ">>>>>[With Cross Validation (Takes up too much RAM)](#scrollTo=CQ2WRerRdX5E)\n",
        "\n",
        ">>>>>[Save Model](#scrollTo=VY6Q610dder6)\n",
        "\n",
        ">>>>[Training From Existing Model](#scrollTo=7_MflnfHQXCz)\n",
        "\n",
        ">>>>>[Load Model From File](#scrollTo=qbDjSSIaoPmH)\n",
        "\n",
        ">>>>>[Training](#scrollTo=um8esTkeoRAe)\n",
        "\n",
        ">[Final Results](#scrollTo=KCRcg5wRM30k)\n",
        "\n",
        ">[Window Size Comparison](#scrollTo=MhySSdKwM30l)\n",
        "\n",
        ">>[BC](#scrollTo=_xgmshK1M30l)\n",
        "\n",
        ">>[RF](#scrollTo=kiYFhCJiM30l)\n",
        "\n",
        ">>[NN](#scrollTo=Y54BwAv-M30l)\n",
        "\n",
        ">[Accuracy Comparison](#scrollTo=yw0R9N0gM30m)\n",
        "\n",
        ">>[NN](#scrollTo=OZhXFnbPM30m)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kYT-Av0M30Y"
      },
      "source": [
        "# &nbsp; Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbZ20BK6Q-rf",
        "outputId": "5f06f534-06d2-4786-97e7-10e9119fa510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RRa3XwTrM30Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BswLk-JvM30a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score, make_scorer, f1_score, precision_score, recall_score\n",
        "from datetime import datetime\n",
        "from keras.optimizers import Adam\n",
        "import itertools\n",
        "from IPython.display import clear_output\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import gc\n",
        "import csv\n",
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIx8GK6iZxH7",
        "outputId": "f8367a58-314e-472e-9248-d121b464302a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtTnTSh1QHih"
      },
      "outputs": [],
      "source": [
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# tf.config.experimental.set_memory_growth(gpus[0], True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wJIjVGNr_zi"
      },
      "source": [
        "# Load Cleaned Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsGpGZJoM30b"
      },
      "source": [
        "## NN: sites_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvGzcancMq8s",
        "outputId": "db741af4-4063-4d06-b71f-0890b1bbb511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2.59 s, sys: 1.53 s, total: 4.12 s\n",
            "Wall time: 5.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# L3 data, Sentinel 3, 1km x 1km, 6 features\n",
        "# sites_data = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_15x15_6_features_na_mean_zero_no_norm.pkl\")\n",
        "sites_data = pd.read_pickle(\"nn_15x15_6_features_na_mean_zero_no_norm.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_0vJMQsQ_yS"
      },
      "outputs": [],
      "source": [
        "# duplicates = sites_data.duplicated(subset=['time','site'], keep=False)\n",
        "# sites_data[duplicates]\n",
        "# sites_data_no_duplicates = sites_data.drop_duplicates(subset=['time', 'site'])\n",
        "# sites_data_no_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyzNbhWbDBad"
      },
      "source": [
        "### Filtering Data so only run NN on part of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBvYca5ElsIR"
      },
      "outputs": [],
      "source": [
        "for feature in ['ZSD', 'CHL', 'SPM', 'KD490', 'BBP', 'CDM']:\n",
        "  sites_data[f'feature_sum_{feature}'] = [sum(sum(i)) for i in sites_data[f'{feature}']]\n",
        "\n",
        "sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP8L-Y5inAvr"
      },
      "outputs": [],
      "source": [
        "sites_data['feature_sum_boolean'] = sites_data[['feature_sum_ZSD', 'feature_sum_CHL','feature_sum_SPM','feature_sum_KD490', 'feature_sum_BBP', 'feature_sum_CDM']].ne(0).all(axis=1)\n",
        "sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuR4MZuQpCbL"
      },
      "outputs": [],
      "source": [
        "sites_data['feature_sum_boolean'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFJ88poYo631"
      },
      "outputs": [],
      "source": [
        "sites_data_ = sites_data[sites_data['feature_sum_boolean'] == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rYkkTr_pGFz"
      },
      "outputs": [],
      "source": [
        "sites_data = sites_data_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMTxPCqdqNi9"
      },
      "outputs": [],
      "source": [
        "sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ue7EAq1qSN7"
      },
      "outputs": [],
      "source": [
        "sites_data = sites_data[['time', 'site', 'SPM', 'ZSD', 'BBP', 'CDM', 'KD490', 'CHL', 'riskLevelLabel']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dow7nn33l4xK"
      },
      "outputs": [],
      "source": [
        "len(sites_data[\"feature_sum_\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD0XSI9VjMbf"
      },
      "outputs": [],
      "source": [
        "sites_data['feature_sum'] = [sum(sum(i)) for i in (sites_data['ZSD'] + sites_data['CHL'] + sites_data['SPM'] + sites_data['KD490'] + sites_data['BBP'] + sites_data['CDM'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjUeMl7bFJi_"
      },
      "outputs": [],
      "source": [
        "sites_data[\"feature_sum\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzfhC-F0EIMA"
      },
      "outputs": [],
      "source": [
        "sites_data.plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfiuW99qj2Xt"
      },
      "outputs": [],
      "source": [
        "sites_data_ = sites_data[sites_data['feature_sum'] > 0]\n",
        "sites_data_.drop('feature_sum', axis=1, inplace=True)\n",
        "sites_data_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BktWi1uHHRdA"
      },
      "outputs": [],
      "source": [
        "sites_data = sites_data_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxLjNlM4M30b"
      },
      "outputs": [],
      "source": [
        "# # L4 data, Sentinel 2, 100m x 100m, 3 features (TUR, SPM, CHL)\n",
        "\n",
        "# sites_data2 = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_51x51.pkl\")\n",
        "# sites_data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjpugJ9McggW"
      },
      "outputs": [],
      "source": [
        "# # L3 data, Sentinel 3, 300m x 300m, 51x51, 1 feature (CHL)\n",
        "\n",
        "# sites_data = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_51x51_chl.pkl\")\n",
        "# sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp6JWx_u_Iec"
      },
      "outputs": [],
      "source": [
        "# # L3 data, Sentinel 3, 1km x 1km, 15x15, 6 feature (CHL), fillna=0\n",
        "\n",
        "# sites_data = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_51x51_6_features.pkl\")\n",
        "# sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cx4k0jzxMno"
      },
      "outputs": [],
      "source": [
        "# # L3 data, Sentinel 3, 1km x 1km, 15x15, 6 feature (CHL), fillna = -10\n",
        "\n",
        "# sites_data = pd.read_pickle(\"/content/drive/My Drive/CapstoneProject/Datasets/nn_51x51_neg10_6_features.pkl\")\n",
        "# sites_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rmFXwOSM30b"
      },
      "outputs": [],
      "source": [
        "# sites_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx5TGBP_r-pr"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# def sizeof_fmt(num, suffix='B'):\n",
        "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "#         if abs(num) < 1024.0:\n",
        "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "#         num /= 1024.0\n",
        "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "#                          key= lambda x: -x[1])[:10]:\n",
        "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Y5lgCnUmMK"
      },
      "outputs": [],
      "source": [
        "# import psutil\n",
        "# split_bar = '='*20\n",
        "# memory_info = psutil.virtual_memory()._asdict()\n",
        "# print(f\"{split_bar} Memory Usage {split_bar}\")\n",
        "# for k,v in memory_info.items():\n",
        "#   print(k, v)\n",
        "# print(f\"{split_bar} CPU Usage {split_bar}\")\n",
        "# print(f\"CPU percent: {psutil.cpu_percent()}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Rh25eDM30d"
      },
      "source": [
        "# Train-Test-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H3A3QaYFoMww"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/train_labels_mean_zero.csv\")[['time', 'site']]\n",
        "# time_site_pairs_valid = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/validation_labels_mean_zero.csv\")[['time', 'site']]\n",
        "# time_site_pairs_test = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/test_labels_mean_zero.csv\")[['time', 'site']]\n",
        "\n",
        "time_site_pairs_train = pd.read_csv(\"~data/train_labels_mean_zero.csv\")[['time', 'site']]\n",
        "time_site_pairs_valid = pd.read_csv(\"~data/validation_labels_mean_zero.csv\")[['time', 'site']]\n",
        "time_site_pairs_test = pd.read_csv(\"~data/test_labels_mean_zero.csv\")[['time', 'site']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THBcRhhjrHJE"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train2 = time_site_pairs2[~time_site_pairs2.isin(time_site_pairs_test2)].dropna()\n",
        "# time_site_pairs_train2 = time_site_pairs_train2.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd6JZin8DtKR"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs = sites_data[['time', 'site']]\n",
        "\n",
        "# # shuffle df\n",
        "# time_site_pairs = time_site_pairs.sample(frac=1, random_state=42)\n",
        "\n",
        "# # rows of split\n",
        "# total_rows = time_site_pairs.shape[0]\n",
        "# train_split = int(0.6 * total_rows)\n",
        "# validation_split = int(0.2 * total_rows)\n",
        "\n",
        "# # split\n",
        "# train_df = time_site_pairs[:train_split]\n",
        "# validation_df = time_site_pairs[train_split:(train_split + validation_split)]\n",
        "# test_df = time_site_pairs[(train_split + validation_split):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "317lNtS9Eql8"
      },
      "outputs": [],
      "source": [
        "# # sanity checks\n",
        "\n",
        "# print(f\"Rows of entire dataset: {time_site_pairs.shape[0]}\")\n",
        "# print(f\"Rows of train: {train_df.shape[0]}\")\n",
        "# print(f\"Rows of validation: {validation_df.shape[0]}\")\n",
        "# print(f\"Rows of test: {test_df.shape[0]}\")\n",
        "\n",
        "# train = sites_data.merge(train_df, on=['time', 'site'], how='inner')\n",
        "# validation = sites_data.merge(validation_df, on=['time', 'site'], how='inner')\n",
        "# test = sites_data.merge(test_df, on=['time', 'site'], how='inner')\n",
        "\n",
        "# print(f\"Positive samples in train: {sum(train['riskLevelLabel'])}. % of Positive samples in train: {sum(train['riskLevelLabel'])/len(train_df)*100}%\")\n",
        "# print(f\"Positive samples in validation: {sum(validation['riskLevelLabel'])}. % of Positive samples in validation: {sum(validation['riskLevelLabel'])/len(validation_df)*100}%\")\n",
        "# print(f\"Positive samples in test: {sum(test['riskLevelLabel'])}. % of Positive samples in test: {sum(test['riskLevelLabel'])/len(test_df)*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2cmb4MEZqb"
      },
      "outputs": [],
      "source": [
        "# # export to csv\n",
        "\n",
        "# train_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/train_labels_mean_zero.csv\")\n",
        "# validation_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/validation_labels_mean_zero.csv\")\n",
        "# test_df.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/test_labels_mean_zero.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klJXVycTM30d"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs = sites_data[['time', 'site']]\n",
        "\n",
        "# # 80/20 split\n",
        "# time_site_pairs_test = time_site_pairs.sample(frac=.2, random_state=42)\n",
        "# time_site_pairs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MikeDtoyM30d"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train = time_site_pairs[~time_site_pairs.isin(time_site_pairs_test)].dropna()\n",
        "# time_site_pairs_train = time_site_pairs_train.sample(frac=1, random_state=42)\n",
        "# time_site_pairs_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdjRP_JPNN4"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_test.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_test_6_features.csv\")\n",
        "# time_site_pairs_train.to_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_train_6_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id1Ckt-5RHxW"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_test = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_test_6_features.csv\")[['time', 'site']]\n",
        "# time_site_pairs_train = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Datasets/time_site_pairs_train_6_features.csv\")[['time', 'site']]\n",
        "# #time_site_pairs_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNWJaO4_lX7e"
      },
      "outputs": [],
      "source": [
        "# time_site_pairs_train.shape[0] + time_site_pairs_test.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfVZtslVDgN"
      },
      "source": [
        "## 5-fold CV on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J-9bdnQsTB8s"
      },
      "outputs": [],
      "source": [
        "# Randomise order of training pairs\n",
        "time_site_pairs_train_new = time_site_pairs_train.sample(frac = 1)\n",
        "\n",
        "train_val_dict = {}\n",
        "\n",
        "# Get CV Train and Test time-site pairs\n",
        "for i in range(5):\n",
        "  split_index = round(len(time_site_pairs_train_new)/5)\n",
        "  train_val_dict[f'val_{i+1}'] = time_site_pairs_train_new[i*split_index: (i+1)*split_index]\n",
        "  train_val_dict[f'train_{i+1}'] = time_site_pairs_train_new.drop(time_site_pairs_train_new.index[i*split_index: (i+1)*split_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqVCHBwqM30d"
      },
      "source": [
        "# Train & Test Models\n",
        "- Train on training time-site pairs\n",
        "- Test on testing time-site pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqmPxsTQM30j"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kweB7fnOM30j"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG9pjA12XV_n"
      },
      "source": [
        "#### Reshaping Data as Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ixN0KB4UM30j"
      },
      "outputs": [],
      "source": [
        "def get_train_test_val_nn(input_data, train_labels, test_labels, dim = 15, oversampling = False, augment=True, desired_pos_ratio =  0.5, train_val_ratio = 0.8):\n",
        "    '''\n",
        "    Gets train, test and validation datasets for a neural network model.\n",
        "\n",
        "    input:\n",
        "        - input_data (pd.DataFrame):\n",
        "            - dataframe of shape (m, n)\n",
        "            - number of datapoints = m\n",
        "            - features to consider = n-1\n",
        "            - one of the columns = 'riskLevelLabel'\n",
        "\n",
        "        - train_labels / test_labels (pd.DataFrame):\n",
        "            - dataframe with two columns 'time' and 'site'\n",
        "            - time and site pairs for train/test data\n",
        "\n",
        "        - oversampling (boolean):\n",
        "            - Whether oversampling should be performed\n",
        "\n",
        "        - desired_pos_ratio (float):\n",
        "            - desired ratio of positive samples when performing random oversampling\n",
        "\n",
        "        - train_val_ratio (float):\n",
        "            - ratio of training data to validation data\n",
        "\n",
        "    output:\n",
        "        - X_train (tensor)\n",
        "        - X_test (tensor)\n",
        "        - X_val (tensor)\n",
        "        - y_train (np.array)\n",
        "        - y_test (np.array)\n",
        "        - y_val (np.array)\n",
        "    '''\n",
        "\n",
        "\n",
        "    train = pd.merge(train_labels, input_data, on=['time', 'site'])\n",
        "    test = pd.merge(test_labels, input_data, on=['time', 'site'])\n",
        "\n",
        "    ################\n",
        "    # Changing window size\n",
        "    ################\n",
        "    w = int((dim-1)/2)\n",
        "\n",
        "    def get_windowed_data(row):\n",
        "        indices = np.array(range(1,226)).reshape(15,15)[7-w:8+w, 7-w:8+w].flatten()\n",
        "        indices = [i-1 for i in indices]\n",
        "        values = row.flatten()[[indices]].reshape(dim,dim)\n",
        "        return values\n",
        "\n",
        "    if dim != 15:\n",
        "        for feature in ['ZSD','CHL','SPM','KD490','BBP','CDM']:\n",
        "            train[f'{feature}'] = train[f'{feature}'].apply(get_windowed_data)\n",
        "            test[f'{feature}'] = test[f'{feature}'].apply(get_windowed_data)\n",
        "\n",
        "    print(train['SPM'][3])\n",
        "\n",
        "    ################\n",
        "    # Getting X & y, train & val\n",
        "    ################\n",
        "\n",
        "    # Getting X & y\n",
        "    features_column_names = list(input_data.columns)\n",
        "    for x in ['riskLevelLabel', 'time', 'site']:\n",
        "        features_column_names.remove(x)\n",
        "\n",
        "    X_train, X_test = train[features_column_names], test[features_column_names]\n",
        "    y_train = train['riskLevelLabel']\n",
        "    y_test = test['riskLevelLabel']\n",
        "\n",
        "    # Train Validation Split\n",
        "    i = int(X_train.shape[0] * train_val_ratio)\n",
        "    X_val, y_val = X_train[i:], y_train[i:]\n",
        "    X_train, y_train = X_train[:i], y_train[:i]\n",
        "\n",
        "    ################\n",
        "    # Oversampling (on training data)\n",
        "    ################\n",
        "\n",
        "    if oversampling:\n",
        "        # Counting number of samples to oversample\n",
        "        num_positives, num_negatives = sum(y_train), len(y_train)-sum(y_train)\n",
        "        num_positives_to_repeat = int(desired_pos_ratio * num_negatives * 2) - num_positives\n",
        "\n",
        "        # Oversampling\n",
        "        ros = RandomOverSampler(sampling_strategy={1: num_positives_to_repeat}, random_state=42)\n",
        "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "    ################\n",
        "    # Reshaping into tensors\n",
        "    ################\n",
        "\n",
        "    X_dfs = []\n",
        "    for X_df in [X_train, X_test, X_val]:\n",
        "\n",
        "        # Reshape and Convert to Tensor\n",
        "        if X_df.shape[1] == 1:\n",
        "            X_df = np.array([i for i in X_df[features_column_names[0]]])\n",
        "            X_df = tf.convert_to_tensor(X_df)\n",
        "            X_df = tf.expand_dims(X_df, axis=3, name=None)\n",
        "        else:\n",
        "            X_df = np.stack([np.stack(X_df[col].values) for col in X_df.columns], axis=1)\n",
        "            X_df = np.transpose(X_df, (0, 2, 3, 1))\n",
        "            X_df = tf.convert_to_tensor(X_df)\n",
        "\n",
        "        # Padding\n",
        "#         X_df = tf.pad(X_df, [[0, 0], [16-w,15-w], [16-w,15-w], [0,0]])\n",
        "\n",
        "        # Append\n",
        "        X_dfs.append(X_df)\n",
        "\n",
        "    X_train, X_test, X_val = X_dfs\n",
        "\n",
        "    ################\n",
        "    # Data Augmentation on minority class\n",
        "    ################\n",
        "\n",
        "    if augment:\n",
        "      # separating positive class data points\n",
        "      positive_indices = [[i] for i in np.where(y_train == 1)[0]]\n",
        "      X_train_pos = tf.gather_nd(X_train, indices=positive_indices)\n",
        "\n",
        "      # augment\n",
        "      augmented_X = []\n",
        "\n",
        "      for j in np.arange(len(X_train_pos)):\n",
        "          original = X_train_pos[j]\n",
        "          hor_flip = tf.image.flip_left_right(original)\n",
        "          ver_flip = tf.image.flip_up_down(original)\n",
        "\n",
        "          augmented_X.append(hor_flip)\n",
        "          augmented_X.append(ver_flip)\n",
        "\n",
        "          for i in [1,2,3]:\n",
        "              augmented_X.append(tf.image.rot90(original, k=i))\n",
        "              augmented_X.append(tf.image.rot90(hor_flip, k=i))\n",
        "              augmented_X.append(tf.image.rot90(ver_flip, k=i))\n",
        "\n",
        "      # add augmented data to X_train\n",
        "      augmented_X_tensor = tf.convert_to_tensor(np.array(augmented_X))\n",
        "      X_train = tf.concat([X_train, augmented_X_tensor], 0)\n",
        "\n",
        "      # expanding y_train to match\n",
        "      y_train = pd.concat([y_train, pd.Series([1]*len(augmented_X))]).reset_index(drop=True)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(X_train.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      idx = [[i] for i in indices]\n",
        "      X_train = tf.gather_nd(X_train, indices=idx)\n",
        "      y_train = y_train[indices]\n",
        "\n",
        "\n",
        "    ################\n",
        "    # Data type for y values\n",
        "    ################\n",
        "\n",
        "    y_train = y_train.astype('float32')\n",
        "    y_test = y_test.astype('float32')\n",
        "    y_val = y_val.astype('float32')\n",
        "\n",
        "    return {'X_train': X_train,\n",
        "            'X_test': X_test,\n",
        "            'X_val': X_val,\n",
        "            'y_train': y_train,\n",
        "            'y_test': y_test,\n",
        "            'y_val': y_val}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8_xAJjEJM30j"
      },
      "outputs": [],
      "source": [
        "# Custom F1 Score\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "        recall = TP / (Positives+K.epsilon())\n",
        "        return recall\n",
        "\n",
        "\n",
        "    def precision_m(y_true, y_pred):\n",
        "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "\n",
        "        precision = TP / (Pred_Positives+K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
        "\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# custom loss function\n",
        "def wbce_custom(weight1_=30):\n",
        "\n",
        "    def wbce(y_true, y_pred, weight1=weight1_, weight0=1.):\n",
        "\n",
        "        tf.cast(y_true, tf.float32)\n",
        "        tf.cast(y_pred, tf.float32)\n",
        "        logloss = -(y_true * K.log(y_pred) * weight1 + (1 - y_true) * K.log(1 - y_pred) * weight0 )\n",
        "\n",
        "        return K.mean(logloss, axis=-1)\n",
        "\n",
        "    return wbce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnAa8YjDXMnx"
      },
      "source": [
        "#### Fitting NN Model (Model Specifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "rJVnzvojM30j"
      },
      "outputs": [],
      "source": [
        "def fit_nn(xy_data, model_type=\"convolution\", existing_model=None, metrics=['acc','AUC','Precision','Recall', f1],\n",
        "           loss=wbce_custom, optimizer='adam',\n",
        "          batch_size=64, epochs=20, dropout=0.25, patience=5, verbose=1):\n",
        "    '''\n",
        "    xy_data: dictionary with X_train, X_test, X_val, y_train, y_test, y_val in this order (dict)\n",
        "    model_type: \"baseline\"/\"convolution\" (string)\n",
        "    loss: \"binary_crossentropy\" (string)\n",
        "    metrics: list of metrics to track. available metrics are: (list of string/function)\n",
        "        - \"acc\"\n",
        "        - \"AUC\"\n",
        "        - \"Precision\"\n",
        "        - \"Recall\"\n",
        "        - f1\n",
        "    '''\n",
        "\n",
        "    # unpacking data\n",
        "    X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "    # weight initializer\n",
        "    # initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "\n",
        "    output_bias = tf.keras.initializers.Constant(np.log([sum(y_train)/(len(y_train)-sum(y_train))]))\n",
        "\n",
        "    # building model\n",
        "    if existing_model==None:\n",
        "      model = models.Sequential()\n",
        "\n",
        "      if model_type == \"baseline\":\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4, input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "      elif model_type == \"convolution\":\n",
        "        model.add(layers.Conv2D(filters=6, kernel_size=5, input_shape=(15,15,6), kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "        model.add(layers.Conv2D(filters=16, kernel_size=3, kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(layers.Activation('relu'))\n",
        "        model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(84, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "        model.add(layers.Dropout(dropout)),\n",
        "        model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "      elif model_type == \"convolution_v3\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=5, padding='same', input_shape=X_train.shape[1:], kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "          model.add(BatchNormalization())\n",
        "          model.add(layers.Activation('relu'))\n",
        "          model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "          model.add(layers.Conv2D(filters=16, kernel_size=5, kernel_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "          model.add(BatchNormalization())\n",
        "          model.add(layers.Activation('relu'))\n",
        "          model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(120, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(84, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))),\n",
        "          model.add(layers.Dropout(dropout)),\n",
        "          model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "          # model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "      elif model_type == \"convolution_v2\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=3, activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(5, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "      elif model_type == \"convolution_v1\":\n",
        "          model.add(layers.Conv2D(filters=6, kernel_size=3, activation='relu', padding='same', input_shape=X_train.shape[1:]))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.AveragePooling2D(pool_size=4, strides=4))\n",
        "          model.add(layers.Flatten())\n",
        "          model.add(layers.Dense(10, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(5, activation='relu'))\n",
        "          model.add(layers.Dropout(dropout))\n",
        "          model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "      else:\n",
        "          print('Model Type Undefined')\n",
        "\n",
        "      model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "      print(model.optimizer.get_config())\n",
        "      print(model.get_config())\n",
        "\n",
        "    else:\n",
        "      model = existing_model\n",
        "      print(model.optimizer.get_config())\n",
        "      print(model.get_config())\n",
        "\n",
        "    # callback for early stopping\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_f1', mode='max', patience=patience)\n",
        "\n",
        "    # callback for model checkpoint\n",
        "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/CapstoneProject/Models/Checkpoints',\n",
        "    #                                                                save_weights_only=True,\n",
        "    #                                                                monitor='val_f1',\n",
        "    #                                                                mode='max',\n",
        "    #                                                                save_best_only=True)\n",
        "\n",
        "    # Fit Model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[GarbageCollectorCallback(), es], verbose=verbose)\n",
        "\n",
        "    # Evaluate Model\n",
        "    result = model.evaluate(X_test, y_test, callbacks=[GarbageCollectorCallback()])\n",
        "\n",
        "    for i in [X_train, X_test, X_val, y_train, y_test, y_val]:\n",
        "      del i\n",
        "    gc.collect()\n",
        "\n",
        "    return model, history, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "geJfSI_osOUE"
      },
      "outputs": [],
      "source": [
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ykHPdYLKsnlk"
      },
      "outputs": [],
      "source": [
        "def run_nn(num_feature=3, model_type=\"convolution\", batch_size=64, epochs=100,\n",
        "           loss=wbce_custom(50), optimizer=Adam(learning_rate=0.0001),\n",
        "           existing_model = None, metrics=[\"f1\"], dropout=0.25, patience=5, verbose=1,\n",
        "           train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_test):\n",
        "\n",
        "    # start time\n",
        "    print(datetime.now())\n",
        "\n",
        "    # Getting Input Data\n",
        "    if num_feature == 1:\n",
        "        input_data_ = sites_data[['CHL', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 3:\n",
        "        input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 6:\n",
        "        input_data_ = sites_data[['ZSD', 'CHL', 'SPM', 'KD490', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    elif num_feature == 6.1:\n",
        "        input_data_ = sites_data[['SPM', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    else:\n",
        "      print('Number of features Error')\n",
        "\n",
        "    # Getting xy_data\n",
        "    xy_data = get_train_test_val_nn(input_data_,\n",
        "                          train_pairs,\n",
        "                          test_pairs)\n",
        "\n",
        "    # Get history and result\n",
        "    model_, history, result = fit_nn(xy_data, model_type, existing_model=existing_model,\n",
        "                                     batch_size=batch_size, epochs=epochs, loss=loss,\n",
        "                                     optimizer=optimizer, dropout=dropout, patience=patience, verbose=verbose)\n",
        "\n",
        "    # Plot\n",
        "    if (existing_model == None) & (verbose != 2) & (verbose != 0):\n",
        "      plot_train_val_loss(history.history, metrics=metrics)\n",
        "\n",
        "    del xy_data\n",
        "    gc.collect()\n",
        "\n",
        "    # end time\n",
        "    print(datetime.now())\n",
        "\n",
        "    return model_, history, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hfnHrElXTDp"
      },
      "source": [
        "#### Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "CiSo_cEfbBzI"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_plot(y_test, y_pred):\n",
        "\n",
        "    # if num_feature == 1:\n",
        "    #     input_data_ = sites_data[['CHL', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    # elif num_feature == 3:\n",
        "    #     input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "    # # Getting xy_data\n",
        "    # xy_data = get_train_test_val_nn(input_data_,\n",
        "    #                        time_site_pairs_train,\n",
        "    #                        time_site_pairs_test,\n",
        "    #                        oversampling = oversampling__)\n",
        "\n",
        "    # X_test = xy_data['X_test']\n",
        "    # y_test = xy_data['y_test']\n",
        "\n",
        "    # preds = model.predict(X_test)\n",
        "    # y_pred = np.where(preds>0.5,1,0)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "\n",
        "    # fig, ax = plt.subplots(figsize=(4,4))\n",
        "    # ax.imshow(cm)\n",
        "    # ax.grid(False)\n",
        "    # ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
        "    # ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
        "    # ax.set_ylim(1.5, -0.5)\n",
        "    # for i in range(2):\n",
        "    #     for j in range(2):\n",
        "    #         ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZaJTTR3nM30k"
      },
      "outputs": [],
      "source": [
        "def plot_train_val_loss(his, metrics=['f1', 'precision', 'recall']):\n",
        "    '''\n",
        "    input:\n",
        "        - history (dictionary)\n",
        "        - metrics (list of strings)\n",
        "    output: 2 graphs\n",
        "    '''\n",
        "    fig, axs = plt.subplots(len(metrics)+1, 1, figsize=(5,5+len(metrics)*2))\n",
        "    fig.tight_layout(pad=5)\n",
        "    axs[0].plot(his['loss'])\n",
        "    axs[0].plot(his['val_loss'])\n",
        "    axs[0].title.set_text('Training Loss vs Validation Loss')\n",
        "    axs[0].set_xlabel(\"Epochs\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend(['Training', 'Validation'])\n",
        "\n",
        "    for j, metric in enumerate(metrics):\n",
        "        axs[j+1].plot(his[f'{metric}'])\n",
        "        axs[j+1].plot(his[f'val_{metric}'])\n",
        "        axs[j+1].title.set_text(f'Training {metric} vs Validation {metric}')\n",
        "        axs[j+1].legend(['Training', 'Validation'])\n",
        "        axs[j+1].set_xlabel(\"Epochs\")\n",
        "        axs[j+1].set_ylabel(f\"{metric}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2xsH78d7Pdo_"
      },
      "outputs": [],
      "source": [
        "def get_combined_history(history_list):\n",
        "  '''\n",
        "  history_list (list): list of dictionaries containing histories, in order\n",
        "  '''\n",
        "  dd = defaultdict(list)\n",
        "  all_history = {}\n",
        "\n",
        "  for d in [old_history, history.history]:\n",
        "    for key, value in d.items():\n",
        "      dd[key].append(value)\n",
        "\n",
        "  for key, value in dd.items():\n",
        "      all_history[key] = [item for sublist in value for item in sublist]\n",
        "\n",
        "  return all_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5O5qTNXy3Hk"
      },
      "source": [
        "#### Save & Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dIx4yB2Xy4HR"
      },
      "outputs": [],
      "source": [
        "def save_model(model, history, result, model_notes, model_specs = '51x51'):\n",
        "  # model name\n",
        "  now = datetime.now()\n",
        "  day, month, time = now.day, now.month, now.strftime(\"%H%M\")\n",
        "  model_name = f\"nn_{model_specs}_{month}_{day}_{time}\"\n",
        "\n",
        "  # save model\n",
        "  model.save(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}.keras')\n",
        "\n",
        "  # save history\n",
        "  with open(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}_hist', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "\n",
        "  model_notes_row = [model_name, model_notes, result]\n",
        "\n",
        "  with open('/content/drive/My Drive/CapstoneProject/Models/model_notes.csv', 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    f.write(\"\\n\")\n",
        "    writer.writerow(model_notes_row)\n",
        "    f.close()\n",
        "\n",
        "  print(f\"Model Name: {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l3-TWlbe0kbI"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, loss_weight=30):\n",
        "  # Load Model\n",
        "  loss = wbce_custom(loss_weight)\n",
        "  metric = f1\n",
        "  old_model = tf.keras.models.load_model(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})\n",
        "\n",
        "  # Load History\n",
        "  with open(f'/content/drive/My Drive/CapstoneProject/Models/{model_name}_hist', \"rb\") as file_pi:\n",
        "      old_history = pickle.load(file_pi)\n",
        "\n",
        "  notes = pd.read_csv((\"/content/drive/My Drive/CapstoneProject/Models/model_notes.csv\"))\n",
        "  print(notes[notes['Model Name'] == model_name]['Notes'].iloc[0])\n",
        "\n",
        "  return old_model, old_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b7DOTtM30k"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orGBi9j_QSm6"
      },
      "source": [
        "#### Training From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inlpz-2fyRS-"
      },
      "source": [
        "##### Without Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iZBA7FJpdPQ7",
        "outputId": "99589db1-fe8b-4167-d871-6371d2fe2702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-19 02:24:00.026959\n",
            "{'name': 'Adam', 'learning_rate': 1e-05, 'decay': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_22', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_44_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_44', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_44', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_44', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_43', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'same', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_45', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_45', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_45', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'MaxPooling2D', 'config': {'name': 'max_pooling2d_44', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_22', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_88', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_66', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_89', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_67', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_90', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_68', 'trainable': True, 'dtype': 'float32', 'rate': 0.1, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_91', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "Epoch 1/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 3.1044 - acc: 0.5766 - auc: 0.5294 - precision: 0.2758 - recall: 0.4111 - f1: 0.3038 - val_loss: 2.6415 - val_acc: 0.4776 - val_auc: 0.5807 - val_precision: 0.0371 - val_recall: 0.6687 - val_f1: 0.0691\n",
            "Epoch 2/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.9692 - acc: 0.5181 - auc: 0.5678 - precision: 0.2953 - recall: 0.6482 - f1: 0.4001 - val_loss: 2.5647 - val_acc: 0.4915 - val_auc: 0.5875 - val_precision: 0.0379 - val_recall: 0.6642 - val_f1: 0.0702\n",
            "Epoch 3/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8999 - acc: 0.5200 - auc: 0.5784 - precision: 0.3004 - recall: 0.6709 - f1: 0.4105 - val_loss: 2.5109 - val_acc: 0.4984 - val_auc: 0.5889 - val_precision: 0.0380 - val_recall: 0.6566 - val_f1: 0.0704\n",
            "Epoch 4/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8505 - acc: 0.5268 - auc: 0.5875 - precision: 0.3047 - recall: 0.6747 - f1: 0.4151 - val_loss: 2.4695 - val_acc: 0.4978 - val_auc: 0.5916 - val_precision: 0.0382 - val_recall: 0.6611 - val_f1: 0.0708\n",
            "Epoch 5/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.8126 - acc: 0.5311 - auc: 0.5881 - precision: 0.3069 - recall: 0.6735 - f1: 0.4171 - val_loss: 2.4370 - val_acc: 0.4996 - val_auc: 0.5922 - val_precision: 0.0383 - val_recall: 0.6596 - val_f1: 0.0709\n",
            "Epoch 6/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7815 - acc: 0.5337 - auc: 0.5917 - precision: 0.3080 - recall: 0.6715 - f1: 0.4178 - val_loss: 2.4098 - val_acc: 0.5031 - val_auc: 0.5943 - val_precision: 0.0384 - val_recall: 0.6581 - val_f1: 0.0712\n",
            "Epoch 7/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.7547 - acc: 0.5364 - auc: 0.5926 - precision: 0.3091 - recall: 0.6694 - f1: 0.4184 - val_loss: 2.3868 - val_acc: 0.5037 - val_auc: 0.5951 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 8/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7318 - acc: 0.5391 - auc: 0.5953 - precision: 0.3104 - recall: 0.6681 - f1: 0.4189 - val_loss: 2.3659 - val_acc: 0.5043 - val_auc: 0.5966 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 9/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.7111 - acc: 0.5412 - auc: 0.5976 - precision: 0.3123 - recall: 0.6723 - f1: 0.4220 - val_loss: 2.3484 - val_acc: 0.5039 - val_auc: 0.5956 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 10/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.6938 - acc: 0.5414 - auc: 0.5974 - precision: 0.3122 - recall: 0.6708 - f1: 0.4212 - val_loss: 2.3335 - val_acc: 0.5033 - val_auc: 0.5971 - val_precision: 0.0388 - val_recall: 0.6642 - val_f1: 0.0719\n",
            "Epoch 11/500\n",
            "1829/1829 [==============================] - 9s 5ms/step - loss: 2.6788 - acc: 0.5422 - auc: 0.5973 - precision: 0.3119 - recall: 0.6669 - f1: 0.4206 - val_loss: 2.3195 - val_acc: 0.5006 - val_auc: 0.5968 - val_precision: 0.0387 - val_recall: 0.6672 - val_f1: 0.0718\n",
            "Epoch 12/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.6629 - acc: 0.5439 - auc: 0.5996 - precision: 0.3139 - recall: 0.6722 - f1: 0.4228 - val_loss: 2.3056 - val_acc: 0.5050 - val_auc: 0.5961 - val_precision: 0.0389 - val_recall: 0.6642 - val_f1: 0.0720\n",
            "Epoch 13/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.6501 - acc: 0.5432 - auc: 0.5994 - precision: 0.3134 - recall: 0.6721 - f1: 0.4226 - val_loss: 2.2947 - val_acc: 0.4998 - val_auc: 0.5990 - val_precision: 0.0387 - val_recall: 0.6672 - val_f1: 0.0716\n",
            "Epoch 14/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6375 - acc: 0.5427 - auc: 0.5990 - precision: 0.3126 - recall: 0.6690 - f1: 0.4218 - val_loss: 2.2823 - val_acc: 0.5048 - val_auc: 0.5988 - val_precision: 0.0390 - val_recall: 0.6657 - val_f1: 0.0721\n",
            "Epoch 15/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6257 - acc: 0.5446 - auc: 0.6017 - precision: 0.3139 - recall: 0.6703 - f1: 0.4229 - val_loss: 2.2736 - val_acc: 0.5013 - val_auc: 0.5983 - val_precision: 0.0388 - val_recall: 0.6672 - val_f1: 0.0718\n",
            "Epoch 16/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6157 - acc: 0.5444 - auc: 0.6009 - precision: 0.3139 - recall: 0.6707 - f1: 0.4233 - val_loss: 2.2624 - val_acc: 0.5050 - val_auc: 0.5978 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0718\n",
            "Epoch 17/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.6052 - acc: 0.5453 - auc: 0.6023 - precision: 0.3146 - recall: 0.6718 - f1: 0.4238 - val_loss: 2.2539 - val_acc: 0.5045 - val_auc: 0.5992 - val_precision: 0.0389 - val_recall: 0.6642 - val_f1: 0.0719\n",
            "Epoch 18/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5959 - acc: 0.5469 - auc: 0.6018 - precision: 0.3158 - recall: 0.6734 - f1: 0.4254 - val_loss: 2.2441 - val_acc: 0.5058 - val_auc: 0.5994 - val_precision: 0.0388 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 19/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5870 - acc: 0.5453 - auc: 0.6019 - precision: 0.3145 - recall: 0.6715 - f1: 0.4237 - val_loss: 2.2366 - val_acc: 0.5046 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 20/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5791 - acc: 0.5438 - auc: 0.6007 - precision: 0.3128 - recall: 0.6665 - f1: 0.4214 - val_loss: 2.2292 - val_acc: 0.5047 - val_auc: 0.5989 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 21/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5708 - acc: 0.5464 - auc: 0.6021 - precision: 0.3151 - recall: 0.6708 - f1: 0.4242 - val_loss: 2.2224 - val_acc: 0.5034 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6627 - val_f1: 0.0717\n",
            "Epoch 22/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5631 - acc: 0.5469 - auc: 0.6032 - precision: 0.3156 - recall: 0.6725 - f1: 0.4248 - val_loss: 2.2150 - val_acc: 0.5050 - val_auc: 0.5972 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 23/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5552 - acc: 0.5472 - auc: 0.6048 - precision: 0.3158 - recall: 0.6725 - f1: 0.4249 - val_loss: 2.2089 - val_acc: 0.5021 - val_auc: 0.5996 - val_precision: 0.0387 - val_recall: 0.6642 - val_f1: 0.0717\n",
            "Epoch 24/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5490 - acc: 0.5472 - auc: 0.6034 - precision: 0.3155 - recall: 0.6709 - f1: 0.4240 - val_loss: 2.2020 - val_acc: 0.5036 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6627 - val_f1: 0.0717\n",
            "Epoch 25/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5426 - acc: 0.5455 - auc: 0.6030 - precision: 0.3141 - recall: 0.6684 - f1: 0.4232 - val_loss: 2.1952 - val_acc: 0.5049 - val_auc: 0.5994 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 26/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5357 - acc: 0.5475 - auc: 0.6043 - precision: 0.3156 - recall: 0.6702 - f1: 0.4244 - val_loss: 2.1914 - val_acc: 0.5033 - val_auc: 0.5981 - val_precision: 0.0386 - val_recall: 0.6611 - val_f1: 0.0716\n",
            "Epoch 27/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5304 - acc: 0.5476 - auc: 0.6038 - precision: 0.3158 - recall: 0.6711 - f1: 0.4253 - val_loss: 2.1840 - val_acc: 0.5061 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0717\n",
            "Epoch 28/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5237 - acc: 0.5484 - auc: 0.6047 - precision: 0.3167 - recall: 0.6734 - f1: 0.4261 - val_loss: 2.1786 - val_acc: 0.5062 - val_auc: 0.5985 - val_precision: 0.0388 - val_recall: 0.6611 - val_f1: 0.0720\n",
            "Epoch 29/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5188 - acc: 0.5477 - auc: 0.6039 - precision: 0.3156 - recall: 0.6692 - f1: 0.4244 - val_loss: 2.1745 - val_acc: 0.5029 - val_auc: 0.5985 - val_precision: 0.0388 - val_recall: 0.6642 - val_f1: 0.0718\n",
            "Epoch 30/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5120 - acc: 0.5487 - auc: 0.6074 - precision: 0.3166 - recall: 0.6718 - f1: 0.4255 - val_loss: 2.1685 - val_acc: 0.5071 - val_auc: 0.5989 - val_precision: 0.0390 - val_recall: 0.6627 - val_f1: 0.0722\n",
            "Epoch 31/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5078 - acc: 0.5489 - auc: 0.6036 - precision: 0.3170 - recall: 0.6737 - f1: 0.4260 - val_loss: 2.1644 - val_acc: 0.5045 - val_auc: 0.5989 - val_precision: 0.0388 - val_recall: 0.6627 - val_f1: 0.0719\n",
            "Epoch 32/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.5029 - acc: 0.5491 - auc: 0.6057 - precision: 0.3173 - recall: 0.6746 - f1: 0.4273 - val_loss: 2.1593 - val_acc: 0.5053 - val_auc: 0.5997 - val_precision: 0.0387 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 33/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4981 - acc: 0.5489 - auc: 0.6055 - precision: 0.3170 - recall: 0.6736 - f1: 0.4265 - val_loss: 2.1542 - val_acc: 0.5057 - val_auc: 0.5987 - val_precision: 0.0387 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 34/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4939 - acc: 0.5478 - auc: 0.6048 - precision: 0.3157 - recall: 0.6694 - f1: 0.4244 - val_loss: 2.1500 - val_acc: 0.5077 - val_auc: 0.5977 - val_precision: 0.0387 - val_recall: 0.6566 - val_f1: 0.0716\n",
            "Epoch 35/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4883 - acc: 0.5487 - auc: 0.6062 - precision: 0.3164 - recall: 0.6710 - f1: 0.4253 - val_loss: 2.1463 - val_acc: 0.5056 - val_auc: 0.5984 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 36/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4845 - acc: 0.5481 - auc: 0.6050 - precision: 0.3163 - recall: 0.6725 - f1: 0.4256 - val_loss: 2.1408 - val_acc: 0.5070 - val_auc: 0.5992 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 37/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4798 - acc: 0.5494 - auc: 0.6074 - precision: 0.3170 - recall: 0.6718 - f1: 0.4259 - val_loss: 2.1373 - val_acc: 0.5065 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 38/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4754 - acc: 0.5487 - auc: 0.6066 - precision: 0.3168 - recall: 0.6731 - f1: 0.4263 - val_loss: 2.1364 - val_acc: 0.5013 - val_auc: 0.6000 - val_precision: 0.0386 - val_recall: 0.6642 - val_f1: 0.0716\n",
            "Epoch 39/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4718 - acc: 0.5481 - auc: 0.6069 - precision: 0.3164 - recall: 0.6725 - f1: 0.4253 - val_loss: 2.1301 - val_acc: 0.5063 - val_auc: 0.5985 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 40/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4676 - acc: 0.5497 - auc: 0.6068 - precision: 0.3175 - recall: 0.6736 - f1: 0.4269 - val_loss: 2.1274 - val_acc: 0.5056 - val_auc: 0.5993 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 41/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4635 - acc: 0.5487 - auc: 0.6076 - precision: 0.3171 - recall: 0.6748 - f1: 0.4271 - val_loss: 2.1234 - val_acc: 0.5046 - val_auc: 0.5988 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 42/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4605 - acc: 0.5485 - auc: 0.6062 - precision: 0.3170 - recall: 0.6751 - f1: 0.4266 - val_loss: 2.1193 - val_acc: 0.5066 - val_auc: 0.5990 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0716\n",
            "Epoch 43/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4579 - acc: 0.5500 - auc: 0.6052 - precision: 0.3176 - recall: 0.6734 - f1: 0.4270 - val_loss: 2.1146 - val_acc: 0.5079 - val_auc: 0.5992 - val_precision: 0.0387 - val_recall: 0.6566 - val_f1: 0.0717\n",
            "Epoch 44/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4532 - acc: 0.5492 - auc: 0.6064 - precision: 0.3171 - recall: 0.6731 - f1: 0.4266 - val_loss: 2.1132 - val_acc: 0.5061 - val_auc: 0.5984 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0717\n",
            "Epoch 45/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4501 - acc: 0.5502 - auc: 0.6057 - precision: 0.3179 - recall: 0.6739 - f1: 0.4274 - val_loss: 2.1104 - val_acc: 0.5051 - val_auc: 0.5989 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 46/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4473 - acc: 0.5484 - auc: 0.6049 - precision: 0.3166 - recall: 0.6729 - f1: 0.4262 - val_loss: 2.1082 - val_acc: 0.5042 - val_auc: 0.5982 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 47/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4430 - acc: 0.5492 - auc: 0.6066 - precision: 0.3175 - recall: 0.6756 - f1: 0.4268 - val_loss: 2.1033 - val_acc: 0.5066 - val_auc: 0.5982 - val_precision: 0.0387 - val_recall: 0.6581 - val_f1: 0.0717\n",
            "Epoch 48/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4403 - acc: 0.5485 - auc: 0.6056 - precision: 0.3165 - recall: 0.6722 - f1: 0.4260 - val_loss: 2.1015 - val_acc: 0.5052 - val_auc: 0.5981 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 49/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4368 - acc: 0.5492 - auc: 0.6066 - precision: 0.3174 - recall: 0.6749 - f1: 0.4268 - val_loss: 2.0979 - val_acc: 0.5053 - val_auc: 0.5984 - val_precision: 0.0386 - val_recall: 0.6581 - val_f1: 0.0715\n",
            "Epoch 50/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.4336 - acc: 0.5496 - auc: 0.6070 - precision: 0.3174 - recall: 0.6735 - f1: 0.4264 - val_loss: 2.0964 - val_acc: 0.5036 - val_auc: 0.5982 - val_precision: 0.0386 - val_recall: 0.6596 - val_f1: 0.0715\n",
            "Epoch 51/500\n",
            "1829/1829 [==============================] - 8s 5ms/step - loss: 2.4309 - acc: 0.5483 - auc: 0.6063 - precision: 0.3168 - recall: 0.6743 - f1: 0.4262 - val_loss: 2.0931 - val_acc: 0.5047 - val_auc: 0.5977 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 52/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4273 - acc: 0.5500 - auc: 0.6073 - precision: 0.3182 - recall: 0.6770 - f1: 0.4284 - val_loss: 2.0884 - val_acc: 0.5075 - val_auc: 0.5981 - val_precision: 0.0389 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 53/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4250 - acc: 0.5494 - auc: 0.6062 - precision: 0.3173 - recall: 0.6737 - f1: 0.4268 - val_loss: 2.0853 - val_acc: 0.5082 - val_auc: 0.5977 - val_precision: 0.0389 - val_recall: 0.6596 - val_f1: 0.0720\n",
            "Epoch 54/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4215 - acc: 0.5489 - auc: 0.6083 - precision: 0.3175 - recall: 0.6763 - f1: 0.4275 - val_loss: 2.0833 - val_acc: 0.5071 - val_auc: 0.5988 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0719\n",
            "Epoch 55/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4194 - acc: 0.5487 - auc: 0.6065 - precision: 0.3173 - recall: 0.6761 - f1: 0.4275 - val_loss: 2.0820 - val_acc: 0.5042 - val_auc: 0.5985 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 56/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4172 - acc: 0.5490 - auc: 0.6052 - precision: 0.3174 - recall: 0.6755 - f1: 0.4272 - val_loss: 2.0792 - val_acc: 0.5049 - val_auc: 0.5992 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 57/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4137 - acc: 0.5482 - auc: 0.6071 - precision: 0.3170 - recall: 0.6758 - f1: 0.4274 - val_loss: 2.0761 - val_acc: 0.5045 - val_auc: 0.5991 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0717\n",
            "Epoch 58/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4115 - acc: 0.5498 - auc: 0.6053 - precision: 0.3175 - recall: 0.6737 - f1: 0.4268 - val_loss: 2.0745 - val_acc: 0.5045 - val_auc: 0.5986 - val_precision: 0.0387 - val_recall: 0.6611 - val_f1: 0.0718\n",
            "Epoch 59/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4081 - acc: 0.5495 - auc: 0.6073 - precision: 0.3178 - recall: 0.6762 - f1: 0.4275 - val_loss: 2.0712 - val_acc: 0.5064 - val_auc: 0.5983 - val_precision: 0.0388 - val_recall: 0.6596 - val_f1: 0.0718\n",
            "Epoch 60/500\n",
            "1829/1829 [==============================] - 8s 4ms/step - loss: 2.4061 - acc: 0.5495 - auc: 0.6078 - precision: 0.3178 - recall: 0.6763 - f1: 0.4281 - val_loss: 2.0705 - val_acc: 0.5023 - val_auc: 0.5987 - val_precision: 0.0387 - val_recall: 0.6642 - val_f1: 0.0717\n",
            "1170/1170 [==============================] - 1s 993us/step - loss: 2.0690 - acc: 0.5054 - auc: 0.6128 - precision: 0.0401 - recall: 0.6863 - f1: 0.0717\n",
            "2023-08-19 02:32:23.350497\n",
            "CPU times: user 15min 32s, sys: 2min 57s, total: 18min 30s\n",
            "Wall time: 8min 23s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%time\n",
        "# run model\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=64, epochs=500,\n",
        "           loss=wbce_custom(3.5), optimizer=Adam(learning_rate=0.00001, decay=0.001), dropout=0.1, patience=30,\n",
        "           existing_model = None, metrics=['f1'], train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ2WRerRdX5E"
      },
      "source": [
        "##### With Cross Validation (Takes up too much RAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Qp17GxM30k"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "\n",
        "# model_types = [\"convolution\"]\n",
        "# num_features = [3]\n",
        "# i = 0\n",
        "\n",
        "# for model_type, num_feature in itertools.product(model_types, num_features):\n",
        "\n",
        "#     for j in range(5):\n",
        "#       # Getting Input Data\n",
        "#       if num_feature == 1:\n",
        "#           input_data_ = sites_data[['CHL', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "#       elif num_feature == 3:\n",
        "#           input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "#       # Getting xy_data\n",
        "#       xy_data = get_train_test_val_nn(input_data_,\n",
        "#                             train_test_dict[f'train_{j+1}'],\n",
        "#                             train_test_dict[f'test_{j+1}'])\n",
        "\n",
        "#       # Get history and result\n",
        "#       model_, history, result = fit_nn(xy_data, model_type, batch_size=32, epochs=3, loss=wbce_custom(50), optimizer=Adam(learning_rate=0.0005))\n",
        "#       model_list.append(model_)\n",
        "#       histories.append(history)\n",
        "#       results.append(result)\n",
        "\n",
        "#       i += 1\n",
        "#       clear_output(wait=True)\n",
        "\n",
        "#       K.clear_session()\n",
        "#       tf.compat.v1.reset_default_graph()\n",
        "#       gc.collect()\n",
        "\n",
        "#       print(f'Progress: {i}/{len(model_types)*len(num_features)*5}')\n",
        "#       print(datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY6Q610dder6"
      },
      "source": [
        "##### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Wr3Fjwdr4l",
        "outputId": "82613b31-6680-4cba-f903-ed648697e14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Name: nn_15x15_7_23_2222\n"
          ]
        }
      ],
      "source": [
        "model_notes = '''dataset: L3 1km x 1km 15x15, fillna = mean then 0, num_feature=6, model_type=\"convolution v3 (lenet)\", batch_size=64, epochs=98,\n",
        "           loss=wbce_custom(30), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = None, metrics=[\"f1\"]. added weight decay l2 regularisation. added batch norm. use kernel_size = 3 instead of 5.\n",
        "           early stopping patience = 30. '''\n",
        "save_model(model, history, result, model_notes, model_specs = '15x15')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvuJxT6xz2u2"
      },
      "source": [
        "##### (Attempt to) Clear RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxeFhWesldyu"
      },
      "outputs": [],
      "source": [
        "# del model\n",
        "# del history\n",
        "# del result\n",
        "# K.clear_session()\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrurCkNFrJ3-"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# def sizeof_fmt(num, suffix='B'):\n",
        "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "#         if abs(num) < 1024.0:\n",
        "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "#         num /= 1024.0\n",
        "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "#                          key= lambda x: -x[1])[:10]:\n",
        "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MflnfHQXCz"
      },
      "source": [
        "#### Training From Existing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbDjSSIaoPmH"
      },
      "source": [
        "##### Load Model From File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paz-QwSVn2o_",
        "outputId": "c1b14e6e-8c01-497a-d20f-e4356b527b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: L3 1km x 1km 15x15, fillna = mean then 0, num_feature=6, model_type=\"convolution v3 (lenet)\", batch_size=64, epochs=200,\n",
            "           loss=wbce_custom(40), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
            "           existing_model = None, metrics=[\"f1\"]. added weight decay l2 regularisation. added batch norm. removed time and site pairs with >8000\n"
          ]
        }
      ],
      "source": [
        "old_model, old_history = load_model('nn_15x15_7_20_1323', loss_weight=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um8esTkeoRAe"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCXBh1-BPFG",
        "outputId": "7b713ac3-8871-41a6-b9d9-c9d1e6d0412e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-20 13:41:36.949359\n",
            "X_train shape: (6052, 15, 15, 6)\n",
            "{'name': 'Adam', 'learning_rate': 5e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_2', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_4_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_4', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_4', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_4', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_5', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_5', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_5', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_6', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.25, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "Epoch 1/200\n",
            "95/95 [==============================] - 3s 21ms/step - loss: 1.4283 - acc: 0.6281 - auc: 0.8495 - precision: 0.0959 - recall: 0.8973 - f1: 0.1693 - val_loss: 2.4292 - val_acc: 0.6440 - val_auc: 0.5914 - val_precision: 0.0645 - val_recall: 0.5303 - val_f1: 0.1084\n",
            "Epoch 2/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.4484 - acc: 0.6112 - auc: 0.8412 - precision: 0.0940 - recall: 0.9202 - f1: 0.1620 - val_loss: 2.3875 - val_acc: 0.5416 - val_auc: 0.5840 - val_precision: 0.0540 - val_recall: 0.5758 - val_f1: 0.0951\n",
            "Epoch 3/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.4310 - acc: 0.5905 - auc: 0.8460 - precision: 0.0900 - recall: 0.9240 - f1: 0.1594 - val_loss: 2.3954 - val_acc: 0.6176 - val_auc: 0.5911 - val_precision: 0.0645 - val_recall: 0.5758 - val_f1: 0.1131\n",
            "Epoch 4/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4084 - acc: 0.6279 - auc: 0.8539 - precision: 0.0975 - recall: 0.9163 - f1: 0.1747 - val_loss: 2.4252 - val_acc: 0.6004 - val_auc: 0.5918 - val_precision: 0.0604 - val_recall: 0.5606 - val_f1: 0.1053\n",
            "Epoch 5/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4651 - acc: 0.6059 - auc: 0.8340 - precision: 0.0922 - recall: 0.9125 - f1: 0.1631 - val_loss: 2.4361 - val_acc: 0.5760 - val_auc: 0.5865 - val_precision: 0.0556 - val_recall: 0.5455 - val_f1: 0.0972\n",
            "Epoch 6/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4218 - acc: 0.6006 - auc: 0.8470 - precision: 0.0914 - recall: 0.9163 - f1: 0.1608 - val_loss: 2.4751 - val_acc: 0.6724 - val_auc: 0.5914 - val_precision: 0.0630 - val_recall: 0.4697 - val_f1: 0.1054\n",
            "Epoch 7/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4087 - acc: 0.6015 - auc: 0.8510 - precision: 0.0919 - recall: 0.9202 - f1: 0.1625 - val_loss: 2.5540 - val_acc: 0.6757 - val_auc: 0.5818 - val_precision: 0.0582 - val_recall: 0.4242 - val_f1: 0.0961\n",
            "Epoch 8/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4498 - acc: 0.6138 - auc: 0.8400 - precision: 0.0933 - recall: 0.9049 - f1: 0.1657 - val_loss: 2.4363 - val_acc: 0.5687 - val_auc: 0.5816 - val_precision: 0.0560 - val_recall: 0.5606 - val_f1: 0.0994\n",
            "Epoch 9/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4544 - acc: 0.6140 - auc: 0.8395 - precision: 0.0914 - recall: 0.8821 - f1: 0.1629 - val_loss: 2.4466 - val_acc: 0.6050 - val_auc: 0.5812 - val_precision: 0.0596 - val_recall: 0.5455 - val_f1: 0.1053\n",
            "Epoch 10/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4195 - acc: 0.6152 - auc: 0.8506 - precision: 0.0943 - recall: 0.9125 - f1: 0.1655 - val_loss: 2.4895 - val_acc: 0.6229 - val_auc: 0.5823 - val_precision: 0.0593 - val_recall: 0.5152 - val_f1: 0.1030\n",
            "Epoch 11/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4130 - acc: 0.6310 - auc: 0.8507 - precision: 0.0986 - recall: 0.9202 - f1: 0.1721 - val_loss: 2.4521 - val_acc: 0.6255 - val_auc: 0.5833 - val_precision: 0.0643 - val_recall: 0.5606 - val_f1: 0.1126\n",
            "Epoch 12/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4381 - acc: 0.6423 - auc: 0.8478 - precision: 0.0994 - recall: 0.8973 - f1: 0.1768 - val_loss: 2.4726 - val_acc: 0.5363 - val_auc: 0.5770 - val_precision: 0.0521 - val_recall: 0.5606 - val_f1: 0.0920\n",
            "Epoch 13/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4365 - acc: 0.6147 - auc: 0.8453 - precision: 0.0932 - recall: 0.9011 - f1: 0.1659 - val_loss: 2.4458 - val_acc: 0.6196 - val_auc: 0.5899 - val_precision: 0.0603 - val_recall: 0.5303 - val_f1: 0.1079\n",
            "Epoch 14/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4508 - acc: 0.6274 - auc: 0.8403 - precision: 0.0968 - recall: 0.9087 - f1: 0.1711 - val_loss: 2.4734 - val_acc: 0.5476 - val_auc: 0.5771 - val_precision: 0.0547 - val_recall: 0.5758 - val_f1: 0.0969\n",
            "Epoch 15/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3991 - acc: 0.6254 - auc: 0.8547 - precision: 0.0969 - recall: 0.9163 - f1: 0.1698 - val_loss: 2.4979 - val_acc: 0.6222 - val_auc: 0.5826 - val_precision: 0.0592 - val_recall: 0.5152 - val_f1: 0.1049\n",
            "Epoch 16/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4242 - acc: 0.6256 - auc: 0.8438 - precision: 0.0973 - recall: 0.9202 - f1: 0.1740 - val_loss: 2.5476 - val_acc: 0.6044 - val_auc: 0.5800 - val_precision: 0.0566 - val_recall: 0.5152 - val_f1: 0.0980\n",
            "Epoch 17/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3655 - acc: 0.6173 - auc: 0.8666 - precision: 0.0967 - recall: 0.9354 - f1: 0.1703 - val_loss: 2.4236 - val_acc: 0.5746 - val_auc: 0.5895 - val_precision: 0.0554 - val_recall: 0.5455 - val_f1: 0.0979\n",
            "Epoch 18/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3886 - acc: 0.6371 - auc: 0.8584 - precision: 0.1005 - recall: 0.9240 - f1: 0.1753 - val_loss: 2.4677 - val_acc: 0.5211 - val_auc: 0.5797 - val_precision: 0.0529 - val_recall: 0.5909 - val_f1: 0.0939\n",
            "Epoch 19/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4079 - acc: 0.6074 - auc: 0.8533 - precision: 0.0926 - recall: 0.9125 - f1: 0.1640 - val_loss: 2.5111 - val_acc: 0.5548 - val_auc: 0.5752 - val_precision: 0.0529 - val_recall: 0.5455 - val_f1: 0.0939\n",
            "Epoch 20/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3836 - acc: 0.6373 - auc: 0.8611 - precision: 0.1005 - recall: 0.9240 - f1: 0.1768 - val_loss: 2.5431 - val_acc: 0.6050 - val_auc: 0.5844 - val_precision: 0.0596 - val_recall: 0.5455 - val_f1: 0.1065\n",
            "Epoch 21/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.4135 - acc: 0.6165 - auc: 0.8491 - precision: 0.0952 - recall: 0.9202 - f1: 0.1703 - val_loss: 2.5034 - val_acc: 0.5892 - val_auc: 0.5844 - val_precision: 0.0559 - val_recall: 0.5303 - val_f1: 0.0982\n",
            "Epoch 22/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4140 - acc: 0.6319 - auc: 0.8492 - precision: 0.0985 - recall: 0.9163 - f1: 0.1729 - val_loss: 2.4616 - val_acc: 0.5277 - val_auc: 0.5839 - val_precision: 0.0524 - val_recall: 0.5758 - val_f1: 0.0936\n",
            "Epoch 23/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3787 - acc: 0.6191 - auc: 0.8614 - precision: 0.0971 - recall: 0.9354 - f1: 0.1714 - val_loss: 2.5500 - val_acc: 0.6387 - val_auc: 0.5824 - val_precision: 0.0619 - val_recall: 0.5152 - val_f1: 0.1094\n",
            "Epoch 24/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3977 - acc: 0.6385 - auc: 0.8573 - precision: 0.0998 - recall: 0.9125 - f1: 0.1749 - val_loss: 2.5209 - val_acc: 0.5594 - val_auc: 0.5812 - val_precision: 0.0548 - val_recall: 0.5606 - val_f1: 0.0974\n",
            "Epoch 25/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.4051 - acc: 0.6267 - auc: 0.8517 - precision: 0.0973 - recall: 0.9163 - f1: 0.1726 - val_loss: 2.5558 - val_acc: 0.6816 - val_auc: 0.5890 - val_precision: 0.0667 - val_recall: 0.4848 - val_f1: 0.1138\n",
            "Epoch 26/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3446 - acc: 0.6550 - auc: 0.8689 - precision: 0.1058 - recall: 0.9316 - f1: 0.1847 - val_loss: 2.6724 - val_acc: 0.6962 - val_auc: 0.5816 - val_precision: 0.0603 - val_recall: 0.4091 - val_f1: 0.0964\n",
            "Epoch 27/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3998 - acc: 0.6421 - auc: 0.8552 - precision: 0.1000 - recall: 0.9049 - f1: 0.1756 - val_loss: 2.5955 - val_acc: 0.5694 - val_auc: 0.5738 - val_precision: 0.0506 - val_recall: 0.5000 - val_f1: 0.0867\n",
            "Epoch 28/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3811 - acc: 0.6246 - auc: 0.8582 - precision: 0.0987 - recall: 0.9392 - f1: 0.1772 - val_loss: 2.5551 - val_acc: 0.5799 - val_auc: 0.5786 - val_precision: 0.0533 - val_recall: 0.5152 - val_f1: 0.0931\n",
            "Epoch 29/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3695 - acc: 0.6545 - auc: 0.8608 - precision: 0.1050 - recall: 0.9240 - f1: 0.1839 - val_loss: 2.4970 - val_acc: 0.5238 - val_auc: 0.5757 - val_precision: 0.0520 - val_recall: 0.5758 - val_f1: 0.0931\n",
            "Epoch 30/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3589 - acc: 0.6404 - auc: 0.8662 - precision: 0.1020 - recall: 0.9316 - f1: 0.1818 - val_loss: 2.5298 - val_acc: 0.5865 - val_auc: 0.5808 - val_precision: 0.0541 - val_recall: 0.5152 - val_f1: 0.0955\n",
            "Epoch 31/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3724 - acc: 0.6328 - auc: 0.8628 - precision: 0.1007 - recall: 0.9392 - f1: 0.1773 - val_loss: 2.5811 - val_acc: 0.6328 - val_auc: 0.5801 - val_precision: 0.0594 - val_recall: 0.5000 - val_f1: 0.1039\n",
            "Epoch 32/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3535 - acc: 0.6548 - auc: 0.8658 - precision: 0.1041 - recall: 0.9125 - f1: 0.1805 - val_loss: 2.5402 - val_acc: 0.5416 - val_auc: 0.5765 - val_precision: 0.0514 - val_recall: 0.5455 - val_f1: 0.0908\n",
            "Epoch 33/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3147 - acc: 0.6507 - auc: 0.8782 - precision: 0.1040 - recall: 0.9240 - f1: 0.1839 - val_loss: 2.5473 - val_acc: 0.5462 - val_auc: 0.5790 - val_precision: 0.0519 - val_recall: 0.5455 - val_f1: 0.0923\n",
            "Epoch 34/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.3746 - acc: 0.6492 - auc: 0.8627 - precision: 0.1029 - recall: 0.9163 - f1: 0.1806 - val_loss: 2.7018 - val_acc: 0.5984 - val_auc: 0.5723 - val_precision: 0.0557 - val_recall: 0.5152 - val_f1: 0.0942\n",
            "Epoch 35/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.3630 - acc: 0.6406 - auc: 0.8656 - precision: 0.1013 - recall: 0.9240 - f1: 0.1789 - val_loss: 2.6780 - val_acc: 0.5964 - val_auc: 0.5733 - val_precision: 0.0525 - val_recall: 0.4848 - val_f1: 0.0909\n",
            "Epoch 36/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3670 - acc: 0.6302 - auc: 0.8659 - precision: 0.0987 - recall: 0.9240 - f1: 0.1752 - val_loss: 2.5538 - val_acc: 0.6176 - val_auc: 0.5845 - val_precision: 0.0585 - val_recall: 0.5152 - val_f1: 0.1023\n",
            "Epoch 37/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3623 - acc: 0.6434 - auc: 0.8653 - precision: 0.1014 - recall: 0.9163 - f1: 0.1797 - val_loss: 2.5896 - val_acc: 0.5707 - val_auc: 0.5732 - val_precision: 0.0508 - val_recall: 0.5000 - val_f1: 0.0896\n",
            "Epoch 38/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3833 - acc: 0.6461 - auc: 0.8604 - precision: 0.1034 - recall: 0.9316 - f1: 0.1833 - val_loss: 2.7649 - val_acc: 0.6777 - val_auc: 0.5739 - val_precision: 0.0586 - val_recall: 0.4242 - val_f1: 0.0948\n",
            "Epoch 39/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3925 - acc: 0.6466 - auc: 0.8578 - precision: 0.1019 - recall: 0.9125 - f1: 0.1763 - val_loss: 2.5852 - val_acc: 0.5984 - val_auc: 0.5820 - val_precision: 0.0572 - val_recall: 0.5303 - val_f1: 0.1011\n",
            "Epoch 40/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3557 - acc: 0.6431 - auc: 0.8669 - precision: 0.1020 - recall: 0.9240 - f1: 0.1808 - val_loss: 2.6771 - val_acc: 0.6156 - val_auc: 0.5745 - val_precision: 0.0521 - val_recall: 0.4545 - val_f1: 0.0871\n",
            "Epoch 41/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3493 - acc: 0.6545 - auc: 0.8683 - precision: 0.1036 - recall: 0.9087 - f1: 0.1794 - val_loss: 2.6053 - val_acc: 0.5436 - val_auc: 0.5740 - val_precision: 0.0516 - val_recall: 0.5455 - val_f1: 0.0912\n",
            "Epoch 42/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3827 - acc: 0.6233 - auc: 0.8587 - precision: 0.0961 - recall: 0.9125 - f1: 0.1668 - val_loss: 2.6816 - val_acc: 0.6347 - val_auc: 0.5744 - val_precision: 0.0548 - val_recall: 0.4545 - val_f1: 0.0926\n",
            "Epoch 43/200\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 1.3594 - acc: 0.6391 - auc: 0.8652 - precision: 0.0990 - recall: 0.9011 - f1: 0.1743 - val_loss: 2.7020 - val_acc: 0.6585 - val_auc: 0.5747 - val_precision: 0.0621 - val_recall: 0.4848 - val_f1: 0.1056\n",
            "Epoch 44/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3179 - acc: 0.6745 - auc: 0.8776 - precision: 0.1108 - recall: 0.9240 - f1: 0.1935 - val_loss: 2.7295 - val_acc: 0.6162 - val_auc: 0.5725 - val_precision: 0.0553 - val_recall: 0.4848 - val_f1: 0.0933\n",
            "Epoch 45/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3413 - acc: 0.6672 - auc: 0.8704 - precision: 0.1090 - recall: 0.9278 - f1: 0.1881 - val_loss: 2.7156 - val_acc: 0.5799 - val_auc: 0.5686 - val_precision: 0.0519 - val_recall: 0.5000 - val_f1: 0.0915\n",
            "Epoch 46/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3444 - acc: 0.6537 - auc: 0.8703 - precision: 0.1034 - recall: 0.9087 - f1: 0.1806 - val_loss: 2.7931 - val_acc: 0.6664 - val_auc: 0.5762 - val_precision: 0.0601 - val_recall: 0.4545 - val_f1: 0.0993\n",
            "Epoch 47/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3363 - acc: 0.6487 - auc: 0.8725 - precision: 0.1051 - recall: 0.9430 - f1: 0.1852 - val_loss: 2.7092 - val_acc: 0.6162 - val_auc: 0.5735 - val_precision: 0.0583 - val_recall: 0.5152 - val_f1: 0.1019\n",
            "Epoch 48/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3467 - acc: 0.6591 - auc: 0.8661 - precision: 0.1073 - recall: 0.9354 - f1: 0.1895 - val_loss: 2.7326 - val_acc: 0.5594 - val_auc: 0.5662 - val_precision: 0.0481 - val_recall: 0.4848 - val_f1: 0.0820\n",
            "Epoch 49/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3444 - acc: 0.6525 - auc: 0.8709 - precision: 0.1052 - recall: 0.9316 - f1: 0.1853 - val_loss: 2.7674 - val_acc: 0.6664 - val_auc: 0.5717 - val_precision: 0.0619 - val_recall: 0.4697 - val_f1: 0.1058\n",
            "Epoch 50/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3358 - acc: 0.6664 - auc: 0.8722 - precision: 0.1084 - recall: 0.9240 - f1: 0.1916 - val_loss: 2.7567 - val_acc: 0.5812 - val_auc: 0.5681 - val_precision: 0.0506 - val_recall: 0.4848 - val_f1: 0.0865\n",
            "Epoch 51/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3210 - acc: 0.6543 - auc: 0.8734 - precision: 0.1039 - recall: 0.9125 - f1: 0.1811 - val_loss: 2.6520 - val_acc: 0.5760 - val_auc: 0.5757 - val_precision: 0.0542 - val_recall: 0.5303 - val_f1: 0.0958\n",
            "Epoch 52/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3379 - acc: 0.6690 - auc: 0.8727 - precision: 0.1081 - recall: 0.9125 - f1: 0.1902 - val_loss: 2.8116 - val_acc: 0.6876 - val_auc: 0.5740 - val_precision: 0.0605 - val_recall: 0.4242 - val_f1: 0.1030\n",
            "Epoch 53/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3080 - acc: 0.6647 - auc: 0.8788 - precision: 0.1089 - recall: 0.9354 - f1: 0.1931 - val_loss: 2.7711 - val_acc: 0.5819 - val_auc: 0.5671 - val_precision: 0.0521 - val_recall: 0.5000 - val_f1: 0.0905\n",
            "Epoch 54/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.2857 - acc: 0.6609 - auc: 0.8867 - precision: 0.1078 - recall: 0.9354 - f1: 0.1886 - val_loss: 2.7937 - val_acc: 0.6229 - val_auc: 0.5704 - val_precision: 0.0547 - val_recall: 0.4697 - val_f1: 0.0942\n",
            "Epoch 55/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.3027 - acc: 0.6644 - auc: 0.8804 - precision: 0.1095 - recall: 0.9430 - f1: 0.1921 - val_loss: 2.8557 - val_acc: 0.6896 - val_auc: 0.5738 - val_precision: 0.0628 - val_recall: 0.4394 - val_f1: 0.1021\n",
            "Epoch 56/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2868 - acc: 0.6861 - auc: 0.8864 - precision: 0.1137 - recall: 0.9163 - f1: 0.1966 - val_loss: 2.8026 - val_acc: 0.5918 - val_auc: 0.5676 - val_precision: 0.0548 - val_recall: 0.5152 - val_f1: 0.0955\n",
            "Epoch 57/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2937 - acc: 0.6519 - auc: 0.8848 - precision: 0.1060 - recall: 0.9430 - f1: 0.1846 - val_loss: 2.8333 - val_acc: 0.6004 - val_auc: 0.5676 - val_precision: 0.0560 - val_recall: 0.5152 - val_f1: 0.0988\n",
            "Epoch 58/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2924 - acc: 0.6895 - auc: 0.8835 - precision: 0.1145 - recall: 0.9125 - f1: 0.1966 - val_loss: 2.8858 - val_acc: 0.6572 - val_auc: 0.5689 - val_precision: 0.0568 - val_recall: 0.4394 - val_f1: 0.0945\n",
            "Epoch 59/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3145 - acc: 0.6745 - auc: 0.8757 - precision: 0.1090 - recall: 0.9049 - f1: 0.1904 - val_loss: 2.7755 - val_acc: 0.6480 - val_auc: 0.5742 - val_precision: 0.0586 - val_recall: 0.4697 - val_f1: 0.1017\n",
            "Epoch 60/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3000 - acc: 0.6680 - auc: 0.8827 - precision: 0.1092 - recall: 0.9278 - f1: 0.1904 - val_loss: 2.8239 - val_acc: 0.6328 - val_auc: 0.5680 - val_precision: 0.0594 - val_recall: 0.5000 - val_f1: 0.1041\n",
            "Epoch 61/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3090 - acc: 0.6889 - auc: 0.8809 - precision: 0.1143 - recall: 0.9125 - f1: 0.1962 - val_loss: 2.8610 - val_acc: 0.6532 - val_auc: 0.5699 - val_precision: 0.0578 - val_recall: 0.4545 - val_f1: 0.0953\n",
            "Epoch 62/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3180 - acc: 0.6689 - auc: 0.8758 - precision: 0.1088 - recall: 0.9202 - f1: 0.1891 - val_loss: 2.7414 - val_acc: 0.5456 - val_auc: 0.5640 - val_precision: 0.0506 - val_recall: 0.5303 - val_f1: 0.0894\n",
            "Epoch 63/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2917 - acc: 0.6776 - auc: 0.8819 - precision: 0.1128 - recall: 0.9354 - f1: 0.1977 - val_loss: 2.8922 - val_acc: 0.6612 - val_auc: 0.5713 - val_precision: 0.0592 - val_recall: 0.4545 - val_f1: 0.0982\n",
            "Epoch 64/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2855 - acc: 0.6786 - auc: 0.8841 - precision: 0.1132 - recall: 0.9354 - f1: 0.1952 - val_loss: 2.8740 - val_acc: 0.6268 - val_auc: 0.5672 - val_precision: 0.0553 - val_recall: 0.4697 - val_f1: 0.0958\n",
            "Epoch 65/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3131 - acc: 0.6970 - auc: 0.8785 - precision: 0.1174 - recall: 0.9163 - f1: 0.2042 - val_loss: 2.8729 - val_acc: 0.5945 - val_auc: 0.5654 - val_precision: 0.0493 - val_recall: 0.4545 - val_f1: 0.0837\n",
            "Epoch 66/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3235 - acc: 0.6674 - auc: 0.8759 - precision: 0.1083 - recall: 0.9202 - f1: 0.1856 - val_loss: 2.9261 - val_acc: 0.6638 - val_auc: 0.5652 - val_precision: 0.0579 - val_recall: 0.4394 - val_f1: 0.0963\n",
            "Epoch 67/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2924 - acc: 0.6791 - auc: 0.8820 - precision: 0.1119 - recall: 0.9202 - f1: 0.1982 - val_loss: 2.8731 - val_acc: 0.5641 - val_auc: 0.5603 - val_precision: 0.0486 - val_recall: 0.4848 - val_f1: 0.0828\n",
            "Epoch 68/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3237 - acc: 0.6646 - auc: 0.8734 - precision: 0.1079 - recall: 0.9240 - f1: 0.1887 - val_loss: 2.9056 - val_acc: 0.6684 - val_auc: 0.5698 - val_precision: 0.0587 - val_recall: 0.4394 - val_f1: 0.0974\n",
            "Epoch 69/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3076 - acc: 0.6884 - auc: 0.8802 - precision: 0.1163 - recall: 0.9354 - f1: 0.2028 - val_loss: 2.9241 - val_acc: 0.5997 - val_auc: 0.5642 - val_precision: 0.0515 - val_recall: 0.4697 - val_f1: 0.0865\n",
            "Epoch 70/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2785 - acc: 0.6512 - auc: 0.8824 - precision: 0.1065 - recall: 0.9506 - f1: 0.1866 - val_loss: 2.8758 - val_acc: 0.6446 - val_auc: 0.5715 - val_precision: 0.0581 - val_recall: 0.4697 - val_f1: 0.1005\n",
            "Epoch 71/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2779 - acc: 0.6951 - auc: 0.8876 - precision: 0.1190 - recall: 0.9392 - f1: 0.2028 - val_loss: 2.9202 - val_acc: 0.5911 - val_auc: 0.5645 - val_precision: 0.0504 - val_recall: 0.4697 - val_f1: 0.0860\n",
            "Epoch 72/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.3064 - acc: 0.6472 - auc: 0.8805 - precision: 0.1031 - recall: 0.9240 - f1: 0.1798 - val_loss: 2.9628 - val_acc: 0.6810 - val_auc: 0.5680 - val_precision: 0.0573 - val_recall: 0.4091 - val_f1: 0.0938\n",
            "Epoch 73/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2995 - acc: 0.6763 - auc: 0.8796 - precision: 0.1124 - recall: 0.9354 - f1: 0.1954 - val_loss: 3.0020 - val_acc: 0.7028 - val_auc: 0.5683 - val_precision: 0.0636 - val_recall: 0.4242 - val_f1: 0.1036\n",
            "Epoch 74/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2704 - acc: 0.6861 - auc: 0.8882 - precision: 0.1141 - recall: 0.9202 - f1: 0.1990 - val_loss: 2.8844 - val_acc: 0.6123 - val_auc: 0.5700 - val_precision: 0.0516 - val_recall: 0.4545 - val_f1: 0.0903\n",
            "Epoch 75/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2498 - acc: 0.6854 - auc: 0.8937 - precision: 0.1161 - recall: 0.9430 - f1: 0.2012 - val_loss: 2.9225 - val_acc: 0.6750 - val_auc: 0.5734 - val_precision: 0.0653 - val_recall: 0.4848 - val_f1: 0.1111\n",
            "Epoch 76/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2828 - acc: 0.6870 - auc: 0.8831 - precision: 0.1137 - recall: 0.9125 - f1: 0.1949 - val_loss: 2.9351 - val_acc: 0.5878 - val_auc: 0.5635 - val_precision: 0.0514 - val_recall: 0.4848 - val_f1: 0.0877\n",
            "Epoch 77/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2780 - acc: 0.6770 - auc: 0.8864 - precision: 0.1119 - recall: 0.9278 - f1: 0.1932 - val_loss: 3.0122 - val_acc: 0.6717 - val_auc: 0.5654 - val_precision: 0.0575 - val_recall: 0.4242 - val_f1: 0.0954\n",
            "Epoch 78/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2868 - acc: 0.6920 - auc: 0.8838 - precision: 0.1172 - recall: 0.9316 - f1: 0.2051 - val_loss: 3.0032 - val_acc: 0.6083 - val_auc: 0.5625 - val_precision: 0.0541 - val_recall: 0.4848 - val_f1: 0.0942\n",
            "Epoch 79/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.3046 - acc: 0.6717 - auc: 0.8792 - precision: 0.1103 - recall: 0.9278 - f1: 0.1926 - val_loss: 2.9594 - val_acc: 0.6612 - val_auc: 0.5689 - val_precision: 0.0609 - val_recall: 0.4697 - val_f1: 0.1046\n",
            "Epoch 80/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2717 - acc: 0.6857 - auc: 0.8905 - precision: 0.1154 - recall: 0.9354 - f1: 0.2027 - val_loss: 3.0240 - val_acc: 0.6169 - val_auc: 0.5629 - val_precision: 0.0538 - val_recall: 0.4697 - val_f1: 0.0905\n",
            "Epoch 81/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2769 - acc: 0.7090 - auc: 0.8875 - precision: 0.1236 - recall: 0.9354 - f1: 0.2080 - val_loss: 3.0177 - val_acc: 0.6460 - val_auc: 0.5671 - val_precision: 0.0583 - val_recall: 0.4697 - val_f1: 0.0985\n",
            "Epoch 82/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2916 - acc: 0.6882 - auc: 0.8822 - precision: 0.1152 - recall: 0.9240 - f1: 0.1977 - val_loss: 2.9293 - val_acc: 0.5958 - val_auc: 0.5673 - val_precision: 0.0510 - val_recall: 0.4697 - val_f1: 0.0871\n",
            "Epoch 83/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2869 - acc: 0.6786 - auc: 0.8850 - precision: 0.1124 - recall: 0.9278 - f1: 0.1941 - val_loss: 2.8612 - val_acc: 0.5608 - val_auc: 0.5622 - val_precision: 0.0510 - val_recall: 0.5152 - val_f1: 0.0903\n",
            "Epoch 84/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2778 - acc: 0.6975 - auc: 0.8864 - precision: 0.1176 - recall: 0.9163 - f1: 0.2035 - val_loss: 3.0229 - val_acc: 0.5991 - val_auc: 0.5608 - val_precision: 0.0514 - val_recall: 0.4697 - val_f1: 0.0874\n",
            "Epoch 85/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2641 - acc: 0.6717 - auc: 0.8899 - precision: 0.1103 - recall: 0.9278 - f1: 0.1908 - val_loss: 3.1191 - val_acc: 0.6229 - val_auc: 0.5583 - val_precision: 0.0483 - val_recall: 0.4091 - val_f1: 0.0806\n",
            "Epoch 86/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2325 - acc: 0.7027 - auc: 0.8974 - precision: 0.1209 - recall: 0.9316 - f1: 0.2086 - val_loss: 3.0538 - val_acc: 0.6295 - val_auc: 0.5595 - val_precision: 0.0524 - val_recall: 0.4394 - val_f1: 0.0871\n",
            "Epoch 87/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2597 - acc: 0.6727 - auc: 0.8899 - precision: 0.1117 - recall: 0.9392 - f1: 0.1912 - val_loss: 3.1698 - val_acc: 0.6929 - val_auc: 0.5622 - val_precision: 0.0596 - val_recall: 0.4091 - val_f1: 0.0962\n",
            "Epoch 88/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2535 - acc: 0.7054 - auc: 0.8925 - precision: 0.1204 - recall: 0.9163 - f1: 0.2094 - val_loss: 3.1656 - val_acc: 0.7213 - val_auc: 0.5658 - val_precision: 0.0594 - val_recall: 0.3636 - val_f1: 0.0972\n",
            "Epoch 89/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2492 - acc: 0.6910 - auc: 0.8925 - precision: 0.1176 - recall: 0.9392 - f1: 0.2017 - val_loss: 3.0969 - val_acc: 0.6559 - val_auc: 0.5611 - val_precision: 0.0530 - val_recall: 0.4091 - val_f1: 0.0893\n",
            "Epoch 90/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2791 - acc: 0.6854 - auc: 0.8849 - precision: 0.1150 - recall: 0.9316 - f1: 0.1986 - val_loss: 3.1210 - val_acc: 0.6948 - val_auc: 0.5634 - val_precision: 0.0580 - val_recall: 0.3939 - val_f1: 0.0947\n",
            "Epoch 91/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2413 - acc: 0.7115 - auc: 0.8957 - precision: 0.1234 - recall: 0.9240 - f1: 0.2110 - val_loss: 3.0143 - val_acc: 0.5958 - val_auc: 0.5616 - val_precision: 0.0525 - val_recall: 0.4848 - val_f1: 0.0925\n",
            "Epoch 92/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2751 - acc: 0.6839 - auc: 0.8864 - precision: 0.1159 - recall: 0.9468 - f1: 0.2039 - val_loss: 3.0824 - val_acc: 0.5865 - val_auc: 0.5581 - val_precision: 0.0498 - val_recall: 0.4697 - val_f1: 0.0836\n",
            "Epoch 93/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2383 - acc: 0.7151 - auc: 0.8979 - precision: 0.1256 - recall: 0.9316 - f1: 0.2148 - val_loss: 3.0997 - val_acc: 0.6526 - val_auc: 0.5643 - val_precision: 0.0560 - val_recall: 0.4394 - val_f1: 0.0929\n",
            "Epoch 94/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2445 - acc: 0.7082 - auc: 0.8944 - precision: 0.1210 - recall: 0.9125 - f1: 0.2073 - val_loss: 2.9777 - val_acc: 0.5793 - val_auc: 0.5605 - val_precision: 0.0518 - val_recall: 0.5000 - val_f1: 0.0911\n",
            "Epoch 95/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2456 - acc: 0.6880 - auc: 0.8944 - precision: 0.1158 - recall: 0.9316 - f1: 0.2010 - val_loss: 3.1644 - val_acc: 0.6770 - val_auc: 0.5587 - val_precision: 0.0566 - val_recall: 0.4091 - val_f1: 0.0949\n",
            "Epoch 96/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2749 - acc: 0.6928 - auc: 0.8864 - precision: 0.1171 - recall: 0.9278 - f1: 0.2031 - val_loss: 3.1152 - val_acc: 0.6004 - val_auc: 0.5512 - val_precision: 0.0486 - val_recall: 0.4394 - val_f1: 0.0823\n",
            "Epoch 97/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2404 - acc: 0.6872 - auc: 0.8949 - precision: 0.1156 - recall: 0.9316 - f1: 0.1963 - val_loss: 3.1919 - val_acc: 0.6757 - val_auc: 0.5564 - val_precision: 0.0545 - val_recall: 0.3939 - val_f1: 0.0887\n",
            "Epoch 98/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2354 - acc: 0.6963 - auc: 0.8979 - precision: 0.1205 - recall: 0.9506 - f1: 0.2053 - val_loss: 3.1293 - val_acc: 0.6433 - val_auc: 0.5586 - val_precision: 0.0545 - val_recall: 0.4394 - val_f1: 0.0913\n",
            "Epoch 99/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2427 - acc: 0.7098 - auc: 0.8942 - precision: 0.1251 - recall: 0.9468 - f1: 0.2179 - val_loss: 3.0868 - val_acc: 0.6169 - val_auc: 0.5583 - val_precision: 0.0507 - val_recall: 0.4394 - val_f1: 0.0867\n",
            "Epoch 100/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2921 - acc: 0.7041 - auc: 0.8846 - precision: 0.1199 - recall: 0.9163 - f1: 0.2060 - val_loss: 3.0533 - val_acc: 0.6499 - val_auc: 0.5614 - val_precision: 0.0573 - val_recall: 0.4545 - val_f1: 0.1003\n",
            "Epoch 101/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2447 - acc: 0.6913 - auc: 0.8956 - precision: 0.1177 - recall: 0.9392 - f1: 0.2080 - val_loss: 3.1510 - val_acc: 0.7371 - val_auc: 0.5699 - val_precision: 0.0632 - val_recall: 0.3636 - val_f1: 0.1018\n",
            "Epoch 102/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2454 - acc: 0.7031 - auc: 0.8928 - precision: 0.1210 - recall: 0.9316 - f1: 0.2087 - val_loss: 3.1834 - val_acc: 0.6704 - val_auc: 0.5603 - val_precision: 0.0536 - val_recall: 0.3939 - val_f1: 0.0888\n",
            "Epoch 103/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2253 - acc: 0.7094 - auc: 0.8982 - precision: 0.1230 - recall: 0.9278 - f1: 0.2133 - val_loss: 3.1346 - val_acc: 0.6565 - val_auc: 0.5631 - val_precision: 0.0549 - val_recall: 0.4242 - val_f1: 0.0922\n",
            "Epoch 104/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2151 - acc: 0.6857 - auc: 0.9019 - precision: 0.1172 - recall: 0.9544 - f1: 0.2042 - val_loss: 3.2794 - val_acc: 0.7384 - val_auc: 0.5652 - val_precision: 0.0565 - val_recall: 0.3182 - val_f1: 0.0900\n",
            "Epoch 105/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.2439 - acc: 0.7056 - auc: 0.8922 - precision: 0.1227 - recall: 0.9392 - f1: 0.2148 - val_loss: 3.2890 - val_acc: 0.6975 - val_auc: 0.5599 - val_precision: 0.0586 - val_recall: 0.3939 - val_f1: 0.0945\n",
            "Epoch 106/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2420 - acc: 0.6984 - auc: 0.8932 - precision: 0.1190 - recall: 0.9278 - f1: 0.2038 - val_loss: 3.1283 - val_acc: 0.6229 - val_auc: 0.5604 - val_precision: 0.0515 - val_recall: 0.4394 - val_f1: 0.0872\n",
            "Epoch 107/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2318 - acc: 0.6993 - auc: 0.8969 - precision: 0.1193 - recall: 0.9278 - f1: 0.2066 - val_loss: 3.3071 - val_acc: 0.7299 - val_auc: 0.5630 - val_precision: 0.0614 - val_recall: 0.3636 - val_f1: 0.0989\n",
            "Epoch 108/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1946 - acc: 0.7234 - auc: 0.9061 - precision: 0.1293 - recall: 0.9354 - f1: 0.2193 - val_loss: 3.3381 - val_acc: 0.6466 - val_auc: 0.5547 - val_precision: 0.0516 - val_recall: 0.4091 - val_f1: 0.0854\n",
            "Epoch 109/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2061 - acc: 0.7242 - auc: 0.9062 - precision: 0.1296 - recall: 0.9354 - f1: 0.2209 - val_loss: 3.1036 - val_acc: 0.6143 - val_auc: 0.5596 - val_precision: 0.0534 - val_recall: 0.4697 - val_f1: 0.0933\n",
            "Epoch 110/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2116 - acc: 0.7084 - auc: 0.9019 - precision: 0.1230 - recall: 0.9316 - f1: 0.2104 - val_loss: 3.1495 - val_acc: 0.6268 - val_auc: 0.5627 - val_precision: 0.0553 - val_recall: 0.4697 - val_f1: 0.0954\n",
            "Epoch 111/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2295 - acc: 0.7237 - auc: 0.8986 - precision: 0.1278 - recall: 0.9202 - f1: 0.2160 - val_loss: 3.1460 - val_acc: 0.6790 - val_auc: 0.5643 - val_precision: 0.0570 - val_recall: 0.4091 - val_f1: 0.0976\n",
            "Epoch 112/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2040 - acc: 0.7184 - auc: 0.9046 - precision: 0.1276 - recall: 0.9392 - f1: 0.2190 - val_loss: 3.3023 - val_acc: 0.6328 - val_auc: 0.5559 - val_precision: 0.0496 - val_recall: 0.4091 - val_f1: 0.0820\n",
            "Epoch 113/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2484 - acc: 0.6996 - auc: 0.8938 - precision: 0.1205 - recall: 0.9392 - f1: 0.2102 - val_loss: 3.2491 - val_acc: 0.7173 - val_auc: 0.5616 - val_precision: 0.0607 - val_recall: 0.3788 - val_f1: 0.0992\n",
            "Epoch 114/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2092 - acc: 0.7247 - auc: 0.9021 - precision: 0.1290 - recall: 0.9278 - f1: 0.2218 - val_loss: 3.2172 - val_acc: 0.6123 - val_auc: 0.5569 - val_precision: 0.0501 - val_recall: 0.4394 - val_f1: 0.0845\n",
            "Epoch 115/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1981 - acc: 0.7095 - auc: 0.9064 - precision: 0.1242 - recall: 0.9392 - f1: 0.2158 - val_loss: 3.3746 - val_acc: 0.6902 - val_auc: 0.5537 - val_precision: 0.0552 - val_recall: 0.3788 - val_f1: 0.0912\n",
            "Epoch 116/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2159 - acc: 0.7201 - auc: 0.9012 - precision: 0.1275 - recall: 0.9316 - f1: 0.2189 - val_loss: 3.1854 - val_acc: 0.5931 - val_auc: 0.5561 - val_precision: 0.0492 - val_recall: 0.4545 - val_f1: 0.0839\n",
            "Epoch 117/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2456 - acc: 0.6978 - auc: 0.8933 - precision: 0.1195 - recall: 0.9354 - f1: 0.2047 - val_loss: 3.2848 - val_acc: 0.6321 - val_auc: 0.5574 - val_precision: 0.0495 - val_recall: 0.4091 - val_f1: 0.0824\n",
            "Epoch 118/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2175 - acc: 0.7161 - auc: 0.8999 - precision: 0.1260 - recall: 0.9316 - f1: 0.2219 - val_loss: 3.3462 - val_acc: 0.7021 - val_auc: 0.5568 - val_precision: 0.0575 - val_recall: 0.3788 - val_f1: 0.0923\n",
            "Epoch 119/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2341 - acc: 0.6937 - auc: 0.8938 - precision: 0.1185 - recall: 0.9392 - f1: 0.2104 - val_loss: 3.1442 - val_acc: 0.6764 - val_auc: 0.5662 - val_precision: 0.0602 - val_recall: 0.4394 - val_f1: 0.1031\n",
            "Epoch 120/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2192 - acc: 0.7123 - auc: 0.8987 - precision: 0.1245 - recall: 0.9316 - f1: 0.2116 - val_loss: 3.3062 - val_acc: 0.6948 - val_auc: 0.5627 - val_precision: 0.0600 - val_recall: 0.4091 - val_f1: 0.0996\n",
            "Epoch 121/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2246 - acc: 0.7031 - auc: 0.8964 - precision: 0.1214 - recall: 0.9354 - f1: 0.2098 - val_loss: 3.2966 - val_acc: 0.6929 - val_auc: 0.5625 - val_precision: 0.0576 - val_recall: 0.3939 - val_f1: 0.0954\n",
            "Epoch 122/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1818 - acc: 0.7226 - auc: 0.9064 - precision: 0.1293 - recall: 0.9392 - f1: 0.2199 - val_loss: 3.1662 - val_acc: 0.6209 - val_auc: 0.5615 - val_precision: 0.0512 - val_recall: 0.4394 - val_f1: 0.0870\n",
            "Epoch 123/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1858 - acc: 0.7204 - auc: 0.9067 - precision: 0.1281 - recall: 0.9354 - f1: 0.2164 - val_loss: 3.4535 - val_acc: 0.7299 - val_auc: 0.5624 - val_precision: 0.0614 - val_recall: 0.3636 - val_f1: 0.0975\n",
            "Epoch 124/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1941 - acc: 0.7075 - auc: 0.9026 - precision: 0.1231 - recall: 0.9354 - f1: 0.2092 - val_loss: 3.2472 - val_acc: 0.6156 - val_auc: 0.5612 - val_precision: 0.0536 - val_recall: 0.4697 - val_f1: 0.0908\n",
            "Epoch 125/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1961 - acc: 0.7089 - auc: 0.9046 - precision: 0.1236 - recall: 0.9354 - f1: 0.2114 - val_loss: 3.3224 - val_acc: 0.6136 - val_auc: 0.5533 - val_precision: 0.0487 - val_recall: 0.4242 - val_f1: 0.0809\n",
            "Epoch 126/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2393 - acc: 0.7059 - auc: 0.8937 - precision: 0.1224 - recall: 0.9354 - f1: 0.2050 - val_loss: 3.3334 - val_acc: 0.6083 - val_auc: 0.5527 - val_precision: 0.0480 - val_recall: 0.4242 - val_f1: 0.0814\n",
            "Epoch 127/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1862 - acc: 0.7125 - auc: 0.9055 - precision: 0.1253 - recall: 0.9392 - f1: 0.2167 - val_loss: 3.2316 - val_acc: 0.6664 - val_auc: 0.5615 - val_precision: 0.0566 - val_recall: 0.4242 - val_f1: 0.0988\n",
            "Epoch 128/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2377 - acc: 0.7094 - auc: 0.8957 - precision: 0.1237 - recall: 0.9354 - f1: 0.2138 - val_loss: 3.2418 - val_acc: 0.6770 - val_auc: 0.5597 - val_precision: 0.0566 - val_recall: 0.4091 - val_f1: 0.0997\n",
            "Epoch 129/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.2117 - acc: 0.7229 - auc: 0.9016 - precision: 0.1271 - recall: 0.9163 - f1: 0.2185 - val_loss: 3.4120 - val_acc: 0.6453 - val_auc: 0.5561 - val_precision: 0.0514 - val_recall: 0.4091 - val_f1: 0.0850\n",
            "Epoch 130/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1897 - acc: 0.7082 - auc: 0.9060 - precision: 0.1229 - recall: 0.9316 - f1: 0.2082 - val_loss: 3.4366 - val_acc: 0.6506 - val_auc: 0.5512 - val_precision: 0.0522 - val_recall: 0.4091 - val_f1: 0.0856\n",
            "Epoch 131/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2154 - acc: 0.7123 - auc: 0.9008 - precision: 0.1268 - recall: 0.9544 - f1: 0.2214 - val_loss: 3.2745 - val_acc: 0.6143 - val_auc: 0.5529 - val_precision: 0.0519 - val_recall: 0.4545 - val_f1: 0.0887\n",
            "Epoch 132/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2230 - acc: 0.6943 - auc: 0.8968 - precision: 0.1176 - recall: 0.9278 - f1: 0.2047 - val_loss: 3.3480 - val_acc: 0.6030 - val_auc: 0.5478 - val_precision: 0.0489 - val_recall: 0.4394 - val_f1: 0.0826\n",
            "Epoch 133/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1909 - acc: 0.7284 - auc: 0.9085 - precision: 0.1309 - recall: 0.9316 - f1: 0.2238 - val_loss: 3.5596 - val_acc: 0.7061 - val_auc: 0.5512 - val_precision: 0.0583 - val_recall: 0.3788 - val_f1: 0.0951\n",
            "Epoch 134/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1868 - acc: 0.7011 - auc: 0.9070 - precision: 0.1229 - recall: 0.9582 - f1: 0.2109 - val_loss: 3.4458 - val_acc: 0.6618 - val_auc: 0.5504 - val_precision: 0.0504 - val_recall: 0.3788 - val_f1: 0.0841\n",
            "Epoch 135/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1550 - acc: 0.7454 - auc: 0.9157 - precision: 0.1418 - recall: 0.9620 - f1: 0.2410 - val_loss: 3.3646 - val_acc: 0.6466 - val_auc: 0.5545 - val_precision: 0.0533 - val_recall: 0.4242 - val_f1: 0.0900\n",
            "Epoch 136/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.2124 - acc: 0.7127 - auc: 0.9006 - precision: 0.1265 - recall: 0.9506 - f1: 0.2152 - val_loss: 3.4200 - val_acc: 0.6565 - val_auc: 0.5547 - val_precision: 0.0549 - val_recall: 0.4242 - val_f1: 0.0915\n",
            "Epoch 137/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1768 - acc: 0.7241 - auc: 0.9087 - precision: 0.1303 - recall: 0.9430 - f1: 0.2239 - val_loss: 3.4282 - val_acc: 0.6803 - val_auc: 0.5576 - val_precision: 0.0553 - val_recall: 0.3939 - val_f1: 0.0910\n",
            "Epoch 138/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1719 - acc: 0.7388 - auc: 0.9093 - precision: 0.1367 - recall: 0.9430 - f1: 0.2319 - val_loss: 3.3552 - val_acc: 0.5799 - val_auc: 0.5558 - val_precision: 0.0505 - val_recall: 0.4848 - val_f1: 0.0851\n",
            "Epoch 139/200\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 1.1696 - acc: 0.7100 - auc: 0.9081 - precision: 0.1255 - recall: 0.9506 - f1: 0.2172 - val_loss: 3.3905 - val_acc: 0.6598 - val_auc: 0.5602 - val_precision: 0.0554 - val_recall: 0.4242 - val_f1: 0.0968\n",
            "Epoch 140/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1431 - acc: 0.7308 - auc: 0.9161 - precision: 0.1336 - recall: 0.9468 - f1: 0.2282 - val_loss: 3.4068 - val_acc: 0.6361 - val_auc: 0.5570 - val_precision: 0.0550 - val_recall: 0.4545 - val_f1: 0.0944\n",
            "Epoch 141/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1866 - acc: 0.7312 - auc: 0.9056 - precision: 0.1317 - recall: 0.9278 - f1: 0.2196 - val_loss: 3.4417 - val_acc: 0.6618 - val_auc: 0.5547 - val_precision: 0.0593 - val_recall: 0.4545 - val_f1: 0.0986\n",
            "Epoch 142/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1175 - acc: 0.7450 - auc: 0.9207 - precision: 0.1400 - recall: 0.9468 - f1: 0.2364 - val_loss: 3.5328 - val_acc: 0.6592 - val_auc: 0.5535 - val_precision: 0.0536 - val_recall: 0.4091 - val_f1: 0.0890\n",
            "Epoch 143/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1833 - acc: 0.7234 - auc: 0.9060 - precision: 0.1297 - recall: 0.9392 - f1: 0.2220 - val_loss: 3.5721 - val_acc: 0.6717 - val_auc: 0.5518 - val_precision: 0.0538 - val_recall: 0.3939 - val_f1: 0.0897\n",
            "Epoch 144/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1707 - acc: 0.7290 - auc: 0.9097 - precision: 0.1328 - recall: 0.9468 - f1: 0.2270 - val_loss: 3.6983 - val_acc: 0.7517 - val_auc: 0.5532 - val_precision: 0.0546 - val_recall: 0.2879 - val_f1: 0.0816\n",
            "Epoch 145/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1607 - acc: 0.7345 - auc: 0.9133 - precision: 0.1352 - recall: 0.9468 - f1: 0.2287 - val_loss: 3.5511 - val_acc: 0.6215 - val_auc: 0.5459 - val_precision: 0.0497 - val_recall: 0.4242 - val_f1: 0.0825\n",
            "Epoch 146/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1834 - acc: 0.7260 - auc: 0.9066 - precision: 0.1292 - recall: 0.9240 - f1: 0.2211 - val_loss: 3.5325 - val_acc: 0.7133 - val_auc: 0.5569 - val_precision: 0.0577 - val_recall: 0.3636 - val_f1: 0.0957\n",
            "Epoch 147/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.2087 - acc: 0.7120 - auc: 0.9021 - precision: 0.1221 - recall: 0.9087 - f1: 0.2102 - val_loss: 3.5518 - val_acc: 0.6367 - val_auc: 0.5528 - val_precision: 0.0519 - val_recall: 0.4242 - val_f1: 0.0857\n",
            "Epoch 148/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1646 - acc: 0.7264 - auc: 0.9081 - precision: 0.1328 - recall: 0.9582 - f1: 0.2277 - val_loss: 3.5111 - val_acc: 0.6380 - val_auc: 0.5521 - val_precision: 0.0520 - val_recall: 0.4242 - val_f1: 0.0869\n",
            "Epoch 149/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1403 - acc: 0.7335 - auc: 0.9171 - precision: 0.1343 - recall: 0.9430 - f1: 0.2248 - val_loss: 3.5533 - val_acc: 0.6433 - val_auc: 0.5508 - val_precision: 0.0528 - val_recall: 0.4242 - val_f1: 0.0871\n",
            "Epoch 150/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1428 - acc: 0.7503 - auc: 0.9154 - precision: 0.1422 - recall: 0.9430 - f1: 0.2418 - val_loss: 3.7306 - val_acc: 0.7213 - val_auc: 0.5488 - val_precision: 0.0637 - val_recall: 0.3939 - val_f1: 0.1017\n",
            "Epoch 151/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1571 - acc: 0.7358 - auc: 0.9124 - precision: 0.1354 - recall: 0.9430 - f1: 0.2320 - val_loss: 3.5187 - val_acc: 0.6460 - val_auc: 0.5548 - val_precision: 0.0532 - val_recall: 0.4242 - val_f1: 0.0898\n",
            "Epoch 152/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1100 - acc: 0.7593 - auc: 0.9229 - precision: 0.1451 - recall: 0.9278 - f1: 0.2388 - val_loss: 3.7728 - val_acc: 0.7517 - val_auc: 0.5524 - val_precision: 0.0621 - val_recall: 0.3333 - val_f1: 0.0932\n",
            "Epoch 153/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1208 - acc: 0.7607 - auc: 0.9199 - precision: 0.1488 - recall: 0.9544 - f1: 0.2441 - val_loss: 3.6261 - val_acc: 0.7259 - val_auc: 0.5603 - val_precision: 0.0560 - val_recall: 0.3333 - val_f1: 0.0892\n",
            "Epoch 154/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1773 - acc: 0.7312 - auc: 0.9038 - precision: 0.1333 - recall: 0.9430 - f1: 0.2332 - val_loss: 3.6460 - val_acc: 0.6836 - val_auc: 0.5500 - val_precision: 0.0540 - val_recall: 0.3788 - val_f1: 0.0895\n",
            "Epoch 155/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1569 - acc: 0.7265 - auc: 0.9127 - precision: 0.1314 - recall: 0.9430 - f1: 0.2251 - val_loss: 3.5209 - val_acc: 0.6387 - val_auc: 0.5537 - val_precision: 0.0538 - val_recall: 0.4394 - val_f1: 0.0906\n",
            "Epoch 156/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1642 - acc: 0.7483 - auc: 0.9126 - precision: 0.1429 - recall: 0.9582 - f1: 0.2390 - val_loss: 3.7954 - val_acc: 0.7028 - val_auc: 0.5470 - val_precision: 0.0576 - val_recall: 0.3788 - val_f1: 0.0927\n",
            "Epoch 157/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1443 - acc: 0.7302 - auc: 0.9146 - precision: 0.1329 - recall: 0.9430 - f1: 0.2272 - val_loss: 3.8316 - val_acc: 0.7543 - val_auc: 0.5533 - val_precision: 0.0578 - val_recall: 0.3030 - val_f1: 0.0872\n",
            "Epoch 158/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1455 - acc: 0.7406 - auc: 0.9165 - precision: 0.1375 - recall: 0.9430 - f1: 0.2268 - val_loss: 3.6884 - val_acc: 0.7001 - val_auc: 0.5567 - val_precision: 0.0611 - val_recall: 0.4091 - val_f1: 0.0989\n",
            "Epoch 159/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1211 - acc: 0.7396 - auc: 0.9209 - precision: 0.1383 - recall: 0.9544 - f1: 0.2365 - val_loss: 3.6504 - val_acc: 0.6625 - val_auc: 0.5554 - val_precision: 0.0594 - val_recall: 0.4545 - val_f1: 0.0987\n",
            "Epoch 160/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1260 - acc: 0.7492 - auc: 0.9196 - precision: 0.1425 - recall: 0.9506 - f1: 0.2411 - val_loss: 3.7797 - val_acc: 0.7411 - val_auc: 0.5554 - val_precision: 0.0595 - val_recall: 0.3333 - val_f1: 0.0943\n",
            "Epoch 161/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1685 - acc: 0.7346 - auc: 0.9115 - precision: 0.1333 - recall: 0.9278 - f1: 0.2210 - val_loss: 3.7408 - val_acc: 0.6592 - val_auc: 0.5503 - val_precision: 0.0571 - val_recall: 0.4394 - val_f1: 0.0942\n",
            "Epoch 162/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1371 - acc: 0.7441 - auc: 0.9154 - precision: 0.1400 - recall: 0.9506 - f1: 0.2393 - val_loss: 3.5234 - val_acc: 0.5938 - val_auc: 0.5529 - val_precision: 0.0507 - val_recall: 0.4697 - val_f1: 0.0891\n",
            "Epoch 163/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1166 - acc: 0.7441 - auc: 0.9198 - precision: 0.1416 - recall: 0.9658 - f1: 0.2372 - val_loss: 3.8093 - val_acc: 0.7517 - val_auc: 0.5579 - val_precision: 0.0597 - val_recall: 0.3182 - val_f1: 0.0918\n",
            "Epoch 164/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1859 - acc: 0.7497 - auc: 0.9059 - precision: 0.1398 - recall: 0.9240 - f1: 0.2385 - val_loss: 3.4599 - val_acc: 0.5720 - val_auc: 0.5551 - val_precision: 0.0495 - val_recall: 0.4848 - val_f1: 0.0872\n",
            "Epoch 165/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1143 - acc: 0.7231 - auc: 0.9199 - precision: 0.1334 - recall: 0.9772 - f1: 0.2271 - val_loss: 3.7961 - val_acc: 0.6486 - val_auc: 0.5470 - val_precision: 0.0519 - val_recall: 0.4091 - val_f1: 0.0855\n",
            "Epoch 166/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1524 - acc: 0.7310 - auc: 0.9131 - precision: 0.1317 - recall: 0.9278 - f1: 0.2238 - val_loss: 3.8500 - val_acc: 0.7199 - val_auc: 0.5541 - val_precision: 0.0634 - val_recall: 0.3939 - val_f1: 0.1036\n",
            "Epoch 167/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1714 - acc: 0.7515 - auc: 0.9094 - precision: 0.1386 - recall: 0.9049 - f1: 0.2310 - val_loss: 3.6317 - val_acc: 0.6882 - val_auc: 0.5528 - val_precision: 0.0568 - val_recall: 0.3939 - val_f1: 0.0931\n",
            "Epoch 168/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1440 - acc: 0.7348 - auc: 0.9138 - precision: 0.1357 - recall: 0.9506 - f1: 0.2322 - val_loss: 3.6895 - val_acc: 0.6658 - val_auc: 0.5518 - val_precision: 0.0547 - val_recall: 0.4091 - val_f1: 0.0917\n",
            "Epoch 169/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1382 - acc: 0.7356 - auc: 0.9176 - precision: 0.1357 - recall: 0.9468 - f1: 0.2312 - val_loss: 3.7034 - val_acc: 0.6559 - val_auc: 0.5463 - val_precision: 0.0548 - val_recall: 0.4242 - val_f1: 0.0911\n",
            "Epoch 170/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1281 - acc: 0.7447 - auc: 0.9173 - precision: 0.1403 - recall: 0.9506 - f1: 0.2339 - val_loss: 3.7810 - val_acc: 0.7332 - val_auc: 0.5538 - val_precision: 0.0622 - val_recall: 0.3636 - val_f1: 0.0995\n",
            "Epoch 171/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1160 - acc: 0.7617 - auc: 0.9213 - precision: 0.1481 - recall: 0.9430 - f1: 0.2521 - val_loss: 4.0510 - val_acc: 0.7563 - val_auc: 0.5485 - val_precision: 0.0634 - val_recall: 0.3333 - val_f1: 0.0975\n",
            "Epoch 172/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1052 - acc: 0.7426 - auc: 0.9228 - precision: 0.1405 - recall: 0.9620 - f1: 0.2376 - val_loss: 3.9721 - val_acc: 0.6658 - val_auc: 0.5403 - val_precision: 0.0528 - val_recall: 0.3939 - val_f1: 0.0859\n",
            "Epoch 173/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1393 - acc: 0.7469 - auc: 0.9147 - precision: 0.1405 - recall: 0.9430 - f1: 0.2354 - val_loss: 3.6651 - val_acc: 0.6162 - val_auc: 0.5475 - val_precision: 0.0522 - val_recall: 0.4545 - val_f1: 0.0880\n",
            "Epoch 174/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0913 - acc: 0.7419 - auc: 0.9250 - precision: 0.1398 - recall: 0.9582 - f1: 0.2416 - val_loss: 3.8508 - val_acc: 0.7365 - val_auc: 0.5535 - val_precision: 0.0607 - val_recall: 0.3485 - val_f1: 0.0940\n",
            "Epoch 175/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1119 - acc: 0.7761 - auc: 0.9226 - precision: 0.1562 - recall: 0.9430 - f1: 0.2617 - val_loss: 3.8771 - val_acc: 0.7133 - val_auc: 0.5505 - val_precision: 0.0577 - val_recall: 0.3636 - val_f1: 0.0943\n",
            "Epoch 176/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1168 - acc: 0.7576 - auc: 0.9204 - precision: 0.1463 - recall: 0.9468 - f1: 0.2442 - val_loss: 3.5483 - val_acc: 0.6044 - val_auc: 0.5497 - val_precision: 0.0521 - val_recall: 0.4697 - val_f1: 0.0927\n",
            "Epoch 177/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1133 - acc: 0.7449 - auc: 0.9205 - precision: 0.1404 - recall: 0.9506 - f1: 0.2373 - val_loss: 4.0341 - val_acc: 0.7563 - val_auc: 0.5453 - val_precision: 0.0583 - val_recall: 0.3030 - val_f1: 0.0909\n",
            "Epoch 178/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1481 - acc: 0.7363 - auc: 0.9127 - precision: 0.1360 - recall: 0.9468 - f1: 0.2327 - val_loss: 3.9151 - val_acc: 0.7008 - val_auc: 0.5480 - val_precision: 0.0552 - val_recall: 0.3636 - val_f1: 0.0892\n",
            "Epoch 179/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1313 - acc: 0.7500 - auc: 0.9184 - precision: 0.1412 - recall: 0.9354 - f1: 0.2395 - val_loss: 3.9752 - val_acc: 0.6962 - val_auc: 0.5450 - val_precision: 0.0563 - val_recall: 0.3788 - val_f1: 0.0898\n",
            "Epoch 180/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1337 - acc: 0.7452 - auc: 0.9161 - precision: 0.1409 - recall: 0.9544 - f1: 0.2345 - val_loss: 3.7394 - val_acc: 0.7048 - val_auc: 0.5551 - val_precision: 0.0621 - val_recall: 0.4091 - val_f1: 0.1033\n",
            "Epoch 181/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1284 - acc: 0.7518 - auc: 0.9169 - precision: 0.1417 - recall: 0.9316 - f1: 0.2379 - val_loss: 3.6234 - val_acc: 0.6235 - val_auc: 0.5561 - val_precision: 0.0548 - val_recall: 0.4697 - val_f1: 0.0965\n",
            "Epoch 182/200\n",
            "95/95 [==============================] - 1s 16ms/step - loss: 1.1054 - acc: 0.7310 - auc: 0.9219 - precision: 0.1348 - recall: 0.9582 - f1: 0.2346 - val_loss: 4.1007 - val_acc: 0.7675 - val_auc: 0.5501 - val_precision: 0.0613 - val_recall: 0.3030 - val_f1: 0.0946\n",
            "Epoch 183/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1252 - acc: 0.7437 - auc: 0.9185 - precision: 0.1390 - recall: 0.9430 - f1: 0.2315 - val_loss: 3.8303 - val_acc: 0.6513 - val_auc: 0.5438 - val_precision: 0.0558 - val_recall: 0.4394 - val_f1: 0.0917\n",
            "Epoch 184/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1276 - acc: 0.7512 - auc: 0.9171 - precision: 0.1414 - recall: 0.9316 - f1: 0.2367 - val_loss: 3.8461 - val_acc: 0.6400 - val_auc: 0.5403 - val_precision: 0.0523 - val_recall: 0.4242 - val_f1: 0.0877\n",
            "Epoch 185/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0938 - acc: 0.7355 - auc: 0.9215 - precision: 0.1380 - recall: 0.9696 - f1: 0.2364 - val_loss: 3.9184 - val_acc: 0.7213 - val_auc: 0.5460 - val_precision: 0.0572 - val_recall: 0.3485 - val_f1: 0.0933\n",
            "Epoch 186/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0963 - acc: 0.7650 - auc: 0.9246 - precision: 0.1515 - recall: 0.9582 - f1: 0.2524 - val_loss: 3.8723 - val_acc: 0.6559 - val_auc: 0.5466 - val_precision: 0.0548 - val_recall: 0.4242 - val_f1: 0.0897\n",
            "Epoch 187/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0701 - acc: 0.7521 - auc: 0.9284 - precision: 0.1439 - recall: 0.9506 - f1: 0.2377 - val_loss: 3.7709 - val_acc: 0.6843 - val_auc: 0.5571 - val_precision: 0.0560 - val_recall: 0.3939 - val_f1: 0.0953\n",
            "Epoch 188/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.0802 - acc: 0.7642 - auc: 0.9278 - precision: 0.1494 - recall: 0.9430 - f1: 0.2506 - val_loss: 4.3039 - val_acc: 0.7985 - val_auc: 0.5543 - val_precision: 0.0655 - val_recall: 0.2727 - val_f1: 0.0923\n",
            "Epoch 189/200\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 1.1242 - acc: 0.7412 - auc: 0.9141 - precision: 0.1370 - recall: 0.9354 - f1: 0.2297 - val_loss: 3.6848 - val_acc: 0.6103 - val_auc: 0.5543 - val_precision: 0.0529 - val_recall: 0.4697 - val_f1: 0.0921\n",
            "Epoch 190/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0935 - acc: 0.7597 - auc: 0.9238 - precision: 0.1449 - recall: 0.9240 - f1: 0.2424 - val_loss: 4.0252 - val_acc: 0.7596 - val_auc: 0.5574 - val_precision: 0.0618 - val_recall: 0.3182 - val_f1: 0.0933\n",
            "Epoch 191/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0941 - acc: 0.7596 - auc: 0.9230 - precision: 0.1478 - recall: 0.9506 - f1: 0.2439 - val_loss: 4.1145 - val_acc: 0.7431 - val_auc: 0.5477 - val_precision: 0.0575 - val_recall: 0.3182 - val_f1: 0.0917\n",
            "Epoch 192/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1130 - acc: 0.7445 - auc: 0.9206 - precision: 0.1394 - recall: 0.9430 - f1: 0.2344 - val_loss: 3.6518 - val_acc: 0.5753 - val_auc: 0.5517 - val_precision: 0.0513 - val_recall: 0.5000 - val_f1: 0.0903\n",
            "Epoch 193/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.1020 - acc: 0.7508 - auc: 0.9218 - precision: 0.1420 - recall: 0.9392 - f1: 0.2366 - val_loss: 4.1081 - val_acc: 0.6856 - val_auc: 0.5474 - val_precision: 0.0563 - val_recall: 0.3939 - val_f1: 0.0916\n",
            "Epoch 194/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0597 - acc: 0.7556 - auc: 0.9306 - precision: 0.1473 - recall: 0.9658 - f1: 0.2472 - val_loss: 4.2513 - val_acc: 0.7596 - val_auc: 0.5500 - val_precision: 0.0643 - val_recall: 0.3333 - val_f1: 0.0969\n",
            "Epoch 195/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0850 - acc: 0.7612 - auc: 0.9262 - precision: 0.1478 - recall: 0.9430 - f1: 0.2451 - val_loss: 3.9565 - val_acc: 0.6546 - val_auc: 0.5461 - val_precision: 0.0563 - val_recall: 0.4394 - val_f1: 0.0926\n",
            "Epoch 196/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0990 - acc: 0.7574 - auc: 0.9233 - precision: 0.1483 - recall: 0.9658 - f1: 0.2439 - val_loss: 4.1062 - val_acc: 0.6757 - val_auc: 0.5451 - val_precision: 0.0526 - val_recall: 0.3788 - val_f1: 0.0849\n",
            "Epoch 197/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.0533 - acc: 0.7688 - auc: 0.9318 - precision: 0.1532 - recall: 0.9544 - f1: 0.2593 - val_loss: 4.2720 - val_acc: 0.6942 - val_auc: 0.5360 - val_precision: 0.0559 - val_recall: 0.3788 - val_f1: 0.0905\n",
            "Epoch 198/200\n",
            "95/95 [==============================] - 2s 16ms/step - loss: 1.1016 - acc: 0.7407 - auc: 0.9222 - precision: 0.1372 - recall: 0.9392 - f1: 0.2369 - val_loss: 3.9556 - val_acc: 0.6737 - val_auc: 0.5469 - val_precision: 0.0560 - val_recall: 0.4091 - val_f1: 0.0906\n",
            "Epoch 199/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0507 - acc: 0.7829 - auc: 0.9336 - precision: 0.1629 - recall: 0.9658 - f1: 0.2743 - val_loss: 4.1702 - val_acc: 0.6440 - val_auc: 0.5359 - val_precision: 0.0512 - val_recall: 0.4091 - val_f1: 0.0847\n",
            "Epoch 200/200\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 1.0640 - acc: 0.7512 - auc: 0.9298 - precision: 0.1434 - recall: 0.9506 - f1: 0.2481 - val_loss: 4.1831 - val_acc: 0.7001 - val_auc: 0.5442 - val_precision: 0.0550 - val_recall: 0.3636 - val_f1: 0.0889\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.1657 - acc: 0.7226 - auc: 0.6742 - precision: 0.0991 - recall: 0.5579 - f1: 0.1575\n",
            "2023-07-20 13:46:35.956762\n",
            "CPU times: user 5min 48s, sys: 22.9 s, total: 6min 11s\n",
            "Wall time: 4min 59s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# run model - change parameters as needed to match initial model!!!\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=64, epochs=200,\n",
        "           loss=wbce_custom(40), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = old_model, metrics=['f1'])\n",
        "\n",
        "# Combine new history with old history\n",
        "all_history = get_combined_history([old_history, history.history])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "7nfVA2CX4WUo",
        "outputId": "5de3376f-0492-443b-a764-86c14bc10908"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x700 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_train_val_loss(all_history, metrics=['f1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMSFvc5Shd2",
        "outputId": "1e772224-f2e4-4cde-cad0-5b35958d59a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Name: nn_51x51_7_10_2317\n"
          ]
        }
      ],
      "source": [
        "# Save Model\n",
        "\n",
        "model_notes = '''dataset: L3 300mx300m CHL, 15x15, num_feature=6, model_type=\"convolution\", batch_size=64, epochs=300,\n",
        "           loss=wbce_custom(50), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
        "           existing_model = None, metrics=[\"f1\"]. added weight decay. added batch norm.'''\n",
        "save_model(model, history, result, model_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNarrgECQ_75"
      },
      "outputs": [],
      "source": [
        "# input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "# # Getting X_test and y_test\n",
        "# xy_data = get_train_test_val_nn(input_data_,\n",
        "#                       time_site_pairs_train,\n",
        "#                       time_site_pairs_test)\n",
        "\n",
        "# X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "# result = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfJitefrng7f"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TfJ6uxge_KS0"
      },
      "outputs": [],
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wLwO3QLRnlCr"
      },
      "outputs": [],
      "source": [
        "# CV\n",
        "\n",
        "# def objective_function(params):\n",
        "#   results = []\n",
        "\n",
        "#   adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience = params.values()\n",
        "\n",
        "#   for i in range(1):\n",
        "#     time_site_pairs_train = train_val_dict[f'train_{i+1}']\n",
        "#     time_site_pairs_test = train_val_dict[f'val_{i+1}']\n",
        "\n",
        "#     _, _, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "#             loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "#             existing_model = None, metrics=['f1'], verbose=2)\n",
        "\n",
        "#     results.append(-result[-1])\n",
        "\n",
        "#   return {'loss': sum(results)/len(results), 'status': STATUS_OK}\n",
        "\n",
        "# No CV\n",
        "def objective_function(params):\n",
        "\n",
        "  adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience = params.values()\n",
        "\n",
        "  _, _, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "          loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "          existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)\n",
        "\n",
        "  f1_score = result[-1]\n",
        "\n",
        "  if f1_score == np.nan:\n",
        "    result = 0\n",
        "  else:\n",
        "    result = f1_score\n",
        "\n",
        "  return {'loss': -result, 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def run_trials():\n",
        "\n",
        "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
        "    max_trials = 5  # initial max_trials. put something small to not have to wait\n",
        "    \n",
        "    try:  # try to load an already saved trials object, and increase the max\n",
        "        trials = pickle.load(open(\"my_model_final.hyperopt\", \"rb\"))\n",
        "        print(\"Found saved Trials! Loading...\")\n",
        "        max_trials = len(trials.trials) + trials_step\n",
        "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
        "    except:  # create a new trials object and start searching\n",
        "        trials = Trials()\n",
        "\n",
        "    best_param = fmin(objective_function,\n",
        "                  param_hyperopt,\n",
        "                  algo=tpe.suggest,\n",
        "                  max_evals=max_trials,\n",
        "                  trials=trials)\n",
        "\n",
        "    print(\"Best:\", best_param)\n",
        "    \n",
        "    # save the trials object\n",
        "    with open(\"my_model_final\" + \".hyperopt\", \"wb\") as f:\n",
        "        pickle.dump(trials, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "AeHofhAxBFja",
        "outputId": "f9235ed1-4aa0-4aeb-895b-092301ac88fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-30 19:29:20.180379                           \n",
            "{'name': 'Adam', 'learning_rate': 0.022781398447970013, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_1', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_1', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.3990292711734842, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "  0%|          | 0/5 [00:12<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-30 19:29:32.807896: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1/1170 [..............................] - ETA: 15s - loss: 1.8413 - acc: 0.0625 - auc: 0.5000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  31/1170 [..............................] - ETA: 1s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0583 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 1.8085 - acc: 0.0362 - auc: 0.5000 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: 1.8052 - acc: 0.0335 - auc: 0.5000 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 139/1170 [==>...........................] - ETA: 1s - loss: 1.8029 - acc: 0.0317 - auc: 0.5000 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 171/1170 [===>..........................] - ETA: 1s - loss: 1.8033 - acc: 0.0320 - auc: 0.5000 - precision: 0.0320 - recall: 1.0000 - f1: 0.0601\n",
            " 205/1170 [====>.........................] - ETA: 1s - loss: 1.8024 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 236/1170 [=====>........................] - ETA: 1s - loss: 1.8017 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 268/1170 [=====>........................] - ETA: 1s - loss: 1.8015 - acc: 0.0306 - auc: 0.5000 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 297/1170 [======>.......................] - ETA: 1s - loss: 1.8008 - acc: 0.0300 - auc: 0.5000 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: 1.8011 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: 1.8012 - acc: 0.0303 - auc: 0.5000 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: 1.8020 - acc: 0.0309 - auc: 0.5000 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: 1.8017 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0579\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: 1.8011 - acc: 0.0303 - auc: 0.5000 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: 1.8007 - acc: 0.0299 - auc: 0.5000 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 1.8005 - acc: 0.0297 - auc: 0.5000 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 571/1170 [=============>................] - ETA: 0s - loss: 1.8006 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 604/1170 [==============>...............] - ETA: 0s - loss: 1.8002 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 636/1170 [===============>..............] - ETA: 0s - loss: 1.8000 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 669/1170 [================>.............] - ETA: 0s - loss: 1.7999 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 703/1170 [=================>............] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: 1.7995 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: 1.8001 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: 1.7996 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: 1.7997 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 1.7998 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 1.7999 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: 1.8001 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: 1.8001 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: 1.8002 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 1.8002 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 19:33:18.309379                           \n",
            "2023-07-30 19:33:18.359034                                                       \n",
            "{'name': 'Adam', 'learning_rate': 0.31169018675714044, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_2_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_2', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_2', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_2', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_2', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_3', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_3', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_3', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_3', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_3', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_4', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_5', 'trainable': True, 'dtype': 'float32', 'rate': 0.08723228372851344, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 19:36:06.922159                                                       \n",
            "2023-07-30 19:36:06.988197                                                       \n",
            "{'name': 'Adam', 'learning_rate': 0.0014065921676501979, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_2', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_4_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_4', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_4', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_4', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_4', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_5', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_5', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_5', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_5', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_2', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_6', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.06352662878518378, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 0.9253 - acc: 0.4688 - auc: 0.1667 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 40%|      | 2/5 [10:44<09:51, 197.27s/trial, best loss: -0.0557558499276638]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 0.7889 - acc: 0.4773 - auc: 0.6535 - precision: 0.0473 - recall: 0.7714 - f1: 0.0821             \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: 0.7994 - acc: 0.4619 - auc: 0.6300 - precision: 0.0486 - recall: 0.7500 - f1: 0.0864\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: 0.7973 - acc: 0.4590 - auc: 0.6427 - precision: 0.0462 - recall: 0.7586 - f1: 0.0816\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: 0.7939 - acc: 0.4591 - auc: 0.6411 - precision: 0.0427 - recall: 0.7544 - f1: 0.0760\n",
            " 148/1170 [==>...........................] - ETA: 1s - loss: 0.7962 - acc: 0.4592 - auc: 0.6477 - precision: 0.0464 - recall: 0.7785 - f1: 0.0833\n",
            " 183/1170 [===>..........................] - ETA: 1s - loss: 0.7958 - acc: 0.4539 - auc: 0.6389 - precision: 0.0428 - recall: 0.7622 - f1: 0.0769\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: 0.7957 - acc: 0.4517 - auc: 0.6315 - precision: 0.0412 - recall: 0.7500 - f1: 0.0741\n",
            " 245/1170 [=====>........................] - ETA: 1s - loss: 0.7940 - acc: 0.4540 - auc: 0.6263 - precision: 0.0405 - recall: 0.7417 - f1: 0.0731\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 0.7935 - acc: 0.4543 - auc: 0.6263 - precision: 0.0402 - recall: 0.7473 - f1: 0.0726\n",
            " 319/1170 [=======>......................] - ETA: 1s - loss: 0.7934 - acc: 0.4545 - auc: 0.6246 - precision: 0.0394 - recall: 0.7450 - f1: 0.0712\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: 0.7930 - acc: 0.4573 - auc: 0.6298 - precision: 0.0409 - recall: 0.7529 - f1: 0.0734\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: 0.7928 - acc: 0.4577 - auc: 0.6309 - precision: 0.0408 - recall: 0.7527 - f1: 0.0736\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: 0.7934 - acc: 0.4582 - auc: 0.6303 - precision: 0.0419 - recall: 0.7571 - f1: 0.0759\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: 0.7927 - acc: 0.4588 - auc: 0.6337 - precision: 0.0415 - recall: 0.7629 - f1: 0.0750\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: 0.7924 - acc: 0.4596 - auc: 0.6350 - precision: 0.0411 - recall: 0.7631 - f1: 0.0744\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: 0.7922 - acc: 0.4592 - auc: 0.6340 - precision: 0.0409 - recall: 0.7625 - f1: 0.0741\n",
            " 561/1170 [=============>................] - ETA: 0s - loss: 0.7919 - acc: 0.4589 - auc: 0.6317 - precision: 0.0403 - recall: 0.7542 - f1: 0.0728\n",
            " 595/1170 [==============>...............] - ETA: 0s - loss: 0.7919 - acc: 0.4586 - auc: 0.6271 - precision: 0.0396 - recall: 0.7482 - f1: 0.0715\n",
            " 630/1170 [===============>..............] - ETA: 0s - loss: 0.7919 - acc: 0.4600 - auc: 0.6260 - precision: 0.0399 - recall: 0.7483 - f1: 0.0721\n",
            " 667/1170 [================>.............] - ETA: 0s - loss: 0.7917 - acc: 0.4595 - auc: 0.6282 - precision: 0.0397 - recall: 0.7524 - f1: 0.0719\n",
            " 687/1170 [================>.............] - ETA: 0s - loss: 0.7920 - acc: 0.4582 - auc: 0.6272 - precision: 0.0394 - recall: 0.7543 - f1: 0.0713\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: 0.7921 - acc: 0.4594 - auc: 0.6267 - precision: 0.0400 - recall: 0.7564 - f1: 0.0723\n",
            " 744/1170 [==================>...........] - ETA: 0s - loss: 0.7918 - acc: 0.4591 - auc: 0.6269 - precision: 0.0397 - recall: 0.7587 - f1: 0.0718\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: 0.7923 - acc: 0.4585 - auc: 0.6269 - precision: 0.0398 - recall: 0.7572 - f1: 0.0722\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 0.7930 - acc: 0.4581 - auc: 0.6249 - precision: 0.0395 - recall: 0.7553 - f1: 0.0717\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 0.7932 - acc: 0.4574 - auc: 0.6245 - precision: 0.0394 - recall: 0.7554 - f1: 0.0714\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: 0.7944 - acc: 0.4568 - auc: 0.6236 - precision: 0.0396 - recall: 0.7579 - f1: 0.0719\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 0.7942 - acc: 0.4575 - auc: 0.6241 - precision: 0.0398 - recall: 0.7596 - f1: 0.0723\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 0.7939 - acc: 0.4571 - auc: 0.6229 - precision: 0.0395 - recall: 0.7570 - f1: 0.0718\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: 0.7943 - acc: 0.4563 - auc: 0.6208 - precision: 0.0394 - recall: 0.7576 - f1: 0.0716\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: 0.7938 - acc: 0.4574 - auc: 0.6223 - precision: 0.0397 - recall: 0.7602 - f1: 0.0722\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: 0.7943 - acc: 0.4569 - auc: 0.6244 - precision: 0.0402 - recall: 0.7653 - f1: 0.0730\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 0.7943 - acc: 0.4568 - auc: 0.6259 - precision: 0.0405 - recall: 0.7674 - f1: 0.0733\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: 0.7951 - acc: 0.4553 - auc: 0.6238 - precision: 0.0403 - recall: 0.7636 - f1: 0.0729\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.4553 - auc: 0.6234 - precision: 0.0402 - recall: 0.7620 - f1: 0.0728\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 0.7949 - acc: 0.4552 - auc: 0.6233 - precision: 0.0402 - recall: 0.7613 - f1: 0.0727\n",
            "\n",
            "2023-07-30 19:40:07.397599                                                       \n",
            "2023-07-30 19:40:07.458233                                                        \n",
            "{'name': 'Adam', 'learning_rate': 1.5695863256449222e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_3', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_6_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_6', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_6', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_6', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_6', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_7', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_7', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_7', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_7', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_3', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_12', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_9', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_10', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_11', 'trainable': True, 'dtype': 'float32', 'rate': 0.43287837870905754, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 1.8313 - acc: 0.0625 - auc: 0.1333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  31/1170 [..............................] - ETA: 1s - loss: 1.6318 - acc: 0.0312 - auc: 0.6328 - precision: 0.0312 - recall: 1.0000 - f1: 0.0583 \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: 1.6416 - acc: 0.0361 - auc: 0.6439 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  95/1170 [=>............................] - ETA: 1s - loss: 1.6330 - acc: 0.0316 - auc: 0.6491 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 135/1170 [==>...........................] - ETA: 1s - loss: 1.6335 - acc: 0.0319 - auc: 0.6591 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 172/1170 [===>..........................] - ETA: 1s - loss: 1.6353 - acc: 0.0320 - auc: 0.6602 - precision: 0.0320 - recall: 1.0000 - f1: 0.0601\n",
            " 208/1170 [====>.........................] - ETA: 1s - loss: 1.6356 - acc: 0.0312 - auc: 0.6547 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: 1.6356 - acc: 0.0306 - auc: 0.6454 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 278/1170 [======>.......................] - ETA: 1s - loss: 1.6332 - acc: 0.0301 - auc: 0.6529 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 315/1170 [=======>......................] - ETA: 1s - loss: 1.6329 - acc: 0.0299 - auc: 0.6491 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: 1.6325 - acc: 0.0303 - auc: 0.6520 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: 1.6322 - acc: 0.0306 - auc: 0.6526 - precision: 0.0306 - recall: 1.0000 - f1: 0.0577\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 1.6331 - acc: 0.0310 - auc: 0.6495 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: 1.6318 - acc: 0.0302 - auc: 0.6498 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: 1.6315 - acc: 0.0302 - auc: 0.6504 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 524/1170 [============>.................] - ETA: 0s - loss: 1.6314 - acc: 0.0299 - auc: 0.6483 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 563/1170 [=============>................] - ETA: 0s - loss: 1.6316 - acc: 0.0297 - auc: 0.6449 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 601/1170 [==============>...............] - ETA: 0s - loss: 1.6312 - acc: 0.0295 - auc: 0.6428 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 640/1170 [===============>..............] - ETA: 0s - loss: 1.6307 - acc: 0.0295 - auc: 0.6407 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 680/1170 [================>.............] - ETA: 0s - loss: 1.6317 - acc: 0.0292 - auc: 0.6396 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 719/1170 [=================>............] - ETA: 0s - loss: 1.6317 - acc: 0.0295 - auc: 0.6392 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 757/1170 [==================>...........] - ETA: 0s - loss: 1.6312 - acc: 0.0291 - auc: 0.6400 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: 1.6322 - acc: 0.0293 - auc: 0.6370 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: 1.6318 - acc: 0.0289 - auc: 0.6374 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: 1.6326 - acc: 0.0291 - auc: 0.6368 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: 1.6333 - acc: 0.0294 - auc: 0.6338 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: 1.6328 - acc: 0.0291 - auc: 0.6322 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: 1.6334 - acc: 0.0290 - auc: 0.6303 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: 1.6333 - acc: 0.0292 - auc: 0.6304 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: 1.6336 - acc: 0.0294 - auc: 0.6332 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: 1.6333 - acc: 0.0294 - auc: 0.6348 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: 1.6345 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: 1.6346 - acc: 0.0295 - auc: 0.6311 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 1.6347 - acc: 0.0296 - auc: 0.6308 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 19:42:55.152368                                                        \n",
            "2023-07-30 19:42:55.210582                                                        \n",
            "{'name': 'Adam', 'learning_rate': 0.00021982119140504425, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_4', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_8_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_8', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_8', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_8', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_8', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_9', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_9', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_9', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_9', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_4', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_16', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_12', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_17', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_13', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_14', 'trainable': True, 'dtype': 'float32', 'rate': 0.24724111871600107, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_19', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 80%|  | 4/5 [21:36<03:17, 197.55s/trial, best loss: -0.07274166494607925]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  26/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9663 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 19:50:59.540708                                                        \n",
            "100%|| 5/5 [21:39<00:00, 259.88s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 5 trials to 6 (+1) trials\n",
            "2023-07-30 19:50:59.604669                           \n",
            "{'name': 'Adam', 'learning_rate': 0.025785122398464748, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_5', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_10_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_10', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_10', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_10', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_10', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_11', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_11', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_11', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_11', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_5', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_20', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_15', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_16', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_22', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_17', 'trainable': True, 'dtype': 'float32', 'rate': 0.46413176072234996, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_23', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 0.4154 - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 83%| | 5/6 [07:00<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0013s). Check your callbacks.\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 0.3817 - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  72/1170 [>.............................] - ETA: 1s - loss: 0.3850 - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  98/1170 [=>............................] - ETA: 1s - loss: 0.3796 - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 124/1170 [==>...........................] - ETA: 1s - loss: 0.3804 - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 165/1170 [===>..........................] - ETA: 1s - loss: 0.3798 - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 205/1170 [====>.........................] - ETA: 1s - loss: 0.3796 - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 247/1170 [=====>........................] - ETA: 1s - loss: 0.3786 - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 288/1170 [======>.......................] - ETA: 1s - loss: 0.3783 - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 0.3783 - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 371/1170 [========>.....................] - ETA: 1s - loss: 0.3784 - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 412/1170 [=========>....................] - ETA: 1s - loss: 0.3791 - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: 0.3788 - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 473/1170 [===========>..................] - ETA: 0s - loss: 0.3785 - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 514/1170 [============>.................] - ETA: 0s - loss: 0.3781 - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 557/1170 [=============>................] - ETA: 0s - loss: 0.3778 - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 591/1170 [==============>...............] - ETA: 0s - loss: 0.3776 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 633/1170 [===============>..............] - ETA: 0s - loss: 0.3775 - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 671/1170 [================>.............] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: 0.3775 - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: 0.3773 - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: 0.3772 - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: 0.3770 - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: 0.3773 - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: 0.3774 - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 0.3776 - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "\n",
            "2023-07-30 19:58:01.903356                           \n",
            "100%|| 6/6 [07:02<00:00, 422.36s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 6 trials to 7 (+1) trials\n",
            "2023-07-30 19:58:01.972486                           \n",
            "{'name': 'Adam', 'learning_rate': 0.0006882315854392572, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_6', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_12_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_12', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_12', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_12', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_12', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_13', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_13', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_13', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_13', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_6', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_24', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_18', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_25', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_19', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_26', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_20', 'trainable': True, 'dtype': 'float32', 'rate': 0.007654268063796377, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_27', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.3200 - acc: 0.0625 - auc: 0.2500 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  36/1170 [..............................] - ETA: 1s - loss: 2.1152 - acc: 0.0330 - auc: 0.6588 - precision: 0.0330 - recall: 1.0000 - f1: 0.0614 \n",
            "  68/1170 [>.............................] - ETA: 1s - loss: 2.1443 - acc: 0.0363 - auc: 0.6229 - precision: 0.0363 - recall: 1.0000 - f1: 0.0679\n",
            " 108/1170 [=>............................] - ETA: 1s - loss: 2.1284 - acc: 0.0315 - auc: 0.6318 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 135/1170 [==>...........................] - ETA: 1s - loss: 2.1271 - acc: 0.0319 - auc: 0.6504 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 159/1170 [===>..........................] - ETA: 1s - loss: 2.1279 - acc: 0.0324 - auc: 0.6509 - precision: 0.0324 - recall: 1.0000 - f1: 0.0610\n",
            " 199/1170 [====>.........................] - ETA: 1s - loss: 2.1266 - acc: 0.0311 - auc: 0.6525 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 242/1170 [=====>........................] - ETA: 1s - loss: 2.1329 - acc: 0.0306 - auc: 0.6326 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 2.1299 - acc: 0.0301 - auc: 0.6346 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 316/1170 [=======>......................] - ETA: 1s - loss: 2.1312 - acc: 0.0299 - auc: 0.6284 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: 2.1286 - acc: 0.0303 - auc: 0.6329 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: 2.1278 - acc: 0.0302 - auc: 0.6317 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: 2.1275 - acc: 0.0310 - auc: 0.6315 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: 2.1276 - acc: 0.0302 - auc: 0.6344 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 504/1170 [===========>..................] - ETA: 0s - loss: 2.1256 - acc: 0.0299 - auc: 0.6373 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 546/1170 [=============>................] - ETA: 0s - loss: 2.1255 - acc: 0.0297 - auc: 0.6349 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 589/1170 [==============>...............] - ETA: 0s - loss: 2.1266 - acc: 0.0296 - auc: 0.6331 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 631/1170 [===============>..............] - ETA: 0s - loss: 2.1251 - acc: 0.0296 - auc: 0.6323 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 672/1170 [================>.............] - ETA: 0s - loss: 2.1263 - acc: 0.0293 - auc: 0.6313 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 715/1170 [=================>............] - ETA: 0s - loss: 2.1257 - acc: 0.0294 - auc: 0.6305 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: 2.1247 - acc: 0.0291 - auc: 0.6303 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: 2.1251 - acc: 0.0292 - auc: 0.6301 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: 2.1262 - acc: 0.0291 - auc: 0.6280 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: 2.1275 - acc: 0.0291 - auc: 0.6266 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: 2.1282 - acc: 0.0295 - auc: 0.6263 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 2.1282 - acc: 0.0292 - auc: 0.6244 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: 2.1288 - acc: 0.0290 - auc: 0.6237 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: 2.1272 - acc: 0.0292 - auc: 0.6257 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: 2.1274 - acc: 0.0294 - auc: 0.6279 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: 2.1285 - acc: 0.0296 - auc: 0.6276 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: 2.1290 - acc: 0.0296 - auc: 0.6254 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.1282 - acc: 0.0296 - auc: 0.6249 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:01:22.611300                           \n",
            "100%|| 7/7 [03:20<00:00, 200.71s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 7 trials to 8 (+1) trials\n",
            "2023-07-30 20:01:22.687024                           \n",
            "{'name': 'Adam', 'learning_rate': 0.00832166518594385, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_7', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_14_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_14', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_14', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_14', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_14', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_15', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_15', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_15', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_15', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_7', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_28', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_21', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_29', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_22', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_30', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_23', 'trainable': True, 'dtype': 'float32', 'rate': 0.35791110427762296, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_31', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  67/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 0s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:08:05.614074                           \n",
            "100%|| 8/8 [06:42<00:00, 402.98s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 8 trials to 9 (+1) trials\n",
            "2023-07-30 20:08:05.665748                           \n",
            "{'name': 'Adam', 'learning_rate': 4.037944046128388e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_8', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_16_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_16', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_16', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_16', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_16', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_17', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_17', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_17', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_17', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_8', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_32', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_24', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_33', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_25', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_34', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_26', 'trainable': True, 'dtype': 'float32', 'rate': 0.2990120289799095, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_35', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.1426 - acc: 0.0625 - auc: 0.2167 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 1.9299 - acc: 0.0339 - auc: 0.6509 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 1.9424 - acc: 0.0362 - auc: 0.6232 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            " 102/1170 [=>............................] - ETA: 1s - loss: 1.9357 - acc: 0.0316 - auc: 0.6422 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 140/1170 [==>...........................] - ETA: 1s - loss: 1.9374 - acc: 0.0317 - auc: 0.6565 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 164/1170 [===>..........................] - ETA: 1s - loss: 1.9358 - acc: 0.0316 - auc: 0.6583 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 194/1170 [===>..........................] - ETA: 1s - loss: 1.9347 - acc: 0.0308 - auc: 0.6585 - precision: 0.0308 - recall: 1.0000 - f1: 0.0578\n",
            " 235/1170 [=====>........................] - ETA: 1s - loss: 1.9367 - acc: 0.0307 - auc: 0.6463 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 276/1170 [======>.......................] - ETA: 1s - loss: 1.9361 - acc: 0.0300 - auc: 0.6463 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 301/1170 [======>.......................] - ETA: 1s - loss: 1.9372 - acc: 0.0299 - auc: 0.6461 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: 1.9356 - acc: 0.0301 - auc: 0.6474 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: 1.9338 - acc: 0.0302 - auc: 0.6487 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 1.9340 - acc: 0.0310 - auc: 0.6476 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: 1.9336 - acc: 0.0304 - auc: 0.6496 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 500/1170 [===========>..................] - ETA: 0s - loss: 1.9327 - acc: 0.0301 - auc: 0.6520 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 525/1170 [============>.................] - ETA: 0s - loss: 1.9333 - acc: 0.0298 - auc: 0.6494 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 563/1170 [=============>................] - ETA: 0s - loss: 1.9337 - acc: 0.0297 - auc: 0.6455 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 604/1170 [==============>...............] - ETA: 0s - loss: 1.9337 - acc: 0.0295 - auc: 0.6424 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 645/1170 [===============>..............] - ETA: 0s - loss: 1.9326 - acc: 0.0296 - auc: 0.6417 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 688/1170 [================>.............] - ETA: 0s - loss: 1.9340 - acc: 0.0291 - auc: 0.6410 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 728/1170 [=================>............] - ETA: 0s - loss: 1.9337 - acc: 0.0291 - auc: 0.6391 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 771/1170 [==================>...........] - ETA: 0s - loss: 1.9338 - acc: 0.0291 - auc: 0.6389 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: 1.9346 - acc: 0.0293 - auc: 0.6383 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: 1.9349 - acc: 0.0289 - auc: 0.6378 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: 1.9358 - acc: 0.0291 - auc: 0.6372 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: 1.9362 - acc: 0.0293 - auc: 0.6353 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 1.9357 - acc: 0.0291 - auc: 0.6350 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: 1.9363 - acc: 0.0290 - auc: 0.6339 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 1.9366 - acc: 0.0292 - auc: 0.6345 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: 1.9363 - acc: 0.0293 - auc: 0.6370 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: 1.9360 - acc: 0.0295 - auc: 0.6401 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 1.9369 - acc: 0.0295 - auc: 0.6359 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 1.9368 - acc: 0.0296 - auc: 0.6367 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:10:16.726370                           \n",
            "100%|| 9/9 [02:11<00:00, 131.11s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 9 trials to 10 (+1) trials\n",
            "2023-07-30 20:10:16.781789                            \n",
            "{'name': 'Adam', 'learning_rate': 7.929295038158712e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_9', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_18_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_18', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_18', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_18', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_18', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_19', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_19', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_19', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_19', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_9', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_36', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_27', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_37', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_28', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_38', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_29', 'trainable': True, 'dtype': 'float32', 'rate': 0.11078871062459339, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_39', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.8546 - acc: 0.0625 - auc: 0.1583 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  32/1170 [..............................] - ETA: 1s - loss: 2.6180 - acc: 0.0332 - auc: 0.6120 - precision: 0.0332 - recall: 1.0000 - f1: 0.0618 \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: 2.6329 - acc: 0.0361 - auc: 0.6163 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  99/1170 [=>............................] - ETA: 1s - loss: 2.6242 - acc: 0.0312 - auc: 0.6335 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 125/1170 [==>...........................] - ETA: 1s - loss: 2.6221 - acc: 0.0320 - auc: 0.6384 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 144/1170 [==>...........................] - ETA: 1s - loss: 2.6249 - acc: 0.0326 - auc: 0.6423 - precision: 0.0326 - recall: 1.0000 - f1: 0.0613\n",
            " 183/1170 [===>..........................] - ETA: 1s - loss: 2.6291 - acc: 0.0316 - auc: 0.6300 - precision: 0.0316 - recall: 1.0000 - f1: 0.0593\n",
            " 225/1170 [====>.........................] - ETA: 1s - loss: 2.6271 - acc: 0.0306 - auc: 0.6296 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: 2.6269 - acc: 0.0305 - auc: 0.6262 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 272/1170 [=====>........................] - ETA: 1s - loss: 2.6245 - acc: 0.0303 - auc: 0.6295 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 312/1170 [=======>......................] - ETA: 1s - loss: 2.6237 - acc: 0.0295 - auc: 0.6284 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 352/1170 [========>.....................] - ETA: 1s - loss: 2.6216 - acc: 0.0303 - auc: 0.6366 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 2.6201 - acc: 0.0305 - auc: 0.6398 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: 2.6209 - acc: 0.0307 - auc: 0.6388 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: 2.6210 - acc: 0.0301 - auc: 0.6405 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: 2.6201 - acc: 0.0300 - auc: 0.6420 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 542/1170 [============>.................] - ETA: 0s - loss: 2.6212 - acc: 0.0298 - auc: 0.6372 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: 2.6221 - acc: 0.0298 - auc: 0.6311 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 625/1170 [===============>..............] - ETA: 0s - loss: 2.6209 - acc: 0.0293 - auc: 0.6308 - precision: 0.0293 - recall: 1.0000 - f1: 0.0554\n",
            " 667/1170 [================>.............] - ETA: 0s - loss: 2.6206 - acc: 0.0293 - auc: 0.6324 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 690/1170 [================>.............] - ETA: 0s - loss: 2.6212 - acc: 0.0291 - auc: 0.6314 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: 2.6215 - acc: 0.0293 - auc: 0.6293 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: 2.6212 - acc: 0.0291 - auc: 0.6296 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 2.6231 - acc: 0.0293 - auc: 0.6261 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: 2.6236 - acc: 0.0290 - auc: 0.6255 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: 2.6237 - acc: 0.0292 - auc: 0.6261 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 2.6240 - acc: 0.0294 - auc: 0.6258 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: 2.6240 - acc: 0.0291 - auc: 0.6232 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.6246 - acc: 0.0290 - auc: 0.6217 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: 2.6247 - acc: 0.0293 - auc: 0.6223 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 2.6248 - acc: 0.0293 - auc: 0.6246 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.6247 - acc: 0.0295 - auc: 0.6262 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: 2.6263 - acc: 0.0296 - auc: 0.6226 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.6260 - acc: 0.0296 - auc: 0.6225 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:12:19.579171                            \n",
            "100%|| 10/10 [02:02<00:00, 122.85s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 10 trials to 11 (+1) trials\n",
            "2023-07-30 20:12:19.632641                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3963159423976986, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_10', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_20_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_20', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_20', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_20', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_20', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_21', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_21', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_21', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_21', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_10', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_40', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_30', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_41', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_31', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_42', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_32', 'trainable': True, 'dtype': 'float32', 'rate': 0.31631775359408465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_43', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:13:47.997122                             \n",
            "100%|| 11/11 [01:28<00:00, 88.41s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 11 trials to 12 (+1) trials\n",
            "2023-07-30 20:13:48.049130                             \n",
            "{'name': 'Adam', 'learning_rate': 0.44818763769596054, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_11', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_22_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_22', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_22', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_22', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_22', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_23', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_23', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_23', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_23', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_11', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_44', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_33', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_45', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_34', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_46', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_35', 'trainable': True, 'dtype': 'float32', 'rate': 0.1856928298783203, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_47', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:16:05.384448                             \n",
            "100%|| 12/12 [02:17<00:00, 137.39s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 12 trials to 13 (+1) trials\n",
            "2023-07-30 20:16:05.437498                             \n",
            "{'name': 'Adam', 'learning_rate': 0.10495461990291796, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_12', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_24_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_24', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_24', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_24', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_24', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_25', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_25', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_25', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_25', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_12', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_48', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_36', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_49', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_37', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_50', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_38', 'trainable': True, 'dtype': 'float32', 'rate': 0.012296750111987165, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_51', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 0s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:22:16.195214                             \n",
            "100%|| 13/13 [06:10<00:00, 370.81s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 13 trials to 14 (+1) trials\n",
            "2023-07-30 20:22:16.332920                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7857485657607498, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_13', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_26_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_26', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_26', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_26', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_26', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_27', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_27', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_27', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_27', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_13', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_52', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_39', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_53', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_40', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_54', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_41', 'trainable': True, 'dtype': 'float32', 'rate': 0.20319001821377797, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_55', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 1s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 0s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:27:36.373671                             \n",
            "100%|| 14/14 [05:20<00:00, 320.09s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 14 trials to 15 (+1) trials\n",
            "2023-07-30 20:27:36.427525                             \n",
            "{'name': 'Adam', 'learning_rate': 0.025111149518369978, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_14', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_28_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_28', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_28', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_28', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_28', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_29', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_29', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_29', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_29', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_14', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_56', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_42', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_57', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_43', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_58', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_44', 'trainable': True, 'dtype': 'float32', 'rate': 0.1183220713429442, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_59', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.6949 - acc: 0.0625 - auc: 0.2333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.6319 - acc: 0.0349 - auc: 0.6540 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: 2.6353 - acc: 0.0370 - auc: 0.6419 - precision: 0.0370 - recall: 1.0000 - f1: 0.0690\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: 2.6344 - acc: 0.0355 - auc: 0.6416 - precision: 0.0355 - recall: 1.0000 - f1: 0.0663\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: 2.6326 - acc: 0.0309 - auc: 0.6380 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 149/1170 [==>...........................] - ETA: 1s - loss: 2.6344 - acc: 0.0331 - auc: 0.6378 - precision: 0.0331 - recall: 1.0000 - f1: 0.0623\n",
            " 192/1170 [===>..........................] - ETA: 1s - loss: 2.6339 - acc: 0.0306 - auc: 0.6324 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 221/1170 [====>.........................] - ETA: 1s - loss: 2.6343 - acc: 0.0307 - auc: 0.6271 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 254/1170 [=====>........................] - ETA: 1s - loss: 2.6338 - acc: 0.0301 - auc: 0.6164 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 288/1170 [======>.......................] - ETA: 1s - loss: 2.6338 - acc: 0.0302 - auc: 0.6152 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: 2.6335 - acc: 0.0297 - auc: 0.6168 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 343/1170 [=======>......................] - ETA: 1s - loss: 2.6329 - acc: 0.0299 - auc: 0.6209 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.6327 - acc: 0.0303 - auc: 0.6214 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: 2.6326 - acc: 0.0310 - auc: 0.6208 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: 2.6323 - acc: 0.0303 - auc: 0.6240 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: 2.6321 - acc: 0.0302 - auc: 0.6248 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 527/1170 [============>.................] - ETA: 0s - loss: 2.6319 - acc: 0.0298 - auc: 0.6212 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 568/1170 [=============>................] - ETA: 0s - loss: 2.6318 - acc: 0.0296 - auc: 0.6209 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 610/1170 [==============>...............] - ETA: 0s - loss: 2.6318 - acc: 0.0295 - auc: 0.6162 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 636/1170 [===============>..............] - ETA: 0s - loss: 2.6316 - acc: 0.0294 - auc: 0.6144 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 670/1170 [================>.............] - ETA: 0s - loss: 2.6316 - acc: 0.0292 - auc: 0.6147 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 710/1170 [=================>............] - ETA: 0s - loss: 2.6319 - acc: 0.0294 - auc: 0.6136 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 2.6315 - acc: 0.0291 - auc: 0.6139 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 778/1170 [==================>...........] - ETA: 0s - loss: 2.6317 - acc: 0.0293 - auc: 0.6135 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: 2.6319 - acc: 0.0292 - auc: 0.6113 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: 2.6317 - acc: 0.0290 - auc: 0.6120 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.6322 - acc: 0.0293 - auc: 0.6111 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: 2.6323 - acc: 0.0293 - auc: 0.6091 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: 2.6322 - acc: 0.0291 - auc: 0.6073 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.6324 - acc: 0.0290 - auc: 0.6047 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: 2.6327 - acc: 0.0292 - auc: 0.6036 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: 2.6327 - acc: 0.0293 - auc: 0.6070 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: 2.6328 - acc: 0.0294 - auc: 0.6084 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 2.6331 - acc: 0.0295 - auc: 0.6039 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: 2.6331 - acc: 0.0296 - auc: 0.6037 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:32:49.952866                             \n",
            "100%|| 15/15 [05:13<00:00, 313.57s/trial, best loss: -0.07274166494607925]\n",
            "Best: {'dropout': 0.06352662878518378, 'learning_rate': 0.0014065921676501979, 'loss_weight': 3.53661238879996, 'patience': 14.806775690638117}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 15 trials to 16 (+1) trials\n",
            "2023-07-30 20:32:50.004461                             \n",
            "{'name': 'Adam', 'learning_rate': 1.848375226671289e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_15', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_30_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_30', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_30', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_30', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_30', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_31', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_31', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_31', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_31', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_15', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_60', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_45', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_61', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_46', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_62', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_47', 'trainable': True, 'dtype': 'float32', 'rate': 0.49621807449983946, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_63', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.0311 - acc: 0.4062 - auc: 0.0833 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 0.9404 - acc: 0.4969 - auc: 0.6077 - precision: 0.0444 - recall: 0.7097 - f1: 0.0746             \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: 0.9487 - acc: 0.4849 - auc: 0.6269 - precision: 0.0503 - recall: 0.7385 - f1: 0.0879\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: 0.9482 - acc: 0.4805 - auc: 0.6356 - precision: 0.0508 - recall: 0.7590 - f1: 0.0893\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: 0.9439 - acc: 0.4844 - auc: 0.6427 - precision: 0.0486 - recall: 0.7640 - f1: 0.0856\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: 0.9411 - acc: 0.4815 - auc: 0.6441 - precision: 0.0445 - recall: 0.7619 - f1: 0.0787\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: 0.9419 - acc: 0.4834 - auc: 0.6435 - precision: 0.0457 - recall: 0.7540 - f1: 0.0806\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: 0.9413 - acc: 0.4846 - auc: 0.6501 - precision: 0.0459 - recall: 0.7634 - f1: 0.0814\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: 0.9426 - acc: 0.4809 - auc: 0.6502 - precision: 0.0466 - recall: 0.7793 - f1: 0.0830\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: 0.9429 - acc: 0.4771 - auc: 0.6440 - precision: 0.0454 - recall: 0.7670 - f1: 0.0811\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: 0.9423 - acc: 0.4769 - auc: 0.6378 - precision: 0.0442 - recall: 0.7624 - f1: 0.0788\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: 0.9415 - acc: 0.4760 - auc: 0.6300 - precision: 0.0422 - recall: 0.7399 - f1: 0.0758\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: 0.9408 - acc: 0.4755 - auc: 0.6262 - precision: 0.0408 - recall: 0.7325 - f1: 0.0736\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: 0.9404 - acc: 0.4774 - auc: 0.6288 - precision: 0.0411 - recall: 0.7323 - f1: 0.0737\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 0.9399 - acc: 0.4757 - auc: 0.6260 - precision: 0.0407 - recall: 0.7415 - f1: 0.0728\n",
            " 338/1170 [=======>......................] - ETA: 1s - loss: 0.9403 - acc: 0.4780 - auc: 0.6302 - precision: 0.0419 - recall: 0.7431 - f1: 0.0749\n",
            " 367/1170 [========>.....................] - ETA: 1s - loss: 0.9402 - acc: 0.4783 - auc: 0.6306 - precision: 0.0419 - recall: 0.7416 - f1: 0.0749\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: 0.9398 - acc: 0.4794 - auc: 0.6334 - precision: 0.0422 - recall: 0.7446 - f1: 0.0754\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: 0.9405 - acc: 0.4789 - auc: 0.6319 - precision: 0.0430 - recall: 0.7449 - f1: 0.0768\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: 0.9405 - acc: 0.4782 - auc: 0.6305 - precision: 0.0429 - recall: 0.7458 - f1: 0.0769\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 0.9398 - acc: 0.4787 - auc: 0.6320 - precision: 0.0424 - recall: 0.7494 - f1: 0.0760\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: 0.9399 - acc: 0.4787 - auc: 0.6347 - precision: 0.0427 - recall: 0.7548 - f1: 0.0765\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: 0.9394 - acc: 0.4801 - auc: 0.6322 - precision: 0.0421 - recall: 0.7510 - f1: 0.0755\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: 0.9391 - acc: 0.4793 - auc: 0.6313 - precision: 0.0416 - recall: 0.7490 - f1: 0.0746\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: 0.9391 - acc: 0.4794 - auc: 0.6293 - precision: 0.0413 - recall: 0.7439 - f1: 0.0741\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: 0.9395 - acc: 0.4786 - auc: 0.6253 - precision: 0.0410 - recall: 0.7377 - f1: 0.0735\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: 0.9391 - acc: 0.4788 - auc: 0.6252 - precision: 0.0405 - recall: 0.7359 - f1: 0.0727\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: 0.9386 - acc: 0.4796 - auc: 0.6239 - precision: 0.0401 - recall: 0.7313 - f1: 0.0720\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: 0.9388 - acc: 0.4795 - auc: 0.6249 - precision: 0.0405 - recall: 0.7358 - f1: 0.0728\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 0.9388 - acc: 0.4789 - auc: 0.6249 - precision: 0.0403 - recall: 0.7382 - f1: 0.0724\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: 0.9387 - acc: 0.4795 - auc: 0.6267 - precision: 0.0405 - recall: 0.7417 - f1: 0.0729\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 0.9386 - acc: 0.4795 - auc: 0.6240 - precision: 0.0402 - recall: 0.7372 - f1: 0.0722\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: 0.9387 - acc: 0.4787 - auc: 0.6251 - precision: 0.0401 - recall: 0.7380 - f1: 0.0721\n",
            " 789/1170 [===================>..........] - ETA: 0s - loss: 0.9392 - acc: 0.4783 - auc: 0.6241 - precision: 0.0404 - recall: 0.7368 - f1: 0.0727\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 0.9390 - acc: 0.4779 - auc: 0.6235 - precision: 0.0399 - recall: 0.7342 - f1: 0.0719\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: 0.9387 - acc: 0.4781 - auc: 0.6244 - precision: 0.0397 - recall: 0.7353 - f1: 0.0715\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: 0.9390 - acc: 0.4776 - auc: 0.6231 - precision: 0.0395 - recall: 0.7312 - f1: 0.0712\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: 0.9393 - acc: 0.4775 - auc: 0.6234 - precision: 0.0399 - recall: 0.7350 - f1: 0.0719\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: 0.9398 - acc: 0.4778 - auc: 0.6222 - precision: 0.0402 - recall: 0.7314 - f1: 0.0725\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: 0.9395 - acc: 0.4779 - auc: 0.6216 - precision: 0.0399 - recall: 0.7332 - f1: 0.0719\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: 0.9394 - acc: 0.4781 - auc: 0.6199 - precision: 0.0397 - recall: 0.7299 - f1: 0.0717\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 0.9394 - acc: 0.4772 - auc: 0.6186 - precision: 0.0395 - recall: 0.7297 - f1: 0.0714\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 0.9393 - acc: 0.4772 - auc: 0.6188 - precision: 0.0396 - recall: 0.7319 - f1: 0.0716\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: 0.9391 - acc: 0.4777 - auc: 0.6188 - precision: 0.0396 - recall: 0.7328 - f1: 0.0715\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 0.9394 - acc: 0.4779 - auc: 0.6205 - precision: 0.0401 - recall: 0.7367 - f1: 0.0723\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 0.9398 - acc: 0.4772 - auc: 0.6211 - precision: 0.0404 - recall: 0.7401 - f1: 0.0728\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: 0.9399 - acc: 0.4771 - auc: 0.6225 - precision: 0.0407 - recall: 0.7416 - f1: 0.0732\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: 0.9402 - acc: 0.4761 - auc: 0.6198 - precision: 0.0404 - recall: 0.7370 - f1: 0.0726\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.4762 - auc: 0.6184 - precision: 0.0403 - recall: 0.7341 - f1: 0.0725\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.4765 - auc: 0.6194 - precision: 0.0405 - recall: 0.7359 - f1: 0.0728\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 0.9403 - acc: 0.4763 - auc: 0.6194 - precision: 0.0405 - recall: 0.7360 - f1: 0.0728\n",
            "\n",
            "2023-07-30 20:41:50.481057                             \n",
            "100%|| 16/16 [09:00<00:00, 540.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 16 trials to 17 (+1) trials\n",
            "2023-07-30 20:41:50.543093                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7604976070539027, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_16', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_32_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_32', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_32', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_32', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_32', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_33', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_33', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_33', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_33', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_16', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_64', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_48', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_65', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_49', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_66', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_50', 'trainable': True, 'dtype': 'float32', 'rate': 0.34562429881537815, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_67', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  25/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9663 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 20:49:28.126310                             \n",
            "100%|| 17/17 [07:37<00:00, 457.64s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 17 trials to 18 (+1) trials\n",
            "2023-07-30 20:49:28.184075                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0011482712432998505, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_17', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_34_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_34', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_34', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_34', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_34', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_35', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_35', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_35', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_35', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_17', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_68', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_51', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_69', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_52', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_70', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_53', 'trainable': True, 'dtype': 'float32', 'rate': 0.09860091406648591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_71', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 2.7746 - acc: 0.0625 - auc: 0.2917 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            " 94%|| 17/18 [04:41<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0013s). Check your callbacks.\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.4619 - acc: 0.0349 - auc: 0.6653 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  62/1170 [>.............................] - ETA: 1s - loss: 2.4814 - acc: 0.0363 - auc: 0.6376 - precision: 0.0363 - recall: 1.0000 - f1: 0.0677\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: 2.4755 - acc: 0.0344 - auc: 0.6501 - precision: 0.0344 - recall: 1.0000 - f1: 0.0644\n",
            "  86/1170 [=>............................] - ETA: 2s - loss: 2.4717 - acc: 0.0342 - auc: 0.6484 - precision: 0.0342 - recall: 1.0000 - f1: 0.0640\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: 2.4653 - acc: 0.0312 - auc: 0.6556 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: 2.4629 - acc: 0.0320 - auc: 0.6582 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.4609 - acc: 0.0320 - auc: 0.6625 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: 2.4690 - acc: 0.0331 - auc: 0.6624 - precision: 0.0331 - recall: 1.0000 - f1: 0.0623\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: 2.4679 - acc: 0.0316 - auc: 0.6560 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: 2.4714 - acc: 0.0316 - auc: 0.6490 - precision: 0.0316 - recall: 1.0000 - f1: 0.0593\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: 2.4751 - acc: 0.0311 - auc: 0.6351 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: 2.4744 - acc: 0.0305 - auc: 0.6266 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: 2.4757 - acc: 0.0304 - auc: 0.6279 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: 2.4719 - acc: 0.0301 - auc: 0.6341 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: 2.4744 - acc: 0.0299 - auc: 0.6281 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: 2.4716 - acc: 0.0302 - auc: 0.6318 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: 2.4707 - acc: 0.0301 - auc: 0.6296 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: 2.4701 - acc: 0.0304 - auc: 0.6327 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: 2.4676 - acc: 0.0309 - auc: 0.6323 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: 2.4664 - acc: 0.0306 - auc: 0.6350 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 2.4671 - acc: 0.0304 - auc: 0.6347 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: 2.4663 - acc: 0.0303 - auc: 0.6365 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: 2.4648 - acc: 0.0301 - auc: 0.6402 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: 2.4648 - acc: 0.0298 - auc: 0.6377 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: 2.4659 - acc: 0.0297 - auc: 0.6361 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 2.4647 - acc: 0.0296 - auc: 0.6374 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: 2.4666 - acc: 0.0296 - auc: 0.6317 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: 2.4664 - acc: 0.0295 - auc: 0.6316 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: 2.4649 - acc: 0.0295 - auc: 0.6300 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: 2.4637 - acc: 0.0294 - auc: 0.6310 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: 2.4668 - acc: 0.0293 - auc: 0.6290 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: 2.4657 - acc: 0.0293 - auc: 0.6304 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: 2.4666 - acc: 0.0294 - auc: 0.6283 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: 2.4653 - acc: 0.0291 - auc: 0.6283 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 764/1170 [==================>...........] - ETA: 0s - loss: 2.4646 - acc: 0.0290 - auc: 0.6305 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 2.4655 - acc: 0.0294 - auc: 0.6285 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: 2.4666 - acc: 0.0293 - auc: 0.6277 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: 2.4658 - acc: 0.0289 - auc: 0.6287 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: 2.4666 - acc: 0.0289 - auc: 0.6284 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 2.4685 - acc: 0.0292 - auc: 0.6261 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: 2.4680 - acc: 0.0293 - auc: 0.6279 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: 2.4682 - acc: 0.0293 - auc: 0.6276 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: 2.4677 - acc: 0.0291 - auc: 0.6271 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: 2.4683 - acc: 0.0290 - auc: 0.6260 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: 2.4681 - acc: 0.0290 - auc: 0.6263 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: 2.4676 - acc: 0.0290 - auc: 0.6260 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 2.4676 - acc: 0.0292 - auc: 0.6271 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 2.4680 - acc: 0.0293 - auc: 0.6287 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: 2.4675 - acc: 0.0293 - auc: 0.6290 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: 2.4681 - acc: 0.0295 - auc: 0.6292 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 2.4691 - acc: 0.0295 - auc: 0.6270 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.4685 - acc: 0.0296 - auc: 0.6275 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:54:13.251005                             \n",
            "100%|| 18/18 [04:45<00:00, 285.12s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 18 trials to 19 (+1) trials\n",
            "2023-07-30 20:54:13.400530                             \n",
            "{'name': 'Adam', 'learning_rate': 4.595802195320208e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_18', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_36_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_36', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_36', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_36', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_36', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_37', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_37', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_37', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_37', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_18', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_72', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_54', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_73', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_55', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_74', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_56', 'trainable': True, 'dtype': 'float32', 'rate': 0.1505142618020146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_75', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 3.0551 - acc: 0.0625 - auc: 0.2500 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.9156 - acc: 0.0349 - auc: 0.6372 - precision: 0.0349 - recall: 1.0000 - f1: 0.0650 \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: 2.9285 - acc: 0.0362 - auc: 0.6178 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: 2.9239 - acc: 0.0339 - auc: 0.6332 - precision: 0.0339 - recall: 1.0000 - f1: 0.0634\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: 2.9223 - acc: 0.0309 - auc: 0.6320 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: 2.9203 - acc: 0.0320 - auc: 0.6383 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: 2.9239 - acc: 0.0317 - auc: 0.6424 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: 2.9245 - acc: 0.0314 - auc: 0.6365 - precision: 0.0314 - recall: 1.0000 - f1: 0.0591\n",
            " 208/1170 [====>.........................] - ETA: 1s - loss: 2.9272 - acc: 0.0312 - auc: 0.6220 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 234/1170 [=====>........................] - ETA: 1s - loss: 2.9259 - acc: 0.0304 - auc: 0.6206 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 260/1170 [=====>........................] - ETA: 1s - loss: 2.9248 - acc: 0.0304 - auc: 0.6177 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 298/1170 [======>.......................] - ETA: 1s - loss: 2.9246 - acc: 0.0299 - auc: 0.6209 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 313/1170 [=======>......................] - ETA: 1s - loss: 2.9233 - acc: 0.0297 - auc: 0.6232 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 341/1170 [=======>......................] - ETA: 1s - loss: 2.9218 - acc: 0.0301 - auc: 0.6295 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 378/1170 [========>.....................] - ETA: 1s - loss: 2.9202 - acc: 0.0304 - auc: 0.6341 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 407/1170 [=========>....................] - ETA: 1s - loss: 2.9207 - acc: 0.0309 - auc: 0.6281 - precision: 0.0309 - recall: 1.0000 - f1: 0.0583\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: 2.9189 - acc: 0.0306 - auc: 0.6331 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: 2.9196 - acc: 0.0303 - auc: 0.6333 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: 2.9176 - acc: 0.0300 - auc: 0.6380 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: 2.9185 - acc: 0.0299 - auc: 0.6345 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: 2.9188 - acc: 0.0297 - auc: 0.6315 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: 2.9193 - acc: 0.0298 - auc: 0.6291 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: 2.9187 - acc: 0.0295 - auc: 0.6287 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 644/1170 [===============>..............] - ETA: 0s - loss: 2.9181 - acc: 0.0296 - auc: 0.6285 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 663/1170 [================>.............] - ETA: 0s - loss: 2.9186 - acc: 0.0294 - auc: 0.6289 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 688/1170 [================>.............] - ETA: 0s - loss: 2.9192 - acc: 0.0291 - auc: 0.6287 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 722/1170 [=================>............] - ETA: 0s - loss: 2.9192 - acc: 0.0293 - auc: 0.6270 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: 2.9188 - acc: 0.0291 - auc: 0.6267 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 767/1170 [==================>...........] - ETA: 0s - loss: 2.9188 - acc: 0.0291 - auc: 0.6266 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: 2.9195 - acc: 0.0292 - auc: 0.6277 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: 2.9195 - acc: 0.0291 - auc: 0.6277 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: 2.9200 - acc: 0.0290 - auc: 0.6268 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: 2.9206 - acc: 0.0291 - auc: 0.6270 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: 2.9207 - acc: 0.0293 - auc: 0.6273 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: 2.9209 - acc: 0.0292 - auc: 0.6258 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: 2.9206 - acc: 0.0290 - auc: 0.6239 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: 2.9215 - acc: 0.0290 - auc: 0.6213 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: 2.9217 - acc: 0.0291 - auc: 0.6206 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: 2.9212 - acc: 0.0292 - auc: 0.6228 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: 2.9217 - acc: 0.0293 - auc: 0.6234 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.9216 - acc: 0.0294 - auc: 0.6238 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: 2.9227 - acc: 0.0295 - auc: 0.6207 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 2.9227 - acc: 0.0295 - auc: 0.6199 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.9227 - acc: 0.0296 - auc: 0.6202 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 20:56:28.184947                             \n",
            "100%|| 19/19 [02:14<00:00, 134.85s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 19 trials to 20 (+1) trials\n",
            "2023-07-30 20:56:28.252025                             \n",
            "{'name': 'Adam', 'learning_rate': 4.770349960026938e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_19', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_38_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_38', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_38', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_38', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_38', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_39', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_39', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_39', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_39', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_19', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_76', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_57', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_77', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_58', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_78', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_59', 'trainable': True, 'dtype': 'float32', 'rate': 0.30959141448703625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_79', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 295/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:00:41.810040                             \n",
            "100%|| 20/20 [04:13<00:00, 253.61s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 20 trials to 21 (+1) trials\n",
            "2023-07-30 21:00:41.867242                             \n",
            "{'name': 'Adam', 'learning_rate': 0.02005822208047446, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_20', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_40_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_40', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_40', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_40', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_40', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_41', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_41', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_41', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_41', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_20', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_80', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_60', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_81', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_61', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_82', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_62', 'trainable': True, 'dtype': 'float32', 'rate': 0.16959313148869704, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_83', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.4008 - acc: 0.0625 - auc: 0.5000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  29/1170 [..............................] - ETA: 2s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0564 \n",
            "  39/1170 [>.............................] - ETA: 3s - loss: 1.3654 - acc: 0.0353 - auc: 0.5000 - precision: 0.0353 - recall: 1.0000 - f1: 0.0655\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: 1.3666 - acc: 0.0362 - auc: 0.5000 - precision: 0.0362 - recall: 1.0000 - f1: 0.0675\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: 1.3664 - acc: 0.0360 - auc: 0.5000 - precision: 0.0360 - recall: 1.0000 - f1: 0.0673\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: 1.3611 - acc: 0.0319 - auc: 0.5000 - precision: 0.0319 - recall: 1.0000 - f1: 0.0598\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: 1.3606 - acc: 0.0315 - auc: 0.5000 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: 1.3611 - acc: 0.0320 - auc: 0.5000 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: 1.3620 - acc: 0.0327 - auc: 0.5000 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: 1.3602 - acc: 0.0312 - auc: 0.5000 - precision: 0.0312 - recall: 1.0000 - f1: 0.0588\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: 1.3600 - acc: 0.0311 - auc: 0.5000 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: 1.3595 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: 1.3594 - acc: 0.0307 - auc: 0.5000 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 300/1170 [======>.......................] - ETA: 1s - loss: 1.3584 - acc: 0.0299 - auc: 0.5000 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 326/1170 [=======>......................] - ETA: 1s - loss: 1.3587 - acc: 0.0301 - auc: 0.5000 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 359/1170 [========>.....................] - ETA: 1s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: 1.3590 - acc: 0.0304 - auc: 0.5000 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: 1.3598 - acc: 0.0310 - auc: 0.5000 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: 1.3591 - acc: 0.0304 - auc: 0.5000 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: 1.3588 - acc: 0.0302 - auc: 0.5000 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: 1.3585 - acc: 0.0300 - auc: 0.5000 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: 1.3582 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: 1.3583 - acc: 0.0298 - auc: 0.5000 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: 1.3576 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 671/1170 [================>.............] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 697/1170 [================>.............] - ETA: 0s - loss: 1.3576 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 735/1170 [=================>............] - ETA: 0s - loss: 1.3574 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 1.3574 - acc: 0.0291 - auc: 0.5000 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: 1.3576 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: 1.3572 - acc: 0.0289 - auc: 0.5000 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: 1.3578 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: 1.3573 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: 1.3573 - acc: 0.0290 - auc: 0.5000 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: 1.3571 - acc: 0.0289 - auc: 0.5000 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 1.3575 - acc: 0.0292 - auc: 0.5000 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.3577 - acc: 0.0293 - auc: 0.5000 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: 1.3578 - acc: 0.0294 - auc: 0.5000 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: 1.3580 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: 1.3579 - acc: 0.0295 - auc: 0.5000 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 1.3580 - acc: 0.0296 - auc: 0.5000 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:04:57.715845                             \n",
            "100%|| 21/21 [04:15<00:00, 255.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 21 trials to 22 (+1) trials\n",
            "2023-07-30 21:04:57.771782                             \n",
            "{'name': 'Adam', 'learning_rate': 4.1871180276166344e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_21', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_42_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_42', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_42', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_42', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_42', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_43', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_43', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_43', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_43', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_21', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_84', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_63', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_85', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_64', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_86', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_65', 'trainable': True, 'dtype': 'float32', 'rate': 0.2074693381688909, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_87', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 3.4983 - acc: 0.0625 - auc: 0.1750 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 3.3110 - acc: 0.0339 - auc: 0.6359 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: 3.3275 - acc: 0.0347 - auc: 0.6111 - precision: 0.0347 - recall: 1.0000 - f1: 0.0647\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: 3.3202 - acc: 0.0370 - auc: 0.6303 - precision: 0.0370 - recall: 1.0000 - f1: 0.0690\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: 3.3214 - acc: 0.0360 - auc: 0.6287 - precision: 0.0360 - recall: 1.0000 - f1: 0.0673\n",
            "  90/1170 [=>............................] - ETA: 3s - loss: 3.3157 - acc: 0.0326 - auc: 0.6238 - precision: 0.0326 - recall: 1.0000 - f1: 0.0611\n",
            " 106/1170 [=>............................] - ETA: 3s - loss: 3.3181 - acc: 0.0315 - auc: 0.6283 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 116/1170 [=>............................] - ETA: 3s - loss: 3.3153 - acc: 0.0315 - auc: 0.6268 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 3.3155 - acc: 0.0327 - auc: 0.6435 - precision: 0.0327 - recall: 1.0000 - f1: 0.0616\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: 3.3160 - acc: 0.0316 - auc: 0.6412 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: 3.3170 - acc: 0.0309 - auc: 0.6362 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: 3.3182 - acc: 0.0305 - auc: 0.6293 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: 3.3176 - acc: 0.0306 - auc: 0.6262 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 3.3163 - acc: 0.0307 - auc: 0.6278 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: 3.3169 - acc: 0.0302 - auc: 0.6264 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 3.3154 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: 3.3143 - acc: 0.0304 - auc: 0.6334 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: 3.3130 - acc: 0.0303 - auc: 0.6364 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 382/1170 [========>.....................] - ETA: 1s - loss: 3.3116 - acc: 0.0302 - auc: 0.6379 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: 3.3125 - acc: 0.0310 - auc: 0.6320 - precision: 0.0310 - recall: 1.0000 - f1: 0.0584\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: 3.3120 - acc: 0.0308 - auc: 0.6348 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: 3.3120 - acc: 0.0305 - auc: 0.6376 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: 3.3122 - acc: 0.0303 - auc: 0.6396 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: 3.3118 - acc: 0.0299 - auc: 0.6369 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: 3.3128 - acc: 0.0299 - auc: 0.6334 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: 3.3124 - acc: 0.0297 - auc: 0.6350 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 3.3131 - acc: 0.0295 - auc: 0.6314 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: 3.3134 - acc: 0.0296 - auc: 0.6309 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: 3.3122 - acc: 0.0294 - auc: 0.6306 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: 3.3122 - acc: 0.0294 - auc: 0.6307 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 3.3136 - acc: 0.0292 - auc: 0.6298 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 3.3133 - acc: 0.0292 - auc: 0.6298 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: 3.3129 - acc: 0.0293 - auc: 0.6292 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: 3.3123 - acc: 0.0291 - auc: 0.6302 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: 3.3134 - acc: 0.0293 - auc: 0.6298 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: 3.3146 - acc: 0.0291 - auc: 0.6277 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: 3.3155 - acc: 0.0290 - auc: 0.6272 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: 3.3152 - acc: 0.0291 - auc: 0.6282 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: 3.3164 - acc: 0.0293 - auc: 0.6266 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: 3.3168 - acc: 0.0294 - auc: 0.6250 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: 3.3171 - acc: 0.0290 - auc: 0.6237 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: 3.3167 - acc: 0.0291 - auc: 0.6223 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: 3.3177 - acc: 0.0290 - auc: 0.6205 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: 3.3175 - acc: 0.0289 - auc: 0.6204 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 3.3172 - acc: 0.0292 - auc: 0.6231 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: 3.3172 - acc: 0.0293 - auc: 0.6241 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 3.3182 - acc: 0.0295 - auc: 0.6240 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: 3.3189 - acc: 0.0296 - auc: 0.6208 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 3.3187 - acc: 0.0295 - auc: 0.6207 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 3.3187 - acc: 0.0296 - auc: 0.6211 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:07:16.683478                             \n",
            "100%|| 22/22 [02:18<00:00, 138.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 22 trials to 23 (+1) trials\n",
            "2023-07-30 21:07:16.739145                             \n",
            "{'name': 'Adam', 'learning_rate': 6.502045086878626e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_22', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_44_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_44', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_44', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_44', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_44', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_45', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_45', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_45', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_45', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_22', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_88', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_66', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_89', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_67', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_90', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_68', 'trainable': True, 'dtype': 'float32', 'rate': 0.31297617212478634, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_91', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.3707 - acc: 0.0625 - auc: 0.1917 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 2.1581 - acc: 0.0323 - auc: 0.6266 - precision: 0.0323 - recall: 1.0000 - f1: 0.0602 \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: 2.1644 - acc: 0.0349 - auc: 0.6363 - precision: 0.0349 - recall: 1.0000 - f1: 0.0649\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: 2.1720 - acc: 0.0361 - auc: 0.6225 - precision: 0.0361 - recall: 1.0000 - f1: 0.0673\n",
            "  73/1170 [>.............................] - ETA: 3s - loss: 2.1666 - acc: 0.0355 - auc: 0.6385 - precision: 0.0355 - recall: 1.0000 - f1: 0.0664\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: 2.1560 - acc: 0.0316 - auc: 0.6449 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: 2.1576 - acc: 0.0315 - auc: 0.6457 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.1548 - acc: 0.0320 - auc: 0.6544 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: 2.1590 - acc: 0.0326 - auc: 0.6560 - precision: 0.0326 - recall: 1.0000 - f1: 0.0614\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: 2.1588 - acc: 0.0311 - auc: 0.6548 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: 2.1605 - acc: 0.0312 - auc: 0.6502 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: 2.1618 - acc: 0.0304 - auc: 0.6404 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: 2.1608 - acc: 0.0301 - auc: 0.6433 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: 2.1604 - acc: 0.0301 - auc: 0.6422 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: 2.1623 - acc: 0.0299 - auc: 0.6356 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: 2.1598 - acc: 0.0297 - auc: 0.6369 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: 2.1590 - acc: 0.0301 - auc: 0.6394 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 371/1170 [========>.....................] - ETA: 1s - loss: 2.1578 - acc: 0.0302 - auc: 0.6439 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: 2.1576 - acc: 0.0309 - auc: 0.6438 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: 2.1576 - acc: 0.0309 - auc: 0.6414 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 2.1573 - acc: 0.0303 - auc: 0.6432 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: 2.1572 - acc: 0.0303 - auc: 0.6426 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: 2.1568 - acc: 0.0300 - auc: 0.6449 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 2.1552 - acc: 0.0299 - auc: 0.6455 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 2.1567 - acc: 0.0297 - auc: 0.6414 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 2.1565 - acc: 0.0296 - auc: 0.6420 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: 2.1574 - acc: 0.0298 - auc: 0.6375 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: 2.1567 - acc: 0.0295 - auc: 0.6365 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 2.1553 - acc: 0.0296 - auc: 0.6363 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 2.1571 - acc: 0.0292 - auc: 0.6348 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: 2.1570 - acc: 0.0294 - auc: 0.6354 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 743/1170 [==================>...........] - ETA: 0s - loss: 2.1565 - acc: 0.0291 - auc: 0.6355 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: 2.1570 - acc: 0.0291 - auc: 0.6339 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 2.1577 - acc: 0.0294 - auc: 0.6326 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: 2.1586 - acc: 0.0291 - auc: 0.6309 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: 2.1581 - acc: 0.0288 - auc: 0.6317 - precision: 0.0288 - recall: 1.0000 - f1: 0.0543\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: 2.1590 - acc: 0.0292 - auc: 0.6320 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.1595 - acc: 0.0293 - auc: 0.6322 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 2.1589 - acc: 0.0292 - auc: 0.6328 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: 2.1590 - acc: 0.0292 - auc: 0.6319 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: 2.1587 - acc: 0.0290 - auc: 0.6319 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: 2.1601 - acc: 0.0291 - auc: 0.6291 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: 2.1590 - acc: 0.0290 - auc: 0.6299 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: 2.1591 - acc: 0.0293 - auc: 0.6331 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.1586 - acc: 0.0294 - auc: 0.6358 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: 2.1598 - acc: 0.0296 - auc: 0.6331 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: 2.1602 - acc: 0.0295 - auc: 0.6307 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.1600 - acc: 0.0296 - auc: 0.6320 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:09:35.687468                             \n",
            "100%|| 23/23 [02:19<00:00, 139.00s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 23 trials to 24 (+1) trials\n",
            "2023-07-30 21:09:35.744275                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0003409035850234096, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_23', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_46_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_46', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_46', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_46', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_46', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_47', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_47', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_47', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_47', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_23', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_92', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_69', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_93', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_70', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_94', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_71', 'trainable': True, 'dtype': 'float32', 'rate': 0.20538823078878699, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_95', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: 0.6453 - acc: 0.9375 - auc: 0.1000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  37/1170 [..............................] - ETA: 1s - loss: 0.5459 - acc: 0.9645 - auc: 0.6781 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: 0.5501 - acc: 0.9622 - auc: 0.6706 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: 0.5497 - acc: 0.9640 - auc: 0.6533 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: 0.5425 - acc: 0.9688 - auc: 0.6599 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: 0.5427 - acc: 0.9682 - auc: 0.6586 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: 0.5422 - acc: 0.9680 - auc: 0.6682 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: 0.5450 - acc: 0.9666 - auc: 0.6627 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: 0.5428 - acc: 0.9686 - auc: 0.6591 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: 0.5425 - acc: 0.9688 - auc: 0.6576 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: 0.5421 - acc: 0.9695 - auc: 0.6491 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: 0.5420 - acc: 0.9695 - auc: 0.6397 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 283/1170 [======>.......................] - ETA: 1s - loss: 0.5418 - acc: 0.9699 - auc: 0.6453 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 307/1170 [======>.......................] - ETA: 1s - loss: 0.5425 - acc: 0.9702 - auc: 0.6418 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: 0.5420 - acc: 0.9697 - auc: 0.6452 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: 0.5415 - acc: 0.9697 - auc: 0.6497 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 0.5416 - acc: 0.9695 - auc: 0.6485 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: 0.5420 - acc: 0.9690 - auc: 0.6450 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: 0.5413 - acc: 0.9696 - auc: 0.6458 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 0.5415 - acc: 0.9697 - auc: 0.6447 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: 0.5410 - acc: 0.9700 - auc: 0.6478 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: 0.5407 - acc: 0.9700 - auc: 0.6470 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: 0.5408 - acc: 0.9703 - auc: 0.6440 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: 0.5408 - acc: 0.9702 - auc: 0.6430 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: 0.5410 - acc: 0.9703 - auc: 0.6405 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 0.5407 - acc: 0.9705 - auc: 0.6417 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: 0.5406 - acc: 0.9707 - auc: 0.6382 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 0.5405 - acc: 0.9704 - auc: 0.6383 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: 0.5406 - acc: 0.9706 - auc: 0.6385 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: 0.5408 - acc: 0.9708 - auc: 0.6378 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: 0.5407 - acc: 0.9707 - auc: 0.6379 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 727/1170 [=================>............] - ETA: 0s - loss: 0.5407 - acc: 0.9708 - auc: 0.6368 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 757/1170 [==================>...........] - ETA: 0s - loss: 0.5406 - acc: 0.9709 - auc: 0.6369 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: 0.5409 - acc: 0.9708 - auc: 0.6362 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: 0.5411 - acc: 0.9708 - auc: 0.6354 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: 0.5411 - acc: 0.9710 - auc: 0.6347 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: 0.5413 - acc: 0.9710 - auc: 0.6348 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 0.5416 - acc: 0.9708 - auc: 0.6345 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: 0.5420 - acc: 0.9706 - auc: 0.6359 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: 0.5417 - acc: 0.9709 - auc: 0.6342 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 0.5415 - acc: 0.9709 - auc: 0.6350 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: 0.5417 - acc: 0.9710 - auc: 0.6323 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: 0.5415 - acc: 0.9710 - auc: 0.6356 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: 0.5417 - acc: 0.9708 - auc: 0.6345 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: 0.5419 - acc: 0.9707 - auc: 0.6370 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: 0.5418 - acc: 0.9707 - auc: 0.6372 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 0.5424 - acc: 0.9705 - auc: 0.6364 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.9704 - auc: 0.6342 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 0.5424 - acc: 0.9704 - auc: 0.6344 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "\n",
            "2023-07-30 21:11:30.955394                             \n",
            "100%|| 24/24 [01:55<00:00, 115.27s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 24 trials to 25 (+1) trials\n",
            "2023-07-30 21:11:31.014670                             \n",
            "{'name': 'Adam', 'learning_rate': 5.545735597680928e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_24', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_48_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_48', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_48', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_48', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_48', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_49', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_49', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_49', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_49', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_24', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_96', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_72', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_97', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_73', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_98', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_74', 'trainable': True, 'dtype': 'float32', 'rate': 0.3518232218889367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_99', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 2.7473 - acc: 0.0625 - auc: 0.1083 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  37/1170 [..............................] - ETA: 1s - loss: 2.4859 - acc: 0.0355 - auc: 0.6309 - precision: 0.0355 - recall: 1.0000 - f1: 0.0657 \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: 2.4967 - acc: 0.0362 - auc: 0.6149 - precision: 0.0362 - recall: 1.0000 - f1: 0.0675\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: 2.4914 - acc: 0.0365 - auc: 0.6367 - precision: 0.0365 - recall: 1.0000 - f1: 0.0683\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: 2.4804 - acc: 0.0326 - auc: 0.6417 - precision: 0.0326 - recall: 1.0000 - f1: 0.0611\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: 2.4809 - acc: 0.0315 - auc: 0.6468 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 116/1170 [=>............................] - ETA: 2s - loss: 2.4783 - acc: 0.0315 - auc: 0.6478 - precision: 0.0315 - recall: 1.0000 - f1: 0.0592\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: 2.4786 - acc: 0.0319 - auc: 0.6628 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: 2.4814 - acc: 0.0320 - auc: 0.6561 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: 2.4843 - acc: 0.0314 - auc: 0.6515 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: 2.4815 - acc: 0.0311 - auc: 0.6527 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: 2.4858 - acc: 0.0307 - auc: 0.6410 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: 2.4847 - acc: 0.0304 - auc: 0.6375 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 2.4836 - acc: 0.0301 - auc: 0.6403 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: 2.4853 - acc: 0.0297 - auc: 0.6329 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 2.4849 - acc: 0.0301 - auc: 0.6371 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: 2.4828 - acc: 0.0303 - auc: 0.6388 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: 2.4829 - acc: 0.0303 - auc: 0.6362 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: 2.4823 - acc: 0.0309 - auc: 0.6361 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: 2.4817 - acc: 0.0303 - auc: 0.6395 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: 2.4824 - acc: 0.0301 - auc: 0.6401 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: 2.4815 - acc: 0.0301 - auc: 0.6420 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: 2.4796 - acc: 0.0300 - auc: 0.6419 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: 2.4804 - acc: 0.0296 - auc: 0.6392 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: 2.4811 - acc: 0.0296 - auc: 0.6379 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: 2.4825 - acc: 0.0297 - auc: 0.6362 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: 2.4813 - acc: 0.0295 - auc: 0.6369 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: 2.4810 - acc: 0.0295 - auc: 0.6340 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: 2.4803 - acc: 0.0295 - auc: 0.6348 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: 2.4828 - acc: 0.0292 - auc: 0.6345 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: 2.4821 - acc: 0.0293 - auc: 0.6338 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 740/1170 [=================>............] - ETA: 0s - loss: 2.4821 - acc: 0.0291 - auc: 0.6330 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 774/1170 [==================>...........] - ETA: 0s - loss: 2.4828 - acc: 0.0292 - auc: 0.6322 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: 2.4851 - acc: 0.0293 - auc: 0.6296 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: 2.4844 - acc: 0.0290 - auc: 0.6307 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: 2.4855 - acc: 0.0292 - auc: 0.6296 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: 2.4855 - acc: 0.0295 - auc: 0.6297 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: 2.4856 - acc: 0.0292 - auc: 0.6291 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: 2.4856 - acc: 0.0292 - auc: 0.6277 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: 2.4857 - acc: 0.0290 - auc: 0.6283 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 2.4866 - acc: 0.0290 - auc: 0.6264 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: 2.4859 - acc: 0.0292 - auc: 0.6276 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: 2.4858 - acc: 0.0293 - auc: 0.6302 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: 2.4855 - acc: 0.0294 - auc: 0.6314 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 2.4867 - acc: 0.0295 - auc: 0.6308 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: 2.4872 - acc: 0.0296 - auc: 0.6283 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: 2.4870 - acc: 0.0295 - auc: 0.6281 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: 2.4868 - acc: 0.0296 - auc: 0.6289 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:13:36.471941                             \n",
            "100%|| 25/25 [02:05<00:00, 125.51s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 25 trials to 26 (+1) trials\n",
            "2023-07-30 21:13:36.531085                             \n",
            "{'name': 'Adam', 'learning_rate': 4.085211163979461e-05, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_25', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_50_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_50', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_50', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_50', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_50', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_51', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_51', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_51', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_51', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_25', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_100', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_75', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_101', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_76', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_102', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_77', 'trainable': True, 'dtype': 'float32', 'rate': 0.17512743599562558, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_103', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.8521 - acc: 0.0625 - auc: 0.2583 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 2.6500 - acc: 0.0331 - auc: 0.6171 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: 2.6665 - acc: 0.0370 - auc: 0.6220 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  68/1170 [>.............................] - ETA: 2s - loss: 2.6775 - acc: 0.0363 - auc: 0.6173 - precision: 0.0363 - recall: 1.0000 - f1: 0.0679\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: 2.6655 - acc: 0.0319 - auc: 0.6289 - precision: 0.0319 - recall: 1.0000 - f1: 0.0598\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: 2.6598 - acc: 0.0323 - auc: 0.6411 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: 2.6656 - acc: 0.0326 - auc: 0.6467 - precision: 0.0326 - recall: 1.0000 - f1: 0.0614\n",
            " 188/1170 [===>..........................] - ETA: 1s - loss: 2.6671 - acc: 0.0311 - auc: 0.6409 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 214/1170 [====>.........................] - ETA: 1s - loss: 2.6694 - acc: 0.0310 - auc: 0.6352 - precision: 0.0310 - recall: 1.0000 - f1: 0.0582\n",
            " 231/1170 [====>.........................] - ETA: 1s - loss: 2.6690 - acc: 0.0306 - auc: 0.6316 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 263/1170 [=====>........................] - ETA: 1s - loss: 2.6667 - acc: 0.0307 - auc: 0.6338 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 284/1170 [======>.......................] - ETA: 1s - loss: 2.6674 - acc: 0.0303 - auc: 0.6355 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 296/1170 [======>.......................] - ETA: 1s - loss: 2.6705 - acc: 0.0301 - auc: 0.6300 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 312/1170 [=======>......................] - ETA: 1s - loss: 2.6692 - acc: 0.0295 - auc: 0.6316 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 330/1170 [=======>......................] - ETA: 1s - loss: 2.6679 - acc: 0.0301 - auc: 0.6333 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 354/1170 [========>.....................] - ETA: 1s - loss: 2.6667 - acc: 0.0304 - auc: 0.6329 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.6665 - acc: 0.0303 - auc: 0.6330 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: 2.6652 - acc: 0.0309 - auc: 0.6346 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: 2.6631 - acc: 0.0310 - auc: 0.6362 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: 2.6651 - acc: 0.0304 - auc: 0.6341 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: 2.6662 - acc: 0.0303 - auc: 0.6348 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: 2.6667 - acc: 0.0302 - auc: 0.6375 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: 2.6653 - acc: 0.0299 - auc: 0.6373 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: 2.6658 - acc: 0.0298 - auc: 0.6350 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: 2.6661 - acc: 0.0299 - auc: 0.6314 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: 2.6656 - acc: 0.0297 - auc: 0.6312 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: 2.6668 - acc: 0.0295 - auc: 0.6273 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 2.6668 - acc: 0.0293 - auc: 0.6262 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 2.6655 - acc: 0.0295 - auc: 0.6271 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: 2.6683 - acc: 0.0293 - auc: 0.6249 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: 2.6684 - acc: 0.0292 - auc: 0.6262 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: 2.6672 - acc: 0.0294 - auc: 0.6243 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: 2.6668 - acc: 0.0291 - auc: 0.6236 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: 2.6661 - acc: 0.0291 - auc: 0.6254 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: 2.6665 - acc: 0.0293 - auc: 0.6249 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: 2.6687 - acc: 0.0293 - auc: 0.6223 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: 2.6682 - acc: 0.0289 - auc: 0.6233 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.6704 - acc: 0.0291 - auc: 0.6219 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 2.6709 - acc: 0.0292 - auc: 0.6212 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: 2.6704 - acc: 0.0294 - auc: 0.6211 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: 2.6695 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: 2.6699 - acc: 0.0290 - auc: 0.6206 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: 2.6707 - acc: 0.0290 - auc: 0.6187 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: 2.6693 - acc: 0.0291 - auc: 0.6207 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: 2.6696 - acc: 0.0293 - auc: 0.6225 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: 2.6687 - acc: 0.0293 - auc: 0.6237 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: 2.6687 - acc: 0.0295 - auc: 0.6248 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: 2.6704 - acc: 0.0295 - auc: 0.6213 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: 2.6696 - acc: 0.0295 - auc: 0.6212 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.6697 - acc: 0.0296 - auc: 0.6212 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:16:37.407854                             \n",
            "100%|| 26/26 [03:00<00:00, 180.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 26 trials to 27 (+1) trials\n",
            "2023-07-30 21:16:37.470560                             \n",
            "{'name': 'Adam', 'learning_rate': 1.0614885222476534e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_26', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_52_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_52', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_52', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_52', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_52', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_53', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_53', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_53', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_53', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_26', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_104', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_78', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_105', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_79', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_106', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_80', 'trainable': True, 'dtype': 'float32', 'rate': 0.25700183984243347, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_107', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.5596 - acc: 0.9062 - auc: 0.1917 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  31/1170 [..............................] - ETA: 2s - loss: 1.4715 - acc: 0.9627 - auc: 0.5949 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00 \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: 1.4770 - acc: 0.9598 - auc: 0.6117 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  60/1170 [>.............................] - ETA: 3s - loss: 1.4801 - acc: 0.9583 - auc: 0.6361 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  71/1170 [>.............................] - ETA: 3s - loss: 1.4801 - acc: 0.9586 - auc: 0.6339 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: 1.4754 - acc: 0.9601 - auc: 0.6301 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: 1.4743 - acc: 0.9622 - auc: 0.6275 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: 1.4737 - acc: 0.9622 - auc: 0.6385 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 1.4755 - acc: 0.9617 - auc: 0.6419 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: 1.4744 - acc: 0.9622 - auc: 0.6358 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: 0.0000e+00\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: 1.4750 - acc: 0.9632 - auc: 0.6304 - precision: 0.0263 - recall: 0.0050 - f1: 0.0025            \n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 1.4741 - acc: 0.9635 - auc: 0.6227 - precision: 0.0227 - recall: 0.0045 - f1: 0.0022\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: 1.4737 - acc: 0.9637 - auc: 0.6194 - precision: 0.0204 - recall: 0.0042 - f1: 0.0020\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: 1.4740 - acc: 0.9636 - auc: 0.6184 - precision: 0.0204 - recall: 0.0040 - f1: 0.0019\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: 1.4728 - acc: 0.9640 - auc: 0.6201 - precision: 0.0196 - recall: 0.0038 - f1: 0.0019\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: 1.4730 - acc: 0.9642 - auc: 0.6209 - precision: 0.0192 - recall: 0.0038 - f1: 0.0018\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: 1.4731 - acc: 0.9643 - auc: 0.6201 - precision: 0.0189 - recall: 0.0036 - f1: 0.0017\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: 1.4721 - acc: 0.9652 - auc: 0.6233 - precision: 0.0185 - recall: 0.0034 - f1: 0.0016\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: 1.4723 - acc: 0.9649 - auc: 0.6286 - precision: 0.0351 - recall: 0.0063 - f1: 0.0030\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: 1.4722 - acc: 0.9647 - auc: 0.6282 - precision: 0.0333 - recall: 0.0059 - f1: 0.0028\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: 1.4719 - acc: 0.9648 - auc: 0.6284 - precision: 0.0317 - recall: 0.0055 - f1: 0.0027\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: 1.4719 - acc: 0.9646 - auc: 0.6307 - precision: 0.0441 - recall: 0.0078 - f1: 0.0051\n",
            " 420/1170 [=========>....................] - ETA: 2s - loss: 1.4723 - acc: 0.9644 - auc: 0.6273 - precision: 0.0435 - recall: 0.0072 - f1: 0.0048\n",
            " 441/1170 [==========>...................] - ETA: 2s - loss: 1.4719 - acc: 0.9649 - auc: 0.6289 - precision: 0.0423 - recall: 0.0070 - f1: 0.0045\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: 1.4720 - acc: 0.9651 - auc: 0.6313 - precision: 0.0526 - recall: 0.0087 - f1: 0.0063\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: 1.4714 - acc: 0.9653 - auc: 0.6312 - precision: 0.0488 - recall: 0.0084 - f1: 0.0060\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: 1.4713 - acc: 0.9654 - auc: 0.6283 - precision: 0.0449 - recall: 0.0079 - f1: 0.0057\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: 1.4716 - acc: 0.9653 - auc: 0.6252 - precision: 0.0625 - recall: 0.0115 - f1: 0.0080\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: 1.4714 - acc: 0.9656 - auc: 0.6237 - precision: 0.0619 - recall: 0.0110 - f1: 0.0077\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: 1.4712 - acc: 0.9660 - auc: 0.6210 - precision: 0.0619 - recall: 0.0107 - f1: 0.0074\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: 1.4713 - acc: 0.9660 - auc: 0.6214 - precision: 0.0612 - recall: 0.0103 - f1: 0.0072\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: 1.4708 - acc: 0.9664 - auc: 0.6211 - precision: 0.0612 - recall: 0.0103 - f1: 0.0071\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: 1.4709 - acc: 0.9662 - auc: 0.6216 - precision: 0.0606 - recall: 0.0101 - f1: 0.0070\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: 1.4710 - acc: 0.9662 - auc: 0.6225 - precision: 0.0600 - recall: 0.0098 - f1: 0.0068\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: 1.4708 - acc: 0.9664 - auc: 0.6225 - precision: 0.0561 - recall: 0.0095 - f1: 0.0065\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: 1.4709 - acc: 0.9664 - auc: 0.6233 - precision: 0.0550 - recall: 0.0091 - f1: 0.0063\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: 1.4707 - acc: 0.9666 - auc: 0.6206 - precision: 0.0531 - recall: 0.0087 - f1: 0.0060\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: 1.4706 - acc: 0.9666 - auc: 0.6223 - precision: 0.0517 - recall: 0.0085 - f1: 0.0058\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: 1.4709 - acc: 0.9664 - auc: 0.6221 - precision: 0.0500 - recall: 0.0082 - f1: 0.0056\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: 1.4714 - acc: 0.9664 - auc: 0.6190 - precision: 0.0492 - recall: 0.0079 - f1: 0.0054\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: 1.4710 - acc: 0.9669 - auc: 0.6196 - precision: 0.0480 - recall: 0.0077 - f1: 0.0052\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: 1.4713 - acc: 0.9668 - auc: 0.6204 - precision: 0.0551 - recall: 0.0086 - f1: 0.0056\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: 1.4718 - acc: 0.9666 - auc: 0.6191 - precision: 0.0534 - recall: 0.0083 - f1: 0.0055\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: 1.4718 - acc: 0.9666 - auc: 0.6180 - precision: 0.0530 - recall: 0.0081 - f1: 0.0054\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: 1.4715 - acc: 0.9668 - auc: 0.6175 - precision: 0.0522 - recall: 0.0080 - f1: 0.0052\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: 1.4713 - acc: 0.9670 - auc: 0.6159 - precision: 0.0519 - recall: 0.0078 - f1: 0.0051\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 1.4716 - acc: 0.9670 - auc: 0.6139 - precision: 0.0511 - recall: 0.0077 - f1: 0.0050\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: 1.4715 - acc: 0.9670 - auc: 0.6160 - precision: 0.0567 - recall: 0.0086 - f1: 0.0056\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: 1.4718 - acc: 0.9670 - auc: 0.6138 - precision: 0.0556 - recall: 0.0083 - f1: 0.0054\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: 1.4720 - acc: 0.9667 - auc: 0.6161 - precision: 0.0541 - recall: 0.0080 - f1: 0.0052\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: 1.4721 - acc: 0.9668 - auc: 0.6171 - precision: 0.0596 - recall: 0.0088 - f1: 0.0057\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: 1.4722 - acc: 0.9667 - auc: 0.6178 - precision: 0.0588 - recall: 0.0086 - f1: 0.0057\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: 1.4725 - acc: 0.9667 - auc: 0.6145 - precision: 0.0573 - recall: 0.0085 - f1: 0.0055\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.4725 - acc: 0.9666 - auc: 0.6133 - precision: 0.0549 - recall: 0.0082 - f1: 0.0054\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.4726 - acc: 0.9665 - auc: 0.6138 - precision: 0.0539 - recall: 0.0081 - f1: 0.0053\n",
            "\n",
            "2023-07-30 21:19:54.910569                             \n",
            "100%|| 27/27 [03:17<00:00, 197.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 27 trials to 28 (+1) trials\n",
            "2023-07-30 21:19:54.969055                             \n",
            "{'name': 'Adam', 'learning_rate': 3.2582018492596743e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_27', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_54_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_54', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_54', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_54', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_54', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_55', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_55', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_55', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_55', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_27', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_108', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_81', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_109', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_82', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_110', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_83', 'trainable': True, 'dtype': 'float32', 'rate': 0.07738458343360594, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_111', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 2.5025 - acc: 0.0625 - auc: 0.1833 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  30/1170 [..............................] - ETA: 2s - loss: 2.2945 - acc: 0.0323 - auc: 0.6352 - precision: 0.0323 - recall: 1.0000 - f1: 0.0602 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.3040 - acc: 0.0353 - auc: 0.6470 - precision: 0.0353 - recall: 1.0000 - f1: 0.0658\n",
            "  64/1170 [>.............................] - ETA: 2s - loss: 2.3049 - acc: 0.0366 - auc: 0.6288 - precision: 0.0366 - recall: 1.0000 - f1: 0.0684\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: 2.3053 - acc: 0.0354 - auc: 0.6404 - precision: 0.0354 - recall: 1.0000 - f1: 0.0662\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: 2.2980 - acc: 0.0312 - auc: 0.6384 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 2.2999 - acc: 0.0323 - auc: 0.6432 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: 2.2991 - acc: 0.0317 - auc: 0.6489 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: 2.2995 - acc: 0.0329 - auc: 0.6441 - precision: 0.0329 - recall: 1.0000 - f1: 0.0618\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: 2.3014 - acc: 0.0318 - auc: 0.6429 - precision: 0.0318 - recall: 1.0000 - f1: 0.0597\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: 2.3004 - acc: 0.0311 - auc: 0.6442 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: 2.3009 - acc: 0.0312 - auc: 0.6434 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: 2.3019 - acc: 0.0306 - auc: 0.6354 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: 2.3003 - acc: 0.0307 - auc: 0.6344 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: 2.3006 - acc: 0.0301 - auc: 0.6360 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: 2.2995 - acc: 0.0297 - auc: 0.6358 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: 2.2975 - acc: 0.0300 - auc: 0.6417 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: 2.2967 - acc: 0.0302 - auc: 0.6430 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: 2.2961 - acc: 0.0302 - auc: 0.6426 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: 2.2952 - acc: 0.0310 - auc: 0.6416 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 2.2953 - acc: 0.0303 - auc: 0.6439 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: 2.2955 - acc: 0.0302 - auc: 0.6445 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 2.2939 - acc: 0.0299 - auc: 0.6456 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: 2.2951 - acc: 0.0297 - auc: 0.6420 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: 2.2943 - acc: 0.0296 - auc: 0.6415 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 2.2946 - acc: 0.0295 - auc: 0.6385 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: 2.2950 - acc: 0.0293 - auc: 0.6358 - precision: 0.0293 - recall: 1.0000 - f1: 0.0554\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: 2.2942 - acc: 0.0296 - auc: 0.6361 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: 2.2948 - acc: 0.0294 - auc: 0.6365 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: 2.2960 - acc: 0.0291 - auc: 0.6355 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: 2.2955 - acc: 0.0294 - auc: 0.6359 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 726/1170 [=================>............] - ETA: 0s - loss: 2.2957 - acc: 0.0292 - auc: 0.6348 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 750/1170 [==================>...........] - ETA: 0s - loss: 2.2944 - acc: 0.0292 - auc: 0.6371 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: 2.2958 - acc: 0.0292 - auc: 0.6343 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: 2.2966 - acc: 0.0291 - auc: 0.6332 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: 2.2965 - acc: 0.0289 - auc: 0.6338 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.2978 - acc: 0.0291 - auc: 0.6326 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: 2.2976 - acc: 0.0291 - auc: 0.6330 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: 2.2978 - acc: 0.0294 - auc: 0.6338 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: 2.2976 - acc: 0.0292 - auc: 0.6324 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: 2.2975 - acc: 0.0292 - auc: 0.6312 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: 2.2979 - acc: 0.0290 - auc: 0.6296 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: 2.2976 - acc: 0.0290 - auc: 0.6305 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: 2.2976 - acc: 0.0292 - auc: 0.6297 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: 2.2978 - acc: 0.0293 - auc: 0.6318 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: 2.2981 - acc: 0.0294 - auc: 0.6329 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.2977 - acc: 0.0295 - auc: 0.6343 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: 2.2990 - acc: 0.0295 - auc: 0.6313 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 2.2992 - acc: 0.0295 - auc: 0.6297 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: 2.2988 - acc: 0.0296 - auc: 0.6302 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 2.2990 - acc: 0.0296 - auc: 0.6300 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:26:12.093219                             \n",
            "100%|| 28/28 [06:17<00:00, 377.18s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 28 trials to 29 (+1) trials\n",
            "2023-07-30 21:26:12.151615                             \n",
            "{'name': 'Adam', 'learning_rate': 0.001416210576066268, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_28', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_56_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_56', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_56', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_56', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_56', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_57', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_57', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_57', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_57', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_28', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_112', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_84', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_113', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_85', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_114', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_86', 'trainable': True, 'dtype': 'float32', 'rate': 0.06359165961443586, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_115', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: 1.9222 - acc: 0.0625 - auc: 0.2667 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 2s - loss: 1.7068 - acc: 0.0331 - auc: 0.6559 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  43/1170 [>.............................] - ETA: 3s - loss: 1.7224 - acc: 0.0349 - auc: 0.6312 - precision: 0.0349 - recall: 1.0000 - f1: 0.0649\n",
            "  62/1170 [>.............................] - ETA: 3s - loss: 1.7289 - acc: 0.0363 - auc: 0.6270 - precision: 0.0363 - recall: 1.0000 - f1: 0.0677\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: 1.7226 - acc: 0.0335 - auc: 0.6333 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: 1.7153 - acc: 0.0316 - auc: 0.6318 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 1.7147 - acc: 0.0323 - auc: 0.6400 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: 1.7147 - acc: 0.0319 - auc: 0.6468 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: 1.7183 - acc: 0.0329 - auc: 0.6382 - precision: 0.0329 - recall: 1.0000 - f1: 0.0618\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: 1.7159 - acc: 0.0314 - auc: 0.6379 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: 1.7153 - acc: 0.0312 - auc: 0.6349 - precision: 0.0312 - recall: 1.0000 - f1: 0.0587\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 1.7156 - acc: 0.0307 - auc: 0.6250 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: 1.7149 - acc: 0.0306 - auc: 0.6264 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 1.7139 - acc: 0.0307 - auc: 0.6295 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: 1.7126 - acc: 0.0302 - auc: 0.6322 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: 1.7111 - acc: 0.0295 - auc: 0.6314 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: 1.7123 - acc: 0.0302 - auc: 0.6322 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 366/1170 [========>.....................] - ETA: 1s - loss: 1.7115 - acc: 0.0304 - auc: 0.6355 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: 1.7117 - acc: 0.0305 - auc: 0.6338 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: 1.7126 - acc: 0.0309 - auc: 0.6321 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: 1.7109 - acc: 0.0303 - auc: 0.6352 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: 1.7114 - acc: 0.0305 - auc: 0.6353 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: 1.7099 - acc: 0.0301 - auc: 0.6366 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: 1.7090 - acc: 0.0299 - auc: 0.6370 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: 1.7093 - acc: 0.0297 - auc: 0.6344 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: 1.7088 - acc: 0.0296 - auc: 0.6343 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: 1.7092 - acc: 0.0297 - auc: 0.6333 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 1.7082 - acc: 0.0295 - auc: 0.6342 - precision: 0.0295 - recall: 1.0000 - f1: 0.0558\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: 1.7077 - acc: 0.0292 - auc: 0.6320 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: 1.7070 - acc: 0.0294 - auc: 0.6331 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: 1.7081 - acc: 0.0295 - auc: 0.6321 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 1.7090 - acc: 0.0292 - auc: 0.6309 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: 1.7086 - acc: 0.0293 - auc: 0.6323 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: 1.7085 - acc: 0.0291 - auc: 0.6309 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: 1.7084 - acc: 0.0290 - auc: 0.6314 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: 1.7093 - acc: 0.0293 - auc: 0.6313 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: 1.7095 - acc: 0.0291 - auc: 0.6295 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: 1.7094 - acc: 0.0290 - auc: 0.6284 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: 1.7098 - acc: 0.0290 - auc: 0.6275 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: 1.7100 - acc: 0.0291 - auc: 0.6270 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: 1.7104 - acc: 0.0292 - auc: 0.6270 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: 1.7107 - acc: 0.0294 - auc: 0.6275 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: 1.7101 - acc: 0.0290 - auc: 0.6263 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 1.7100 - acc: 0.0291 - auc: 0.6262 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: 1.7106 - acc: 0.0291 - auc: 0.6238 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 1.7100 - acc: 0.0290 - auc: 0.6261 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: 1.7097 - acc: 0.0291 - auc: 0.6268 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: 1.7097 - acc: 0.0292 - auc: 0.6287 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 1.7102 - acc: 0.0293 - auc: 0.6295 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: 1.7106 - acc: 0.0295 - auc: 0.6309 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: 1.7116 - acc: 0.0295 - auc: 0.6286 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.7112 - acc: 0.0295 - auc: 0.6284 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.7114 - acc: 0.0296 - auc: 0.6287 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 21:27:50.511353                             \n",
            "100%|| 29/29 [01:38<00:00, 98.41s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 29 trials to 30 (+1) trials\n",
            "2023-07-30 21:27:50.574522                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5037537639192469, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_29', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_58_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_58', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_58', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_58', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_58', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_59', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_59', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_59', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_59', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_29', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_116', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_87', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_117', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_88', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_118', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_89', 'trainable': True, 'dtype': 'float32', 'rate': 0.009850274712180945, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_119', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:28:50.063158                             \n",
            "100%|| 30/30 [00:59<00:00, 59.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 30 trials to 31 (+1) trials\n",
            "2023-07-30 21:28:50.122606                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2134877090302406, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_30', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_60_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_60', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_60', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_60', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_60', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_61', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_61', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_61', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_61', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_30', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_120', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_90', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_121', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_91', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_122', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_92', 'trainable': True, 'dtype': 'float32', 'rate': 0.02552724194976823, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_123', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:29:47.838759                             \n",
            "100%|| 31/31 [00:57<00:00, 57.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 31 trials to 32 (+1) trials\n",
            "2023-07-30 21:29:47.987998                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00470555778189363, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_31', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_62_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_62', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_62', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_62', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_62', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_63', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_63', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_63', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_63', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_31', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_124', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_93', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_125', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_94', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_126', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_95', 'trainable': True, 'dtype': 'float32', 'rate': 0.030865734550545365, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_127', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.5646 - acc: 0.0625 - auc: 0.3000 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  35/1170 [..............................] - ETA: 1s - loss: 1.4511 - acc: 0.0455 - auc: 0.6812 - precision: 0.0343 - recall: 1.0000 - f1: 0.0638 \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: 1.4583 - acc: 0.0472 - auc: 0.6755 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: 1.4584 - acc: 0.0510 - auc: 0.6592 - precision: 0.0375 - recall: 1.0000 - f1: 0.0701\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: 1.4604 - acc: 0.0475 - auc: 0.6562 - precision: 0.0358 - recall: 1.0000 - f1: 0.0672\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: 1.4543 - acc: 0.0448 - auc: 0.6617 - precision: 0.0327 - recall: 1.0000 - f1: 0.0613\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: 1.4527 - acc: 0.0450 - auc: 0.6640 - precision: 0.0325 - recall: 1.0000 - f1: 0.0611\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: 1.4508 - acc: 0.0460 - auc: 0.6694 - precision: 0.0325 - recall: 1.0000 - f1: 0.0610\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: 1.4548 - acc: 0.0469 - auc: 0.6635 - precision: 0.0332 - recall: 1.0000 - f1: 0.0625\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: 1.4555 - acc: 0.0460 - auc: 0.6557 - precision: 0.0324 - recall: 1.0000 - f1: 0.0609\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: 1.4542 - acc: 0.0439 - auc: 0.6522 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: 1.4566 - acc: 0.0438 - auc: 0.6448 - precision: 0.0311 - recall: 0.9953 - f1: 0.0585\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: 1.4559 - acc: 0.0435 - auc: 0.6371 - precision: 0.0307 - recall: 0.9957 - f1: 0.0579\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: 1.4553 - acc: 0.0428 - auc: 0.6368 - precision: 0.0309 - recall: 0.9962 - f1: 0.0582\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 1.4549 - acc: 0.0423 - auc: 0.6372 - precision: 0.0304 - recall: 0.9963 - f1: 0.0573\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: 1.4563 - acc: 0.0417 - auc: 0.6323 - precision: 0.0303 - recall: 0.9965 - f1: 0.0570\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: 1.4552 - acc: 0.0418 - auc: 0.6348 - precision: 0.0302 - recall: 0.9968 - f1: 0.0569\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 1.4547 - acc: 0.0420 - auc: 0.6349 - precision: 0.0303 - recall: 0.9970 - f1: 0.0572\n",
            " 373/1170 [========>.....................] - ETA: 1s - loss: 1.4539 - acc: 0.0422 - auc: 0.6412 - precision: 0.0305 - recall: 0.9972 - f1: 0.0575\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: 1.4541 - acc: 0.0429 - auc: 0.6385 - precision: 0.0312 - recall: 0.9975 - f1: 0.0588\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: 1.4538 - acc: 0.0427 - auc: 0.6402 - precision: 0.0311 - recall: 0.9976 - f1: 0.0585\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: 1.4534 - acc: 0.0422 - auc: 0.6431 - precision: 0.0307 - recall: 0.9977 - f1: 0.0579\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: 1.4533 - acc: 0.0419 - auc: 0.6438 - precision: 0.0304 - recall: 0.9978 - f1: 0.0573\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: 1.4531 - acc: 0.0420 - auc: 0.6431 - precision: 0.0304 - recall: 0.9979 - f1: 0.0573\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: 1.4525 - acc: 0.0416 - auc: 0.6429 - precision: 0.0302 - recall: 0.9980 - f1: 0.0570\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: 1.4532 - acc: 0.0418 - auc: 0.6406 - precision: 0.0301 - recall: 0.9981 - f1: 0.0568\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: 1.4524 - acc: 0.0420 - auc: 0.6398 - precision: 0.0300 - recall: 0.9981 - f1: 0.0567\n",
            " 567/1170 [=============>................] - ETA: 1s - loss: 1.4522 - acc: 0.0420 - auc: 0.6413 - precision: 0.0300 - recall: 0.9981 - f1: 0.0565\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: 1.4528 - acc: 0.0420 - auc: 0.6364 - precision: 0.0299 - recall: 0.9982 - f1: 0.0565\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: 1.4526 - acc: 0.0417 - auc: 0.6384 - precision: 0.0299 - recall: 0.9982 - f1: 0.0563\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 1.4521 - acc: 0.0413 - auc: 0.6355 - precision: 0.0296 - recall: 0.9983 - f1: 0.0559\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: 1.4522 - acc: 0.0415 - auc: 0.6350 - precision: 0.0298 - recall: 0.9984 - f1: 0.0562\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: 1.4525 - acc: 0.0415 - auc: 0.6354 - precision: 0.0297 - recall: 0.9984 - f1: 0.0560\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: 1.4533 - acc: 0.0413 - auc: 0.6348 - precision: 0.0295 - recall: 0.9984 - f1: 0.0556\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: 1.4525 - acc: 0.0414 - auc: 0.6355 - precision: 0.0296 - recall: 0.9985 - f1: 0.0558\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: 1.4531 - acc: 0.0415 - auc: 0.6336 - precision: 0.0296 - recall: 0.9985 - f1: 0.0558\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: 1.4526 - acc: 0.0412 - auc: 0.6337 - precision: 0.0294 - recall: 0.9985 - f1: 0.0554\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: 1.4528 - acc: 0.0412 - auc: 0.6330 - precision: 0.0294 - recall: 0.9986 - f1: 0.0554\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: 1.4530 - acc: 0.0414 - auc: 0.6338 - precision: 0.0297 - recall: 0.9987 - f1: 0.0560\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: 1.4535 - acc: 0.0411 - auc: 0.6320 - precision: 0.0295 - recall: 0.9974 - f1: 0.0556\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: 1.4530 - acc: 0.0407 - auc: 0.6330 - precision: 0.0292 - recall: 0.9974 - f1: 0.0550\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: 1.4540 - acc: 0.0405 - auc: 0.6308 - precision: 0.0293 - recall: 0.9962 - f1: 0.0552\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: 1.4546 - acc: 0.0408 - auc: 0.6307 - precision: 0.0294 - recall: 0.9952 - f1: 0.0554\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6332 - precision: 0.0297 - recall: 0.9953 - f1: 0.0559\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: 1.4546 - acc: 0.0411 - auc: 0.6318 - precision: 0.0295 - recall: 0.9954 - f1: 0.0556\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6307 - precision: 0.0294 - recall: 0.9955 - f1: 0.0554\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: 1.4544 - acc: 0.0412 - auc: 0.6301 - precision: 0.0293 - recall: 0.9956 - f1: 0.0552\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: 1.4551 - acc: 0.0410 - auc: 0.6286 - precision: 0.0293 - recall: 0.9956 - f1: 0.0552\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: 1.4545 - acc: 0.0410 - auc: 0.6298 - precision: 0.0292 - recall: 0.9957 - f1: 0.0550\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: 1.4545 - acc: 0.0411 - auc: 0.6297 - precision: 0.0294 - recall: 0.9959 - f1: 0.0555\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: 1.4546 - acc: 0.0413 - auc: 0.6332 - precision: 0.0296 - recall: 0.9960 - f1: 0.0558\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: 1.4545 - acc: 0.0414 - auc: 0.6340 - precision: 0.0296 - recall: 0.9961 - f1: 0.0558\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 1.4554 - acc: 0.0415 - auc: 0.6318 - precision: 0.0298 - recall: 0.9962 - f1: 0.0561\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: 1.4554 - acc: 0.0415 - auc: 0.6304 - precision: 0.0298 - recall: 0.9963 - f1: 0.0560\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: 1.4554 - acc: 0.0415 - auc: 0.6305 - precision: 0.0298 - recall: 0.9964 - f1: 0.0562\n",
            "\n",
            "2023-07-30 21:43:08.760894                             \n",
            "100%|| 32/32 [13:20<00:00, 800.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 32 trials to 33 (+1) trials\n",
            "2023-07-30 21:43:08.831484                             \n",
            "{'name': 'Adam', 'learning_rate': 0.34004878674597416, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_32', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_64_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_64', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_64', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_64', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_64', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_65', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_65', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_65', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_65', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_32', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_128', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_96', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_129', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_97', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_130', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_98', 'trainable': True, 'dtype': 'float32', 'rate': 0.49563774441109687, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_131', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:44:07.286538                             \n",
            "100%|| 33/33 [00:58<00:00, 58.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 33 trials to 34 (+1) trials\n",
            "2023-07-30 21:44:07.371958                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4215667018418808, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_33', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_66_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_66', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_66', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_66', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_66', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_67', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_67', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_67', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_67', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_33', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_132', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_99', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_133', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_100', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_134', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_101', 'trainable': True, 'dtype': 'float32', 'rate': 0.4958218355884791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_135', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1038/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:45:14.166071                             \n",
            "100%|| 34/34 [01:06<00:00, 66.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 34 trials to 35 (+1) trials\n",
            "2023-07-30 21:45:14.227802                             \n",
            "{'name': 'Adam', 'learning_rate': 0.16260120093047567, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_34', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_68_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_68', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_68', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_68', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_68', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_69', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_69', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_69', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_69', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_34', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_136', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_102', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_137', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_103', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_138', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_104', 'trainable': True, 'dtype': 'float32', 'rate': 0.48830697245471266, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_139', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  38/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:46:12.605945                             \n",
            "100%|| 35/35 [00:58<00:00, 58.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 35 trials to 36 (+1) trials\n",
            "2023-07-30 21:46:12.665614                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3291661395808618, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_35', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_70_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_70', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_70', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_70', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_70', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_71', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_71', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_71', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_71', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_35', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_140', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_105', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_141', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_106', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_142', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_107', 'trainable': True, 'dtype': 'float32', 'rate': 0.37989766557021465, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_143', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:47:11.530855                             \n",
            "100%|| 36/36 [00:58<00:00, 58.92s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 36 trials to 37 (+1) trials\n",
            "2023-07-30 21:47:11.670530                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9086257499020481, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_36', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_72_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_72', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_72', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_72', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_72', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_73', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_73', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_73', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_73', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_36', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_144', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_108', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_145', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_109', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_146', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_110', 'trainable': True, 'dtype': 'float32', 'rate': 0.3885416844836314, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_147', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:48:02.708860                             \n",
            "100%|| 37/37 [00:51<00:00, 51.10s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 37 trials to 38 (+1) trials\n",
            "2023-07-30 21:48:02.768585                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5504430797123047, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_37', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_74_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_74', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_74', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_74', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_74', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_75', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_75', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_75', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_75', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_37', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_148', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_111', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_149', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_112', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_150', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_113', 'trainable': True, 'dtype': 'float32', 'rate': 0.3907239582212232, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_151', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 369/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:48:53.475070                             \n",
            "100%|| 38/38 [00:50<00:00, 50.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 38 trials to 39 (+1) trials\n",
            "2023-07-30 21:48:53.534346                             \n",
            "{'name': 'Adam', 'learning_rate': 0.24041289518611525, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_38', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_76_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_76', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_76', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_76', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_76', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_77', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_77', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_77', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_77', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_38', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_152', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_114', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_153', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_115', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_154', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_116', 'trainable': True, 'dtype': 'float32', 'rate': 0.3750029721082884, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_155', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:49:46.244282                             \n",
            "100%|| 39/39 [00:52<00:00, 52.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 39 trials to 40 (+1) trials\n",
            "2023-07-30 21:49:46.304502                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2275063984552216, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_39', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_78_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_78', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_78', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_78', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_78', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_79', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_79', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_79', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_79', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_39', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_156', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_117', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_157', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_118', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_158', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_119', 'trainable': True, 'dtype': 'float32', 'rate': 0.4897418145854439, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_159', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 332/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:50:44.850968                             \n",
            "100%|| 40/40 [00:58<00:00, 58.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 40 trials to 41 (+1) trials\n",
            "2023-07-30 21:50:44.908954                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5698474362113314, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_40', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_80_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_80', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_80', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_80', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_80', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_81', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_81', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_81', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_81', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_40', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_160', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_120', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_161', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_121', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_162', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_122', 'trainable': True, 'dtype': 'float32', 'rate': 0.47869335649109007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_163', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  40/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:51:52.138374                             \n",
            "100%|| 41/41 [01:07<00:00, 67.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 41 trials to 42 (+1) trials\n",
            "2023-07-30 21:51:52.199739                             \n",
            "{'name': 'Adam', 'learning_rate': 0.1572789606078243, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_41', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_82_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_82', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_82', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_82', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_82', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_83', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_83', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_83', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_83', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_41', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_164', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_123', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_165', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_124', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_166', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_125', 'trainable': True, 'dtype': 'float32', 'rate': 0.4824014444258588, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_167', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  40/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:57:03.316648                             \n",
            "100%|| 42/42 [05:11<00:00, 311.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 42 trials to 43 (+1) trials\n",
            "2023-07-30 21:57:03.375668                             \n",
            "{'name': 'Adam', 'learning_rate': 0.48183478974461685, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_42', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_84_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_84', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_84', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_84', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_84', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_85', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_85', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_85', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_85', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_42', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_168', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_126', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_169', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_127', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_170', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_128', 'trainable': True, 'dtype': 'float32', 'rate': 0.48115010444413625, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_171', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  29/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:57:54.394967                             \n",
            "100%|| 43/43 [00:51<00:00, 51.07s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 43 trials to 44 (+1) trials\n",
            "2023-07-30 21:57:54.453328                             \n",
            "{'name': 'Adam', 'learning_rate': 0.21567454458132654, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_43', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_86_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_86', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_86', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_86', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_86', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_87', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_87', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_87', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_87', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_43', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_172', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_129', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_173', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_130', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_174', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_131', 'trainable': True, 'dtype': 'float32', 'rate': 0.3897120582651761, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_175', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  27/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 21:58:54.261129                             \n",
            "100%|| 44/44 [00:59<00:00, 59.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 44 trials to 45 (+1) trials\n",
            "2023-07-30 21:58:54.321039                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4220124024578253, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_44', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_88_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_88', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_88', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_88', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_88', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_89', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_89', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_89', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_89', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_44', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_176', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_132', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_177', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_133', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_178', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_134', 'trainable': True, 'dtype': 'float32', 'rate': 0.3765617057819877, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_179', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:04:09.801017                             \n",
            "100%|| 45/45 [05:15<00:00, 315.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 45 trials to 46 (+1) trials\n",
            "2023-07-30 22:04:09.948712                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3081750345491424, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_45', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_90_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_90', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_90', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_90', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_90', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_91', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_91', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_91', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_91', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_45', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_180', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_135', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_181', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_136', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_182', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_137', 'trainable': True, 'dtype': 'float32', 'rate': 0.48311341676788566, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_183', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:05:09.308877                             \n",
            "100%|| 46/46 [00:59<00:00, 59.42s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 46 trials to 47 (+1) trials\n",
            "2023-07-30 22:05:09.368198                             \n",
            "{'name': 'Adam', 'learning_rate': 0.12660174534366173, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_46', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_92_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_92', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_92', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_92', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_92', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_93', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_93', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_93', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_93', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_46', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_184', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_138', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_185', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_139', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_186', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_140', 'trainable': True, 'dtype': 'float32', 'rate': 0.4747565677227151, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_187', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:12:44.137043                             \n",
            "100%|| 47/47 [07:34<00:00, 454.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 47 trials to 48 (+1) trials\n",
            "2023-07-30 22:12:44.199059                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5536751854290182, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_47', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_94_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_94', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_94', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_94', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_94', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_95', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_95', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_95', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_95', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_47', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_188', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_141', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_189', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_142', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_190', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_143', 'trainable': True, 'dtype': 'float32', 'rate': 0.37936548673336845, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_191', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:13:37.331606                             \n",
            "100%|| 48/48 [00:53<00:00, 53.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 48 trials to 49 (+1) trials\n",
            "2023-07-30 22:13:37.389500                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00020303386505032435, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_48', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_96_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_96', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_96', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_96', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_96', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_97', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_97', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_97', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_97', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_48', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_192', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_144', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_193', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_145', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_194', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_146', 'trainable': True, 'dtype': 'float32', 'rate': 0.494005086690422, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_195', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: 1.3690 - acc: 0.0625 - auc: 0.3167 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  29/1170 [..............................] - ETA: 2s - loss: 1.2725 - acc: 0.0302 - auc: 0.6486 - precision: 0.0302 - recall: 1.0000 - f1: 0.0564 \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: 1.2850 - acc: 0.0347 - auc: 0.6273 - precision: 0.0347 - recall: 1.0000 - f1: 0.0647\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: 1.2858 - acc: 0.0369 - auc: 0.6396 - precision: 0.0369 - recall: 1.0000 - f1: 0.0688\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: 1.2849 - acc: 0.0353 - auc: 0.6485 - precision: 0.0353 - recall: 1.0000 - f1: 0.0661\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: 1.2769 - acc: 0.0309 - auc: 0.6520 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: 1.2775 - acc: 0.0320 - auc: 0.6617 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: 1.2791 - acc: 0.0317 - auc: 0.6651 - precision: 0.0317 - recall: 1.0000 - f1: 0.0597\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: 1.2812 - acc: 0.0329 - auc: 0.6596 - precision: 0.0329 - recall: 1.0000 - f1: 0.0619\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: 1.2805 - acc: 0.0316 - auc: 0.6492 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: 1.2791 - acc: 0.0311 - auc: 0.6484 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 215/1170 [====>.........................] - ETA: 2s - loss: 1.2801 - acc: 0.0308 - auc: 0.6395 - precision: 0.0308 - recall: 1.0000 - f1: 0.0579\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: 1.2797 - acc: 0.0306 - auc: 0.6349 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: 1.2792 - acc: 0.0304 - auc: 0.6347 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: 1.2783 - acc: 0.0301 - auc: 0.6371 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: 1.2793 - acc: 0.0301 - auc: 0.6274 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: 1.2781 - acc: 0.0296 - auc: 0.6311 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: 1.2788 - acc: 0.0302 - auc: 0.6331 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: 1.2785 - acc: 0.0302 - auc: 0.6332 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: 1.2786 - acc: 0.0303 - auc: 0.6341 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: 1.2782 - acc: 0.0302 - auc: 0.6341 - precision: 0.0302 - recall: 1.0000 - f1: 0.0569\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: 1.2791 - acc: 0.0310 - auc: 0.6347 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: 1.2788 - acc: 0.0307 - auc: 0.6331 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: 1.2782 - acc: 0.0304 - auc: 0.6331 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 1.2781 - acc: 0.0303 - auc: 0.6346 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: 1.2776 - acc: 0.0300 - auc: 0.6342 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: 1.2774 - acc: 0.0299 - auc: 0.6334 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: 1.2778 - acc: 0.0298 - auc: 0.6307 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: 1.2777 - acc: 0.0297 - auc: 0.6283 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: 1.2785 - acc: 0.0298 - auc: 0.6205 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: 1.2778 - acc: 0.0295 - auc: 0.6209 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: 1.2774 - acc: 0.0292 - auc: 0.6202 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: 1.2776 - acc: 0.0296 - auc: 0.6216 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: 1.2775 - acc: 0.0293 - auc: 0.6227 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: 1.2775 - acc: 0.0292 - auc: 0.6215 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: 1.2775 - acc: 0.0293 - auc: 0.6210 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: 1.2774 - acc: 0.0292 - auc: 0.6195 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: 1.2771 - acc: 0.0291 - auc: 0.6216 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: 1.2777 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: 1.2778 - acc: 0.0292 - auc: 0.6204 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: 1.2782 - acc: 0.0292 - auc: 0.6194 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: 1.2776 - acc: 0.0289 - auc: 0.6208 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 1.2782 - acc: 0.0290 - auc: 0.6191 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: 1.2782 - acc: 0.0291 - auc: 0.6195 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6189 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: 1.2789 - acc: 0.0294 - auc: 0.6188 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6183 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: 1.2785 - acc: 0.0292 - auc: 0.6183 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: 1.2783 - acc: 0.0291 - auc: 0.6175 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: 1.2782 - acc: 0.0290 - auc: 0.6181 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 1.2786 - acc: 0.0290 - auc: 0.6156 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: 1.2783 - acc: 0.0289 - auc: 0.6160 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: 1.2787 - acc: 0.0292 - auc: 0.6169 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.2790 - acc: 0.0293 - auc: 0.6179 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: 1.2788 - acc: 0.0293 - auc: 0.6196 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: 1.2791 - acc: 0.0295 - auc: 0.6213 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 1.2793 - acc: 0.0295 - auc: 0.6199 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.0295 - auc: 0.6186 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: 1.2795 - acc: 0.0295 - auc: 0.6194 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 1.2795 - acc: 0.0296 - auc: 0.6200 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 22:15:02.242784                             \n",
            "100%|| 49/49 [01:24<00:00, 84.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 49 trials to 50 (+1) trials\n",
            "2023-07-30 22:15:02.305599                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6991370578489954, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_49', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_98_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_98', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_98', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_98', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_98', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_99', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_99', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_99', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_99', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_49', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_196', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_147', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_197', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_148', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_198', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_149', 'trainable': True, 'dtype': 'float32', 'rate': 0.374512792893276, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_199', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 28s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  17/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9596 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  32/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 4s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  40/1170 [>.............................] - ETA: 6s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  42/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  52/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 7s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 6s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 6s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 9s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 10s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 10s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            " 125/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 9s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 9s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 9s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 8s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 8s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 7s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 8s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 9s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 9s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 9s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 8s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 7s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 7s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 7s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 7s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 7s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 6s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 6s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 5s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 5s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 4s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 3s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 2s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 9s 8ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:22:12.407335                             \n",
            "100%|| 50/50 [07:10<00:00, 430.21s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 50 trials to 51 (+1) trials\n",
            "2023-07-30 22:22:12.545049                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6278383296275578, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_50', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_100_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_100', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_100', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_100', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_100', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_101', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_101', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_101', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_101', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_50', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_200', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_150', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_201', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_151', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_202', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_152', 'trainable': True, 'dtype': 'float32', 'rate': 0.3610326171573935, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_203', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 50/51 [08:43<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  62/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:31:01.334511                             \n",
            "100%|| 51/51 [08:48<00:00, 528.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 51 trials to 52 (+1) trials\n",
            "2023-07-30 22:31:01.406436                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5428541232560751, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_51', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_102_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_102', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_102', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_102', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_102', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_103', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_103', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_103', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_103', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_51', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_204', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_153', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_205', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_154', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_206', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_155', 'trainable': True, 'dtype': 'float32', 'rate': 0.3745690966887675, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_207', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 980/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:32:04.932893                             \n",
            "100%|| 52/52 [01:03<00:00, 63.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 52 trials to 53 (+1) trials\n",
            "2023-07-30 22:32:05.004607                             \n",
            "{'name': 'Adam', 'learning_rate': 0.006077755955858521, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_52', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_104_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_104', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_104', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_104', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_104', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_105', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_105', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_105', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_105', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_52', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_208', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_156', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_209', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_157', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_210', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_158', 'trainable': True, 'dtype': 'float32', 'rate': 0.3714117348039391, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_211', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: 1.0519 - acc: 0.0625 - auc: 0.1333 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  28/1170 [..............................] - ETA: 2s - loss: 0.9565 - acc: 0.0312 - auc: 0.6174 - precision: 0.0312 - recall: 1.0000 - f1: 0.0584 \n",
            "  36/1170 [..............................] - ETA: 3s - loss: 0.9603 - acc: 0.0330 - auc: 0.6261 - precision: 0.0330 - recall: 1.0000 - f1: 0.0614\n",
            "  49/1170 [>.............................] - ETA: 3s - loss: 0.9670 - acc: 0.0370 - auc: 0.6289 - precision: 0.0370 - recall: 1.0000 - f1: 0.0688\n",
            "  63/1170 [>.............................] - ETA: 3s - loss: 0.9653 - acc: 0.0362 - auc: 0.6281 - precision: 0.0362 - recall: 1.0000 - f1: 0.0676\n",
            "  77/1170 [>.............................] - ETA: 3s - loss: 0.9653 - acc: 0.0353 - auc: 0.6416 - precision: 0.0353 - recall: 1.0000 - f1: 0.0661\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: 0.9619 - acc: 0.0330 - auc: 0.6290 - precision: 0.0330 - recall: 1.0000 - f1: 0.0618\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: 0.9614 - acc: 0.0309 - auc: 0.6333 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 0.9610 - acc: 0.0320 - auc: 0.6371 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: 0.9623 - acc: 0.0323 - auc: 0.6407 - precision: 0.0323 - recall: 1.0000 - f1: 0.0609\n",
            " 161/1170 [===>..........................] - ETA: 3s - loss: 0.9627 - acc: 0.0320 - auc: 0.6331 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: 0.9621 - acc: 0.0316 - auc: 0.6358 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 189/1170 [===>..........................] - ETA: 3s - loss: 0.9626 - acc: 0.0309 - auc: 0.6303 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 203/1170 [====>.........................] - ETA: 3s - loss: 0.9629 - acc: 0.0311 - auc: 0.6309 - precision: 0.0311 - recall: 1.0000 - f1: 0.0584\n",
            " 215/1170 [====>.........................] - ETA: 3s - loss: 0.9635 - acc: 0.0308 - auc: 0.6249 - precision: 0.0308 - recall: 1.0000 - f1: 0.0579\n",
            " 226/1170 [====>.........................] - ETA: 3s - loss: 0.9621 - acc: 0.0306 - auc: 0.6284 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: 0.9625 - acc: 0.0306 - auc: 0.6205 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: 0.9619 - acc: 0.0301 - auc: 0.6182 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 266/1170 [=====>........................] - ETA: 3s - loss: 0.9618 - acc: 0.0307 - auc: 0.6164 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: 0.9613 - acc: 0.0301 - auc: 0.6189 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: 0.9618 - acc: 0.0302 - auc: 0.6174 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 299/1170 [======>.......................] - ETA: 3s - loss: 0.9617 - acc: 0.0300 - auc: 0.6181 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 312/1170 [=======>......................] - ETA: 3s - loss: 0.9607 - acc: 0.0295 - auc: 0.6206 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 319/1170 [=======>......................] - ETA: 3s - loss: 0.9606 - acc: 0.0296 - auc: 0.6210 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: 0.9612 - acc: 0.0302 - auc: 0.6230 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 354/1170 [========>.....................] - ETA: 3s - loss: 0.9611 - acc: 0.0304 - auc: 0.6239 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 369/1170 [========>.....................] - ETA: 2s - loss: 0.9608 - acc: 0.0303 - auc: 0.6258 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: 0.9600 - acc: 0.0300 - auc: 0.6265 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: 0.9610 - acc: 0.0309 - auc: 0.6249 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: 0.9610 - acc: 0.0308 - auc: 0.6242 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: 0.9609 - acc: 0.0308 - auc: 0.6256 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: 0.9607 - acc: 0.0306 - auc: 0.6268 - precision: 0.0306 - recall: 1.0000 - f1: 0.0576\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: 0.9602 - acc: 0.0304 - auc: 0.6271 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 472/1170 [===========>..................] - ETA: 2s - loss: 0.9603 - acc: 0.0303 - auc: 0.6271 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: 0.9596 - acc: 0.0300 - auc: 0.6282 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 519/1170 [============>.................] - ETA: 2s - loss: 0.9597 - acc: 0.0299 - auc: 0.6242 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 531/1170 [============>.................] - ETA: 2s - loss: 0.9595 - acc: 0.0297 - auc: 0.6257 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 540/1170 [============>.................] - ETA: 2s - loss: 0.9598 - acc: 0.0297 - auc: 0.6242 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: 0.9598 - acc: 0.0298 - auc: 0.6221 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 568/1170 [=============>................] - ETA: 2s - loss: 0.9594 - acc: 0.0296 - auc: 0.6232 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 584/1170 [=============>................] - ETA: 2s - loss: 0.9599 - acc: 0.0298 - auc: 0.6188 - precision: 0.0298 - recall: 1.0000 - f1: 0.0561\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: 0.9594 - acc: 0.0295 - auc: 0.6200 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: 0.9594 - acc: 0.0294 - auc: 0.6192 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: 0.9590 - acc: 0.0293 - auc: 0.6192 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: 0.9592 - acc: 0.0296 - auc: 0.6204 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: 0.9591 - acc: 0.0294 - auc: 0.6206 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: 0.9592 - acc: 0.0294 - auc: 0.6203 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: 0.9590 - acc: 0.0293 - auc: 0.6211 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: 0.9588 - acc: 0.0291 - auc: 0.6212 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: 0.9589 - acc: 0.0292 - auc: 0.6228 - precision: 0.0292 - recall: 1.0000 - f1: 0.0552\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: 0.9593 - acc: 0.0294 - auc: 0.6212 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: 0.9589 - acc: 0.0291 - auc: 0.6198 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: 0.9588 - acc: 0.0291 - auc: 0.6208 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: 0.9590 - acc: 0.0290 - auc: 0.6196 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 0.9591 - acc: 0.0292 - auc: 0.6205 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: 0.9593 - acc: 0.0292 - auc: 0.6188 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: 0.9594 - acc: 0.0292 - auc: 0.6180 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 828/1170 [====================>.........] - ETA: 1s - loss: 0.9590 - acc: 0.0289 - auc: 0.6189 - precision: 0.0289 - recall: 1.0000 - f1: 0.0546\n",
            " 851/1170 [====================>.........] - ETA: 1s - loss: 0.9593 - acc: 0.0290 - auc: 0.6176 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 865/1170 [=====================>........] - ETA: 1s - loss: 0.9595 - acc: 0.0291 - auc: 0.6171 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 878/1170 [=====================>........] - ETA: 1s - loss: 0.9596 - acc: 0.0291 - auc: 0.6184 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: 0.9599 - acc: 0.0293 - auc: 0.6176 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: 0.9600 - acc: 0.0294 - auc: 0.6177 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 0.9600 - acc: 0.0294 - auc: 0.6166 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: 0.9598 - acc: 0.0292 - auc: 0.6159 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 0.9598 - acc: 0.0292 - auc: 0.6139 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: 0.9595 - acc: 0.0290 - auc: 0.6143 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 0.9599 - acc: 0.0290 - auc: 0.6121 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: 0.9599 - acc: 0.0290 - auc: 0.6131 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: 0.9598 - acc: 0.0290 - auc: 0.6133 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: 0.9601 - acc: 0.0292 - auc: 0.6131 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: 0.9602 - acc: 0.0292 - auc: 0.6128 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: 0.9601 - acc: 0.0292 - auc: 0.6145 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: 0.9602 - acc: 0.0292 - auc: 0.6155 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: 0.9604 - acc: 0.0293 - auc: 0.6149 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: 0.9603 - acc: 0.0293 - auc: 0.6153 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: 0.9606 - acc: 0.0295 - auc: 0.6163 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 0.9611 - acc: 0.0295 - auc: 0.6133 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.0295 - auc: 0.6121 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.0296 - auc: 0.6121 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: 0.9611 - acc: 0.0295 - auc: 0.6114 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: 0.9611 - acc: 0.0296 - auc: 0.6122 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-30 22:33:19.136767                             \n",
            "100%|| 53/53 [01:14<00:00, 74.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 53 trials to 54 (+1) trials\n",
            "2023-07-30 22:33:19.196213                             \n",
            "{'name': 'Adam', 'learning_rate': 0.19767535212585802, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_53', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_106_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_106', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_106', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_106', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_106', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_107', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_107', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_107', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_107', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_53', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_212', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_159', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_213', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_160', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_214', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_161', 'trainable': True, 'dtype': 'float32', 'rate': 0.25698907951469163, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_215', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 509/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:38:45.202832                             \n",
            "100%|| 54/54 [05:26<00:00, 326.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 54 trials to 55 (+1) trials\n",
            "2023-07-30 22:38:45.278056                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5943576615756214, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_54', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_108_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_108', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_108', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_108', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_108', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_109', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_109', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_109', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_109', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_54', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_216', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_162', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_217', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_163', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_218', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_164', 'trainable': True, 'dtype': 'float32', 'rate': 0.2428672449618119, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_219', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  43/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  57/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:46:27.884960                             \n",
            "100%|| 55/55 [07:42<00:00, 462.66s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 55 trials to 56 (+1) trials\n",
            "2023-07-30 22:46:27.945282                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5478990863457, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_55', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_110_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_110', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_110', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_110', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_110', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_111', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_111', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_111', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_111', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_55', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_220', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_165', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_221', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_166', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_222', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_167', 'trainable': True, 'dtype': 'float32', 'rate': 0.26375018375434367, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_223', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 22:51:56.970992                             \n",
            "100%|| 56/56 [05:29<00:00, 329.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 56 trials to 57 (+1) trials\n",
            "2023-07-30 22:51:57.035398                             \n",
            "{'name': 'Adam', 'learning_rate': 0.003842335918990532, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_56', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_112_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_112', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_112', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_112', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_112', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_113', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_113', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_113', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_113', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_56', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_224', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_168', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_225', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_169', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_226', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_170', 'trainable': True, 'dtype': 'float32', 'rate': 0.0003084513343053763, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_227', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: 1.2810 - acc: 0.2500 - auc: 0.3333 - precision: 0.0769 - recall: 1.0000 - f1: 0.1429\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 1.2249 - acc: 0.1866 - auc: 0.6174 - precision: 0.0371 - recall: 0.9429 - f1: 0.0676 \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: 1.2374 - acc: 0.1857 - auc: 0.6014 - precision: 0.0392 - recall: 0.9153 - f1: 0.0720\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: 1.2355 - acc: 0.1870 - auc: 0.6148 - precision: 0.0393 - recall: 0.9221 - f1: 0.0724\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: 1.2333 - acc: 0.1843 - auc: 0.6258 - precision: 0.0370 - recall: 0.9362 - f1: 0.0684\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: 1.2292 - acc: 0.1840 - auc: 0.6305 - precision: 0.0348 - recall: 0.9388 - f1: 0.0643\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: 1.2282 - acc: 0.1859 - auc: 0.6379 - precision: 0.0361 - recall: 0.9504 - f1: 0.0668\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: 1.2286 - acc: 0.1834 - auc: 0.6442 - precision: 0.0361 - recall: 0.9552 - f1: 0.0670\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: 1.2310 - acc: 0.1860 - auc: 0.6336 - precision: 0.0361 - recall: 0.9400 - f1: 0.0671\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: 1.2289 - acc: 0.1875 - auc: 0.6319 - precision: 0.0362 - recall: 0.9455 - f1: 0.0673\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: 1.2305 - acc: 0.1855 - auc: 0.6306 - precision: 0.0356 - recall: 0.9438 - f1: 0.0663\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: 1.2296 - acc: 0.1853 - auc: 0.6291 - precision: 0.0348 - recall: 0.9490 - f1: 0.0649\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: 1.2309 - acc: 0.1859 - auc: 0.6164 - precision: 0.0343 - recall: 0.9401 - f1: 0.0641\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: 1.2308 - acc: 0.1851 - auc: 0.6136 - precision: 0.0341 - recall: 0.9381 - f1: 0.0637\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: 1.2301 - acc: 0.1860 - auc: 0.6119 - precision: 0.0343 - recall: 0.9397 - f1: 0.0642\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: 1.2299 - acc: 0.1853 - auc: 0.6082 - precision: 0.0337 - recall: 0.9383 - f1: 0.0632\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: 1.2305 - acc: 0.1846 - auc: 0.6099 - precision: 0.0339 - recall: 0.9365 - f1: 0.0635\n",
            " 261/1170 [=====>........................] - ETA: 3s - loss: 1.2303 - acc: 0.1846 - auc: 0.6112 - precision: 0.0341 - recall: 0.9375 - f1: 0.0638\n",
            " 265/1170 [=====>........................] - ETA: 3s - loss: 1.2302 - acc: 0.1848 - auc: 0.6131 - precision: 0.0342 - recall: 0.9385 - f1: 0.0639\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: 1.2295 - acc: 0.1851 - auc: 0.6119 - precision: 0.0335 - recall: 0.9363 - f1: 0.0627\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: 1.2304 - acc: 0.1833 - auc: 0.6054 - precision: 0.0330 - recall: 0.9286 - f1: 0.0618\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: 1.2293 - acc: 0.1842 - auc: 0.6099 - precision: 0.0330 - recall: 0.9302 - f1: 0.0619\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: 1.2292 - acc: 0.1848 - auc: 0.6121 - precision: 0.0333 - recall: 0.9308 - f1: 0.0625\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 1.2289 - acc: 0.1847 - auc: 0.6106 - precision: 0.0332 - recall: 0.9299 - f1: 0.0623\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: 1.2283 - acc: 0.1851 - auc: 0.6117 - precision: 0.0336 - recall: 0.9331 - f1: 0.0628\n",
            " 365/1170 [========>.....................] - ETA: 2s - loss: 1.2279 - acc: 0.1846 - auc: 0.6136 - precision: 0.0338 - recall: 0.9352 - f1: 0.0633\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: 1.2274 - acc: 0.1845 - auc: 0.6134 - precision: 0.0336 - recall: 0.9313 - f1: 0.0629\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: 1.2276 - acc: 0.1838 - auc: 0.6120 - precision: 0.0334 - recall: 0.9326 - f1: 0.0627\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: 1.2277 - acc: 0.1839 - auc: 0.6106 - precision: 0.0336 - recall: 0.9340 - f1: 0.0630\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: 1.2275 - acc: 0.1857 - auc: 0.6093 - precision: 0.0344 - recall: 0.9355 - f1: 0.0645\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: 1.2263 - acc: 0.1849 - auc: 0.6148 - precision: 0.0343 - recall: 0.9351 - f1: 0.0643\n",
            " 440/1170 [==========>...................] - ETA: 2s - loss: 1.2262 - acc: 0.1839 - auc: 0.6159 - precision: 0.0340 - recall: 0.9372 - f1: 0.0637\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: 1.2264 - acc: 0.1833 - auc: 0.6162 - precision: 0.0337 - recall: 0.9361 - f1: 0.0632\n",
            " 464/1170 [==========>...................] - ETA: 2s - loss: 1.2259 - acc: 0.1831 - auc: 0.6185 - precision: 0.0335 - recall: 0.9375 - f1: 0.0629\n",
            " 468/1170 [===========>..................] - ETA: 2s - loss: 1.2262 - acc: 0.1827 - auc: 0.6182 - precision: 0.0336 - recall: 0.9360 - f1: 0.0630\n",
            " 471/1170 [===========>..................] - ETA: 2s - loss: 1.2261 - acc: 0.1829 - auc: 0.6181 - precision: 0.0337 - recall: 0.9365 - f1: 0.0632\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: 1.2264 - acc: 0.1826 - auc: 0.6207 - precision: 0.0339 - recall: 0.9376 - f1: 0.0636\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: 1.2259 - acc: 0.1826 - auc: 0.6197 - precision: 0.0335 - recall: 0.9382 - f1: 0.0628\n",
            " 504/1170 [===========>..................] - ETA: 2s - loss: 1.2251 - acc: 0.1826 - auc: 0.6202 - precision: 0.0332 - recall: 0.9378 - f1: 0.0622\n",
            " 516/1170 [============>.................] - ETA: 2s - loss: 1.2253 - acc: 0.1828 - auc: 0.6181 - precision: 0.0332 - recall: 0.9371 - f1: 0.0622\n",
            " 534/1170 [============>.................] - ETA: 2s - loss: 1.2255 - acc: 0.1823 - auc: 0.6155 - precision: 0.0329 - recall: 0.9387 - f1: 0.0618\n",
            " 546/1170 [=============>................] - ETA: 2s - loss: 1.2258 - acc: 0.1818 - auc: 0.6173 - precision: 0.0330 - recall: 0.9383 - f1: 0.0619\n",
            " 557/1170 [=============>................] - ETA: 2s - loss: 1.2258 - acc: 0.1819 - auc: 0.6151 - precision: 0.0330 - recall: 0.9358 - f1: 0.0618\n",
            " 569/1170 [=============>................] - ETA: 2s - loss: 1.2256 - acc: 0.1818 - auc: 0.6180 - precision: 0.0330 - recall: 0.9354 - f1: 0.0618\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: 1.2259 - acc: 0.1815 - auc: 0.6164 - precision: 0.0329 - recall: 0.9331 - f1: 0.0617\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: 1.2259 - acc: 0.1814 - auc: 0.6156 - precision: 0.0328 - recall: 0.9319 - f1: 0.0616\n",
            " 590/1170 [==============>...............] - ETA: 2s - loss: 1.2256 - acc: 0.1813 - auc: 0.6155 - precision: 0.0327 - recall: 0.9320 - f1: 0.0613\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: 1.2255 - acc: 0.1812 - auc: 0.6155 - precision: 0.0326 - recall: 0.9320 - f1: 0.0612\n",
            " 598/1170 [==============>...............] - ETA: 2s - loss: 1.2253 - acc: 0.1811 - auc: 0.6161 - precision: 0.0325 - recall: 0.9325 - f1: 0.0610\n",
            " 612/1170 [==============>...............] - ETA: 2s - loss: 1.2254 - acc: 0.1809 - auc: 0.6164 - precision: 0.0325 - recall: 0.9308 - f1: 0.0611\n",
            " 626/1170 [===============>..............] - ETA: 2s - loss: 1.2248 - acc: 0.1811 - auc: 0.6161 - precision: 0.0323 - recall: 0.9319 - f1: 0.0607\n",
            " 633/1170 [===============>..............] - ETA: 2s - loss: 1.2247 - acc: 0.1814 - auc: 0.6160 - precision: 0.0325 - recall: 0.9313 - f1: 0.0610\n",
            " 637/1170 [===============>..............] - ETA: 2s - loss: 1.2243 - acc: 0.1816 - auc: 0.6167 - precision: 0.0324 - recall: 0.9316 - f1: 0.0608\n",
            " 650/1170 [===============>..............] - ETA: 2s - loss: 1.2245 - acc: 0.1815 - auc: 0.6181 - precision: 0.0325 - recall: 0.9315 - f1: 0.0610\n",
            " 661/1170 [===============>..............] - ETA: 2s - loss: 1.2250 - acc: 0.1810 - auc: 0.6177 - precision: 0.0325 - recall: 0.9325 - f1: 0.0609\n",
            " 666/1170 [================>.............] - ETA: 2s - loss: 1.2250 - acc: 0.1808 - auc: 0.6185 - precision: 0.0324 - recall: 0.9329 - f1: 0.0608\n",
            " 681/1170 [================>.............] - ETA: 2s - loss: 1.2263 - acc: 0.1795 - auc: 0.6157 - precision: 0.0322 - recall: 0.9310 - f1: 0.0605\n",
            " 700/1170 [================>.............] - ETA: 2s - loss: 1.2262 - acc: 0.1792 - auc: 0.6163 - precision: 0.0322 - recall: 0.9313 - f1: 0.0604\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: 1.2261 - acc: 0.1799 - auc: 0.6158 - precision: 0.0324 - recall: 0.9316 - f1: 0.0608\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: 1.2262 - acc: 0.1796 - auc: 0.6150 - precision: 0.0322 - recall: 0.9323 - f1: 0.0605\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: 1.2262 - acc: 0.1790 - auc: 0.6138 - precision: 0.0321 - recall: 0.9333 - f1: 0.0602\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: 1.2258 - acc: 0.1794 - auc: 0.6155 - precision: 0.0321 - recall: 0.9330 - f1: 0.0604\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: 1.2261 - acc: 0.1794 - auc: 0.6151 - precision: 0.0321 - recall: 0.9336 - f1: 0.0604\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: 1.2262 - acc: 0.1793 - auc: 0.6138 - precision: 0.0321 - recall: 0.9328 - f1: 0.0603\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 1.2258 - acc: 0.1798 - auc: 0.6154 - precision: 0.0323 - recall: 0.9342 - f1: 0.0606\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 1.2265 - acc: 0.1795 - auc: 0.6124 - precision: 0.0323 - recall: 0.9330 - f1: 0.0606\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: 1.2267 - acc: 0.1794 - auc: 0.6116 - precision: 0.0322 - recall: 0.9329 - f1: 0.0605\n",
            " 828/1170 [====================>.........] - ETA: 1s - loss: 1.2263 - acc: 0.1792 - auc: 0.6126 - precision: 0.0319 - recall: 0.9335 - f1: 0.0599\n",
            " 847/1170 [====================>.........] - ETA: 1s - loss: 1.2266 - acc: 0.1793 - auc: 0.6108 - precision: 0.0319 - recall: 0.9325 - f1: 0.0599\n",
            " 869/1170 [=====================>........] - ETA: 1s - loss: 1.2268 - acc: 0.1794 - auc: 0.6107 - precision: 0.0322 - recall: 0.9334 - f1: 0.0603\n",
            " 886/1170 [=====================>........] - ETA: 1s - loss: 1.2272 - acc: 0.1795 - auc: 0.6094 - precision: 0.0322 - recall: 0.9335 - f1: 0.0604\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: 1.2275 - acc: 0.1798 - auc: 0.6096 - precision: 0.0325 - recall: 0.9352 - f1: 0.0610\n",
            " 914/1170 [======================>.......] - ETA: 1s - loss: 1.2275 - acc: 0.1796 - auc: 0.6092 - precision: 0.0325 - recall: 0.9360 - f1: 0.0609\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: 1.2275 - acc: 0.1790 - auc: 0.6081 - precision: 0.0320 - recall: 0.9344 - f1: 0.0601\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: 1.2274 - acc: 0.1794 - auc: 0.6078 - precision: 0.0321 - recall: 0.9349 - f1: 0.0603\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: 1.2279 - acc: 0.1787 - auc: 0.6070 - precision: 0.0320 - recall: 0.9327 - f1: 0.0601\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: 1.2279 - acc: 0.1789 - auc: 0.6073 - precision: 0.0320 - recall: 0.9329 - f1: 0.0601\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: 1.2274 - acc: 0.1792 - auc: 0.6071 - precision: 0.0320 - recall: 0.9340 - f1: 0.0600\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: 1.2274 - acc: 0.1795 - auc: 0.6084 - precision: 0.0322 - recall: 0.9348 - f1: 0.0605\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: 1.2272 - acc: 0.1797 - auc: 0.6105 - precision: 0.0323 - recall: 0.9357 - f1: 0.0606\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 1.2275 - acc: 0.1796 - auc: 0.6104 - precision: 0.0324 - recall: 0.9366 - f1: 0.0609\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: 1.2277 - acc: 0.1794 - auc: 0.6105 - precision: 0.0326 - recall: 0.9380 - f1: 0.0612\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: 1.2277 - acc: 0.1795 - auc: 0.6118 - precision: 0.0327 - recall: 0.9374 - f1: 0.0612\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: 1.2281 - acc: 0.1792 - auc: 0.6103 - precision: 0.0328 - recall: 0.9386 - f1: 0.0614\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: 1.2284 - acc: 0.1791 - auc: 0.6104 - precision: 0.0328 - recall: 0.9387 - f1: 0.0614\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: 1.2279 - acc: 0.1794 - auc: 0.6108 - precision: 0.0327 - recall: 0.9393 - f1: 0.0613\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.1796 - auc: 0.6107 - precision: 0.0328 - recall: 0.9400 - f1: 0.0614\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: 1.2278 - acc: 0.1797 - auc: 0.6114 - precision: 0.0328 - recall: 0.9403 - f1: 0.0616\n",
            "\n",
            "2023-07-30 23:05:53.733325                             \n",
            "100%|| 57/57 [13:56<00:00, 836.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 57 trials to 58 (+1) trials\n",
            "2023-07-30 23:05:53.949459                             \n",
            "{'name': 'Adam', 'learning_rate': 0.29801146033562625, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_57', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_114_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_114', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_114', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_114', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_114', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_115', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_115', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_115', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_115', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_57', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_228', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_171', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_229', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_172', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_230', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_173', 'trainable': True, 'dtype': 'float32', 'rate': 0.25464632549352695, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_231', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 57/58 [04:32<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0022s). Check your callbacks.\n",
            "  22/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  41/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  58/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:10:32.215625                             \n",
            "100%|| 58/58 [04:38<00:00, 278.39s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 58 trials to 59 (+1) trials\n",
            "2023-07-30 23:10:32.409469                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6036989399345184, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_58', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_116_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_116', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_116', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_116', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_116', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_117', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_117', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_117', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_117', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_58', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_232', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_174', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_233', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_175', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_234', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_176', 'trainable': True, 'dtype': 'float32', 'rate': 0.2591618559925702, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_235', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 58/59 [04:32<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0017s). Check your callbacks.\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  62/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:15:10.924379                             \n",
            "100%|| 59/59 [04:38<00:00, 278.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 59 trials to 60 (+1) trials\n",
            "2023-07-30 23:15:11.298830                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7184720653373794, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_59', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_118_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_118', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_118', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_118', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_118', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_119', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_119', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_119', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_119', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_59', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_236', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_177', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_237', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_178', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_238', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_179', 'trainable': True, 'dtype': 'float32', 'rate': 0.25791119549001407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_239', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  60/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:23:15.300598                             \n",
            "100%|| 60/60 [08:04<00:00, 484.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 60 trials to 61 (+1) trials\n",
            "2023-07-30 23:23:15.496788                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9444304103657876, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_60', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_120_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_120', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_120', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_120', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_120', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_121', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_121', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_121', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_121', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_60', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_240', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_180', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_241', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_181', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_242', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_182', 'trainable': True, 'dtype': 'float32', 'rate': 0.2554127907730454, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_243', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:31:08.571484                             \n",
            "100%|| 61/61 [07:53<00:00, 473.14s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 61 trials to 62 (+1) trials\n",
            "2023-07-30 23:31:08.725461                             \n",
            "{'name': 'Adam', 'learning_rate': 0.953368986716328, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_61', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_122_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_122', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_122', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_122', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_122', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_123', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_123', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_123', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_123', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_61', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_244', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_183', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_245', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_184', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_246', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_185', 'trainable': True, 'dtype': 'float32', 'rate': 0.2580929612659941, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_247', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 215/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:39:06.784438                             \n",
            "100%|| 62/62 [07:58<00:00, 478.12s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 62 trials to 63 (+1) trials\n",
            "2023-07-30 23:39:06.855106                             \n",
            "{'name': 'Adam', 'learning_rate': 0.13229624691303804, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_62', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_124_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_124', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_124', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_124', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_124', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_125', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_125', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_125', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_125', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_62', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_248', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_186', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_249', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_187', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_250', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_188', 'trainable': True, 'dtype': 'float32', 'rate': 0.2579788492015591, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_251', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 98%|| 62/63 [04:21<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  63/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:43:34.297034                             \n",
            "100%|| 63/63 [04:27<00:00, 267.51s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 63 trials to 64 (+1) trials\n",
            "2023-07-30 23:43:34.458570                             \n",
            "{'name': 'Adam', 'learning_rate': 0.21959673885547523, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_63', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_126_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_126', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_126', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_126', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_126', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_127', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_127', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_127', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_127', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_63', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_252', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_189', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_253', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_190', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_254', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_191', 'trainable': True, 'dtype': 'float32', 'rate': 0.4848463949766212, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_255', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  69/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:48:39.195663                             \n",
            "100%|| 64/64 [05:04<00:00, 304.81s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 64 trials to 65 (+1) trials\n",
            "2023-07-30 23:48:39.270916                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5768952264140849, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_64', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_128_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_128', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_128', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_128', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_128', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_129', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_129', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_129', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_129', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_64', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_256', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_192', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_257', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_193', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_258', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_194', 'trainable': True, 'dtype': 'float32', 'rate': 0.49901082060743085, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_259', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  69/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 2s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-30 23:52:30.683291                             \n",
            "100%|| 65/65 [03:51<00:00, 231.47s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 65 trials to 66 (+1) trials\n",
            "2023-07-30 23:52:30.747024                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8324671365502878, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_65', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_130_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_130', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_130', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_130', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_130', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_131', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_131', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_131', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_131', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_65', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_260', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_195', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_261', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_196', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_262', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_197', 'trainable': True, 'dtype': 'float32', 'rate': 0.49625349403054, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_263', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  61/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:00:46.983595                             \n",
            "100%|| 66/66 [08:16<00:00, 496.34s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 66 trials to 67 (+1) trials\n",
            "2023-07-31 00:00:47.113493                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3579129518848078, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_66', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_132_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_132', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_132', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_132', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_132', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_133', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_133', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_133', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_133', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_66', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_264', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_198', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_265', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_199', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_266', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_200', 'trainable': True, 'dtype': 'float32', 'rate': 0.2628951355938684, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_267', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 23s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:09:47.263398                             \n",
            "100%|| 67/67 [09:00<00:00, 540.24s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 67 trials to 68 (+1) trials\n",
            "2023-07-31 00:09:47.360996                             \n",
            "{'name': 'Adam', 'learning_rate': 0.398611709446185, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_67', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_134_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_134', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_134', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_134', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_134', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_135', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_135', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_135', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_135', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_67', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_268', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_201', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_269', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_202', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_270', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_203', 'trainable': True, 'dtype': 'float32', 'rate': 0.26122925565925104, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_271', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:14:15.667855                             \n",
            "100%|| 68/68 [04:28<00:00, 268.39s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 68 trials to 69 (+1) trials\n",
            "2023-07-31 00:14:15.756130                             \n",
            "{'name': 'Adam', 'learning_rate': 0.00020438666294315516, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_68', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_136_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_136', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_136', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_136', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_136', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_137', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_137', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_137', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_137', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_68', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_272', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_204', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_273', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_205', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_274', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_206', 'trainable': True, 'dtype': 'float32', 'rate': 0.2712669298967298, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_275', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 1.8549 - acc: 0.0625 - auc: 0.2250 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            " 99%|| 68/69 [04:56<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
            "  24/1170 [..............................] - ETA: 2s - loss: 1.6928 - acc: 0.0339 - auc: 0.6389 - precision: 0.0339 - recall: 1.0000 - f1: 0.0631 \n",
            "  40/1170 [>.............................] - ETA: 3s - loss: 1.6974 - acc: 0.0344 - auc: 0.6233 - precision: 0.0344 - recall: 1.0000 - f1: 0.0638\n",
            "  52/1170 [>.............................] - ETA: 3s - loss: 1.7036 - acc: 0.0361 - auc: 0.6236 - precision: 0.0361 - recall: 1.0000 - f1: 0.0671\n",
            "  61/1170 [>.............................] - ETA: 3s - loss: 1.7022 - acc: 0.0369 - auc: 0.6147 - precision: 0.0369 - recall: 1.0000 - f1: 0.0688\n",
            "  71/1170 [>.............................] - ETA: 4s - loss: 1.7038 - acc: 0.0365 - auc: 0.6206 - precision: 0.0365 - recall: 1.0000 - f1: 0.0683\n",
            "  75/1170 [>.............................] - ETA: 4s - loss: 1.7022 - acc: 0.0354 - auc: 0.6241 - precision: 0.0354 - recall: 1.0000 - f1: 0.0662\n",
            "  83/1170 [=>............................] - ETA: 5s - loss: 1.6966 - acc: 0.0335 - auc: 0.6299 - precision: 0.0335 - recall: 1.0000 - f1: 0.0627\n",
            "  92/1170 [=>............................] - ETA: 5s - loss: 1.6916 - acc: 0.0323 - auc: 0.6302 - precision: 0.0323 - recall: 1.0000 - f1: 0.0605\n",
            " 105/1170 [=>............................] - ETA: 5s - loss: 1.6917 - acc: 0.0312 - auc: 0.6372 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 124/1170 [==>...........................] - ETA: 4s - loss: 1.6913 - acc: 0.0320 - auc: 0.6436 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 134/1170 [==>...........................] - ETA: 4s - loss: 1.6926 - acc: 0.0319 - auc: 0.6513 - precision: 0.0319 - recall: 1.0000 - f1: 0.0601\n",
            " 148/1170 [==>...........................] - ETA: 4s - loss: 1.6958 - acc: 0.0334 - auc: 0.6465 - precision: 0.0334 - recall: 1.0000 - f1: 0.0627\n",
            " 164/1170 [===>..........................] - ETA: 4s - loss: 1.6933 - acc: 0.0316 - auc: 0.6455 - precision: 0.0316 - recall: 1.0000 - f1: 0.0595\n",
            " 178/1170 [===>..........................] - ETA: 4s - loss: 1.6956 - acc: 0.0316 - auc: 0.6466 - precision: 0.0316 - recall: 1.0000 - f1: 0.0594\n",
            " 191/1170 [===>..........................] - ETA: 4s - loss: 1.6937 - acc: 0.0308 - auc: 0.6497 - precision: 0.0308 - recall: 1.0000 - f1: 0.0578\n",
            " 201/1170 [====>.........................] - ETA: 4s - loss: 1.6924 - acc: 0.0309 - auc: 0.6497 - precision: 0.0309 - recall: 1.0000 - f1: 0.0582\n",
            " 212/1170 [====>.........................] - ETA: 4s - loss: 1.6951 - acc: 0.0311 - auc: 0.6376 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 226/1170 [====>.........................] - ETA: 4s - loss: 1.6947 - acc: 0.0306 - auc: 0.6335 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 238/1170 [=====>........................] - ETA: 4s - loss: 1.6941 - acc: 0.0305 - auc: 0.6306 - precision: 0.0305 - recall: 1.0000 - f1: 0.0573\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: 1.6943 - acc: 0.0301 - auc: 0.6286 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 265/1170 [=====>........................] - ETA: 3s - loss: 1.6943 - acc: 0.0307 - auc: 0.6292 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 279/1170 [======>.......................] - ETA: 3s - loss: 1.6932 - acc: 0.0301 - auc: 0.6296 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: 1.6949 - acc: 0.0302 - auc: 0.6210 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 310/1170 [======>.......................] - ETA: 3s - loss: 1.6943 - acc: 0.0296 - auc: 0.6203 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 321/1170 [=======>......................] - ETA: 3s - loss: 1.6936 - acc: 0.0295 - auc: 0.6228 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: 1.6940 - acc: 0.0302 - auc: 0.6259 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 342/1170 [=======>......................] - ETA: 3s - loss: 1.6932 - acc: 0.0300 - auc: 0.6251 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 354/1170 [========>.....................] - ETA: 3s - loss: 1.6934 - acc: 0.0304 - auc: 0.6259 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 365/1170 [========>.....................] - ETA: 3s - loss: 1.6929 - acc: 0.0304 - auc: 0.6259 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 377/1170 [========>.....................] - ETA: 3s - loss: 1.6921 - acc: 0.0303 - auc: 0.6247 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 402/1170 [=========>....................] - ETA: 3s - loss: 1.6938 - acc: 0.0310 - auc: 0.6207 - precision: 0.0310 - recall: 1.0000 - f1: 0.0584\n",
            " 425/1170 [=========>....................] - ETA: 3s - loss: 1.6929 - acc: 0.0309 - auc: 0.6233 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 435/1170 [==========>...................] - ETA: 3s - loss: 1.6923 - acc: 0.0307 - auc: 0.6253 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: 1.6915 - acc: 0.0303 - auc: 0.6262 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: 1.6919 - acc: 0.0304 - auc: 0.6280 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: 1.6916 - acc: 0.0302 - auc: 0.6282 - precision: 0.0302 - recall: 1.0000 - f1: 0.0570\n",
            " 480/1170 [===========>..................] - ETA: 2s - loss: 1.6914 - acc: 0.0303 - auc: 0.6284 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 493/1170 [===========>..................] - ETA: 2s - loss: 1.6908 - acc: 0.0300 - auc: 0.6289 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 505/1170 [===========>..................] - ETA: 2s - loss: 1.6900 - acc: 0.0300 - auc: 0.6291 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 514/1170 [============>.................] - ETA: 2s - loss: 1.6895 - acc: 0.0300 - auc: 0.6307 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 523/1170 [============>.................] - ETA: 2s - loss: 1.6902 - acc: 0.0299 - auc: 0.6284 - precision: 0.0299 - recall: 1.0000 - f1: 0.0563\n",
            " 535/1170 [============>.................] - ETA: 2s - loss: 1.6903 - acc: 0.0296 - auc: 0.6266 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 549/1170 [=============>................] - ETA: 2s - loss: 1.6909 - acc: 0.0299 - auc: 0.6249 - precision: 0.0299 - recall: 1.0000 - f1: 0.0564\n",
            " 563/1170 [=============>................] - ETA: 2s - loss: 1.6908 - acc: 0.0297 - auc: 0.6262 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 572/1170 [=============>................] - ETA: 2s - loss: 1.6907 - acc: 0.0298 - auc: 0.6260 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 580/1170 [=============>................] - ETA: 2s - loss: 1.6912 - acc: 0.0298 - auc: 0.6240 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: 1.6905 - acc: 0.0296 - auc: 0.6235 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 601/1170 [==============>...............] - ETA: 2s - loss: 1.6902 - acc: 0.0295 - auc: 0.6241 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 616/1170 [==============>...............] - ETA: 2s - loss: 1.6905 - acc: 0.0294 - auc: 0.6229 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 638/1170 [===============>..............] - ETA: 2s - loss: 1.6892 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 660/1170 [===============>..............] - ETA: 2s - loss: 1.6900 - acc: 0.0295 - auc: 0.6230 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 671/1170 [================>.............] - ETA: 2s - loss: 1.6903 - acc: 0.0292 - auc: 0.6227 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 684/1170 [================>.............] - ETA: 2s - loss: 1.6909 - acc: 0.0292 - auc: 0.6221 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 1.6908 - acc: 0.0292 - auc: 0.6221 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: 1.6910 - acc: 0.0295 - auc: 0.6211 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: 1.6911 - acc: 0.0293 - auc: 0.6207 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: 1.6908 - acc: 0.0292 - auc: 0.6206 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: 1.6906 - acc: 0.0291 - auc: 0.6203 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: 1.6902 - acc: 0.0292 - auc: 0.6230 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: 1.6904 - acc: 0.0291 - auc: 0.6217 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: 1.6911 - acc: 0.0293 - auc: 0.6219 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: 1.6911 - acc: 0.0292 - auc: 0.6212 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 1.6917 - acc: 0.0293 - auc: 0.6200 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 810/1170 [===================>..........] - ETA: 1s - loss: 1.6927 - acc: 0.0293 - auc: 0.6193 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 823/1170 [====================>.........] - ETA: 1s - loss: 1.6921 - acc: 0.0290 - auc: 0.6197 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 830/1170 [====================>.........] - ETA: 1s - loss: 1.6919 - acc: 0.0289 - auc: 0.6205 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: 1.6918 - acc: 0.0289 - auc: 0.6206 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 843/1170 [====================>.........] - ETA: 1s - loss: 1.6926 - acc: 0.0290 - auc: 0.6201 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 853/1170 [====================>.........] - ETA: 1s - loss: 1.6925 - acc: 0.0290 - auc: 0.6203 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: 1.6927 - acc: 0.0291 - auc: 0.6199 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 879/1170 [=====================>........] - ETA: 1s - loss: 1.6932 - acc: 0.0291 - auc: 0.6199 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: 1.6930 - acc: 0.0292 - auc: 0.6208 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 900/1170 [======================>.......] - ETA: 1s - loss: 1.6931 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 911/1170 [======================>.......] - ETA: 1s - loss: 1.6933 - acc: 0.0294 - auc: 0.6213 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 919/1170 [======================>.......] - ETA: 1s - loss: 1.6930 - acc: 0.0293 - auc: 0.6208 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 931/1170 [======================>.......] - ETA: 1s - loss: 1.6930 - acc: 0.0291 - auc: 0.6202 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 1.6931 - acc: 0.0292 - auc: 0.6197 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6208 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: 1.6928 - acc: 0.0291 - auc: 0.6212 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: 1.6931 - acc: 0.0290 - auc: 0.6200 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: 1.6935 - acc: 0.0290 - auc: 0.6185 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: 1.6930 - acc: 0.0290 - auc: 0.6190 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: 1.6929 - acc: 0.0290 - auc: 0.6190 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6205 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: 1.6927 - acc: 0.0291 - auc: 0.6207 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: 1.6926 - acc: 0.0292 - auc: 0.6214 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: 1.6929 - acc: 0.0293 - auc: 0.6214 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: 1.6930 - acc: 0.0293 - auc: 0.6213 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: 1.6925 - acc: 0.0294 - auc: 0.6247 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: 1.6935 - acc: 0.0295 - auc: 0.6229 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 1.6937 - acc: 0.0295 - auc: 0.6219 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: 1.6940 - acc: 0.0295 - auc: 0.6211 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: 1.6935 - acc: 0.0295 - auc: 0.6218 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: 1.6938 - acc: 0.0296 - auc: 0.6223 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 00:19:18.245401                             \n",
            "100%|| 69/69 [05:02<00:00, 302.57s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 69 trials to 70 (+1) trials\n",
            "2023-07-31 00:19:18.348550                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7478430445712738, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_69', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_138_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_138', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_138', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_138', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_138', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_139', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_139', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_139', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_139', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_69', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_276', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_207', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_277', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_208', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_278', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_209', 'trainable': True, 'dtype': 'float32', 'rate': 0.49012464130586253, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_279', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  61/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:28:39.897339                             \n",
            "100%|| 70/70 [09:21<00:00, 561.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 70 trials to 71 (+1) trials\n",
            "2023-07-31 00:28:40.212711                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6645233154570755, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_70', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_140_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_140', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_140', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_140', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_140', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_141', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_141', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_141', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_141', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_70', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_280', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_210', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_281', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_211', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_282', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_212', 'trainable': True, 'dtype': 'float32', 'rate': 0.4891477145793152, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_283', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 70/71 [09:25<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  59/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 382/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1094/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:38:11.708763                             \n",
            "100%|| 71/71 [09:31<00:00, 571.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 71 trials to 72 (+1) trials\n",
            "2023-07-31 00:38:11.867400                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6772052987280605, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_71', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_142_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_142', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_142', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_142', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_142', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_143', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_143', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_143', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_143', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_71', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_284', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_213', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_285', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_214', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_286', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_215', 'trainable': True, 'dtype': 'float32', 'rate': 0.2172914528564956, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_287', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 24s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  19/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  36/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  46/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  64/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 4s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 4s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 4s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 4s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 4s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 4s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:42:57.751566                             \n",
            "100%|| 72/72 [04:46<00:00, 286.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 72 trials to 73 (+1) trials\n",
            "2023-07-31 00:42:57.970976                             \n",
            "{'name': 'Adam', 'learning_rate': 0.33104385724238167, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_72', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_144_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_144', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_144', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_144', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_144', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_145', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_145', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_145', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_145', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_72', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_288', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_216', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_289', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_217', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_290', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_218', 'trainable': True, 'dtype': 'float32', 'rate': 0.48784262463923833, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_291', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 72/73 [08:36<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0017s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 00:51:40.307012                             \n",
            "100%|| 73/73 [08:42<00:00, 522.44s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 73 trials to 74 (+1) trials\n",
            "2023-07-31 00:51:40.426397                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3090876398904634, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_73', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_146_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_146', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_146', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_146', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_146', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_147', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_147', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_147', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_147', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_73', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_292', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_219', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_293', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_220', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_294', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_221', 'trainable': True, 'dtype': 'float32', 'rate': 0.49794414100655127, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_295', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:00:19.442277                             \n",
            "100%|| 74/74 [08:39<00:00, 519.09s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 74 trials to 75 (+1) trials\n",
            "2023-07-31 01:00:19.526390                             \n",
            "{'name': 'Adam', 'learning_rate': 0.09873581962166446, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_74', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_148_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_148', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_148', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_148', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_148', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_149', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_149', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_149', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_149', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_74', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_296', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_222', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_297', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_223', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_298', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_224', 'trainable': True, 'dtype': 'float32', 'rate': 0.4996848091283176, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_299', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 74/75 [04:38<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
            "  28/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 3s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:05:04.392253                             \n",
            "100%|| 75/75 [04:44<00:00, 284.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 75 trials to 76 (+1) trials\n",
            "2023-07-31 01:05:04.484346                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3530785775140963, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_75', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_150_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_150', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_150', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_150', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_150', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_151', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_151', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_151', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_151', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_75', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_300', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_225', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_301', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_226', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_302', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_227', 'trainable': True, 'dtype': 'float32', 'rate': 0.4975547322075673, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_303', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 75/76 [04:29<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:09:38.583007                             \n",
            "100%|| 76/76 [04:34<00:00, 274.20s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 76 trials to 77 (+1) trials\n",
            "2023-07-31 01:09:38.689083                             \n",
            "{'name': 'Adam', 'learning_rate': 0.38523016696577345, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_76', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_152_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_152', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_152', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_152', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_152', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_153', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_153', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_153', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_153', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_76', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_304', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_228', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_305', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_229', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_306', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_230', 'trainable': True, 'dtype': 'float32', 'rate': 0.3398334611020291, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_307', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  17/1170 [..............................] - ETA: 3s - loss: nan - acc: 0.9596 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  60/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 3s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:18:10.195919                             \n",
            "100%|| 77/77 [08:31<00:00, 511.61s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 77 trials to 78 (+1) trials\n",
            "2023-07-31 01:18:10.312405                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7565104372977938, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_77', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_154_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_154', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_154', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_154', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_154', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_155', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_155', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_155', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_155', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_77', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_308', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_231', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_309', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_232', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_310', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_233', 'trainable': True, 'dtype': 'float32', 'rate': 0.2286543694712219, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_311', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  64/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 3s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1038/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 5s 4ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:22:53.762254                             \n",
            "100%|| 78/78 [04:43<00:00, 283.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 78 trials to 79 (+1) trials\n",
            "2023-07-31 01:22:53.860975                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8719978625410024, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_78', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_156_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_156', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_156', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_156', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_156', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_157', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_157', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_157', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_157', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_78', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_312', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_234', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_313', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_235', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_314', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_236', 'trainable': True, 'dtype': 'float32', 'rate': 0.4884363160629447, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_315', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 78/79 [04:13<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0027s). Check your callbacks.\n",
            "  29/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 295/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 596/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:27:11.802797                             \n",
            "100%|| 79/79 [04:18<00:00, 258.04s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 79 trials to 80 (+1) trials\n",
            "2023-07-31 01:27:11.908512                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4130029240297524, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_79', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_158_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_158', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_158', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_158', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_158', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_159', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_159', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_159', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_159', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_79', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_316', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_237', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_317', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_238', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_318', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_239', 'trainable': True, 'dtype': 'float32', 'rate': 0.48853537177932216, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_319', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1000/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:31:31.724769                             \n",
            "100%|| 80/80 [04:19<00:00, 259.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 80 trials to 81 (+1) trials\n",
            "2023-07-31 01:31:31.814017                             \n",
            "{'name': 'Adam', 'learning_rate': 1.1681975858502103e-06, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_80', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_160_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_160', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_160', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_160', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_160', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_161', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_161', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_161', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_161', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_80', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_320', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_240', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_321', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_241', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_322', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_242', 'trainable': True, 'dtype': 'float32', 'rate': 0.4896752360492409, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_323', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 27s - loss: 2.5851 - acc: 0.1250 - auc: 0.3000 - precision: 0.0667 - recall: 1.0000 - f1: 0.1250\n",
            "  34/1170 [..............................] - ETA: 1s - loss: 2.3936 - acc: 0.0423 - auc: 0.6024 - precision: 0.0343 - recall: 0.9737 - f1: 0.0640 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.3963 - acc: 0.0435 - auc: 0.6043 - precision: 0.0350 - recall: 0.9808 - f1: 0.0653\n",
            "  65/1170 [>.............................] - ETA: 2s - loss: 2.4036 - acc: 0.0457 - auc: 0.5960 - precision: 0.0364 - recall: 0.9868 - f1: 0.0682\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: 2.3863 - acc: 0.0441 - auc: 0.6236 - precision: 0.0340 - recall: 0.9885 - f1: 0.0637\n",
            "  91/1170 [=>............................] - ETA: 3s - loss: 2.3759 - acc: 0.0422 - auc: 0.6149 - precision: 0.0323 - recall: 0.9894 - f1: 0.0605\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: 2.3711 - acc: 0.0426 - auc: 0.6204 - precision: 0.0316 - recall: 0.9910 - f1: 0.0594\n",
            " 118/1170 [==>...........................] - ETA: 3s - loss: 2.3746 - acc: 0.0434 - auc: 0.6127 - precision: 0.0322 - recall: 0.9917 - f1: 0.0604\n",
            " 123/1170 [==>...........................] - ETA: 3s - loss: 2.3755 - acc: 0.0440 - auc: 0.6203 - precision: 0.0324 - recall: 0.9921 - f1: 0.0609\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: 2.3719 - acc: 0.0424 - auc: 0.6280 - precision: 0.0318 - recall: 0.9928 - f1: 0.0599\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: 2.3765 - acc: 0.0427 - auc: 0.6141 - precision: 0.0323 - recall: 0.9867 - f1: 0.0607\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: 2.3787 - acc: 0.0435 - auc: 0.6114 - precision: 0.0325 - recall: 0.9812 - f1: 0.0611\n",
            " 164/1170 [===>..........................] - ETA: 3s - loss: 2.3723 - acc: 0.0423 - auc: 0.6055 - precision: 0.0314 - recall: 0.9819 - f1: 0.0591\n",
            " 178/1170 [===>..........................] - ETA: 3s - loss: 2.3725 - acc: 0.0421 - auc: 0.6005 - precision: 0.0314 - recall: 0.9833 - f1: 0.0591\n",
            " 197/1170 [====>.........................] - ETA: 3s - loss: 2.3684 - acc: 0.0414 - auc: 0.5994 - precision: 0.0308 - recall: 0.9846 - f1: 0.0580\n",
            " 217/1170 [====>.........................] - ETA: 3s - loss: 2.3670 - acc: 0.0416 - auc: 0.5930 - precision: 0.0306 - recall: 0.9859 - f1: 0.0576\n",
            " 237/1170 [=====>........................] - ETA: 3s - loss: 2.3666 - acc: 0.0417 - auc: 0.5861 - precision: 0.0306 - recall: 0.9871 - f1: 0.0575\n",
            " 257/1170 [=====>........................] - ETA: 3s - loss: 2.3671 - acc: 0.0415 - auc: 0.5836 - precision: 0.0306 - recall: 0.9881 - f1: 0.0576\n",
            " 282/1170 [======>.......................] - ETA: 3s - loss: 2.3639 - acc: 0.0407 - auc: 0.5839 - precision: 0.0301 - recall: 0.9890 - f1: 0.0568\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: 2.3635 - acc: 0.0401 - auc: 0.5754 - precision: 0.0298 - recall: 0.9825 - f1: 0.0561\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: 2.3618 - acc: 0.0397 - auc: 0.5765 - precision: 0.0295 - recall: 0.9829 - f1: 0.0557\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: 2.3626 - acc: 0.0397 - auc: 0.5843 - precision: 0.0297 - recall: 0.9839 - f1: 0.0560\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: 2.3641 - acc: 0.0403 - auc: 0.5871 - precision: 0.0301 - recall: 0.9851 - f1: 0.0566\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: 2.3651 - acc: 0.0405 - auc: 0.5869 - precision: 0.0301 - recall: 0.9805 - f1: 0.0567\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: 2.3631 - acc: 0.0404 - auc: 0.5896 - precision: 0.0298 - recall: 0.9811 - f1: 0.0562\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: 2.3687 - acc: 0.0412 - auc: 0.5885 - precision: 0.0307 - recall: 0.9823 - f1: 0.0578\n",
            " 411/1170 [=========>....................] - ETA: 2s - loss: 2.3670 - acc: 0.0407 - auc: 0.5866 - precision: 0.0304 - recall: 0.9826 - f1: 0.0573\n",
            " 418/1170 [=========>....................] - ETA: 2s - loss: 2.3693 - acc: 0.0412 - auc: 0.5886 - precision: 0.0308 - recall: 0.9807 - f1: 0.0579\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: 2.3673 - acc: 0.0408 - auc: 0.5872 - precision: 0.0304 - recall: 0.9790 - f1: 0.0572\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: 2.3653 - acc: 0.0406 - auc: 0.5858 - precision: 0.0301 - recall: 0.9795 - f1: 0.0566\n",
            " 473/1170 [===========>..................] - ETA: 2s - loss: 2.3650 - acc: 0.0409 - auc: 0.5894 - precision: 0.0301 - recall: 0.9804 - f1: 0.0567\n",
            " 495/1170 [===========>..................] - ETA: 2s - loss: 2.3629 - acc: 0.0403 - auc: 0.5896 - precision: 0.0297 - recall: 0.9789 - f1: 0.0560\n",
            " 515/1170 [============>.................] - ETA: 2s - loss: 2.3625 - acc: 0.0399 - auc: 0.5878 - precision: 0.0296 - recall: 0.9797 - f1: 0.0559\n",
            " 533/1170 [============>.................] - ETA: 2s - loss: 2.3610 - acc: 0.0399 - auc: 0.5869 - precision: 0.0294 - recall: 0.9802 - f1: 0.0555\n",
            " 544/1170 [============>.................] - ETA: 2s - loss: 2.3613 - acc: 0.0400 - auc: 0.5850 - precision: 0.0295 - recall: 0.9807 - f1: 0.0556\n",
            " 550/1170 [=============>................] - ETA: 2s - loss: 2.3626 - acc: 0.0402 - auc: 0.5818 - precision: 0.0297 - recall: 0.9810 - f1: 0.0560\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: 2.3621 - acc: 0.0400 - auc: 0.5775 - precision: 0.0296 - recall: 0.9817 - f1: 0.0558\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: 2.3605 - acc: 0.0397 - auc: 0.5754 - precision: 0.0293 - recall: 0.9804 - f1: 0.0552\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: 2.3604 - acc: 0.0396 - auc: 0.5771 - precision: 0.0293 - recall: 0.9810 - f1: 0.0553\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: 2.3608 - acc: 0.0395 - auc: 0.5758 - precision: 0.0293 - recall: 0.9816 - f1: 0.0553\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: 2.3602 - acc: 0.0394 - auc: 0.5769 - precision: 0.0293 - recall: 0.9821 - f1: 0.0552\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: 2.3597 - acc: 0.0394 - auc: 0.5800 - precision: 0.0292 - recall: 0.9824 - f1: 0.0551\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: 2.3576 - acc: 0.0392 - auc: 0.5780 - precision: 0.0289 - recall: 0.9828 - f1: 0.0545\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: 2.3594 - acc: 0.0396 - auc: 0.5770 - precision: 0.0292 - recall: 0.9838 - f1: 0.0550\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: 2.3576 - acc: 0.0391 - auc: 0.5797 - precision: 0.0290 - recall: 0.9843 - f1: 0.0546\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: 2.3592 - acc: 0.0390 - auc: 0.5768 - precision: 0.0292 - recall: 0.9851 - f1: 0.0550\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: 2.3592 - acc: 0.0390 - auc: 0.5767 - precision: 0.0292 - recall: 0.9854 - f1: 0.0550\n",
            " 817/1170 [===================>..........] - ETA: 1s - loss: 2.3577 - acc: 0.0387 - auc: 0.5773 - precision: 0.0290 - recall: 0.9855 - f1: 0.0546\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: 2.3563 - acc: 0.0384 - auc: 0.5789 - precision: 0.0287 - recall: 0.9857 - f1: 0.0542\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: 2.3577 - acc: 0.0385 - auc: 0.5787 - precision: 0.0289 - recall: 0.9849 - f1: 0.0545\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: 2.3584 - acc: 0.0387 - auc: 0.5786 - precision: 0.0290 - recall: 0.9853 - f1: 0.0547\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: 2.3592 - acc: 0.0389 - auc: 0.5762 - precision: 0.0292 - recall: 0.9856 - f1: 0.0549\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: 2.3599 - acc: 0.0391 - auc: 0.5751 - precision: 0.0293 - recall: 0.9860 - f1: 0.0552\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: 2.3586 - acc: 0.0390 - auc: 0.5743 - precision: 0.0291 - recall: 0.9864 - f1: 0.0548\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: 2.3577 - acc: 0.0389 - auc: 0.5749 - precision: 0.0290 - recall: 0.9868 - f1: 0.0546\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: 2.3576 - acc: 0.0388 - auc: 0.5728 - precision: 0.0289 - recall: 0.9859 - f1: 0.0545\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: 2.3577 - acc: 0.0388 - auc: 0.5734 - precision: 0.0289 - recall: 0.9861 - f1: 0.0545\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: 2.3577 - acc: 0.0389 - auc: 0.5711 - precision: 0.0289 - recall: 0.9863 - f1: 0.0545\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: 2.3590 - acc: 0.0392 - auc: 0.5714 - precision: 0.0291 - recall: 0.9865 - f1: 0.0549\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: 2.3586 - acc: 0.0390 - auc: 0.5731 - precision: 0.0291 - recall: 0.9867 - f1: 0.0548\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: 2.3597 - acc: 0.0393 - auc: 0.5738 - precision: 0.0293 - recall: 0.9870 - f1: 0.0552\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: 2.3598 - acc: 0.0393 - auc: 0.5758 - precision: 0.0293 - recall: 0.9873 - f1: 0.0552\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: 2.3602 - acc: 0.0394 - auc: 0.5765 - precision: 0.0294 - recall: 0.9876 - f1: 0.0554\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: 2.3604 - acc: 0.0393 - auc: 0.5753 - precision: 0.0294 - recall: 0.9878 - f1: 0.0554\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 2.3609 - acc: 0.0394 - auc: 0.5753 - precision: 0.0295 - recall: 0.9880 - f1: 0.0556\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: 2.3605 - acc: 0.0393 - auc: 0.5753 - precision: 0.0295 - recall: 0.9881 - f1: 0.0555\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.3609 - acc: 0.0393 - auc: 0.5762 - precision: 0.0295 - recall: 0.9882 - f1: 0.0556\n",
            "\n",
            "2023-07-31 01:32:56.498271                             \n",
            "100%|| 81/81 [01:24<00:00, 84.76s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 81 trials to 82 (+1) trials\n",
            "2023-07-31 01:32:56.576356                             \n",
            "{'name': 'Adam', 'learning_rate': 0.07252457999446249, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_81', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_162_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_162', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_162', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_162', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_162', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_163', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_163', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_163', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_163', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_81', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_324', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_243', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_325', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_244', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_326', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_245', 'trainable': True, 'dtype': 'float32', 'rate': 0.2208892933528847, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_327', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  64/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 955/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:37:06.821744                             \n",
            "100%|| 82/82 [04:10<00:00, 250.32s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 82 trials to 83 (+1) trials\n",
            "2023-07-31 01:37:07.008827                             \n",
            "{'name': 'Adam', 'learning_rate': 0.37673140677437306, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_82', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_164_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_164', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_164', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_164', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_164', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_165', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_165', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_165', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_165', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_82', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_328', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_246', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_329', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_247', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_330', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_248', 'trainable': True, 'dtype': 'float32', 'rate': 0.036465482158622914, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_331', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 313/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:44:53.906677                             \n",
            "100%|| 83/83 [07:46<00:00, 466.98s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 83 trials to 84 (+1) trials\n",
            "2023-07-31 01:44:54.069166                             \n",
            "{'name': 'Adam', 'learning_rate': 0.878876693571428, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_83', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_166_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_166', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_166', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_166', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_166', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_167', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_167', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_167', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_167', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_83', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_332', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_249', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_333', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_250', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_334', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_251', 'trainable': True, 'dtype': 'float32', 'rate': 0.22215052728998824, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_335', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  66/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 425/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 604/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 705/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:48:00.550149                             \n",
            "100%|| 84/84 [03:06<00:00, 186.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 84 trials to 85 (+1) trials\n",
            "2023-07-31 01:48:00.625999                             \n",
            "{'name': 'Adam', 'learning_rate': 0.6979557003572269, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_84', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_168_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_168', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_168', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_168', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_168', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_169', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_169', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_169', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_169', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_84', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_336', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_252', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_337', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_253', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_338', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_254', 'trainable': True, 'dtype': 'float32', 'rate': 0.044140841255347535, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_339', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 01:52:12.051337                             \n",
            "100%|| 85/85 [04:11<00:00, 251.50s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 85 trials to 86 (+1) trials\n",
            "2023-07-31 01:52:12.237753                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0031117218755322743, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_85', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_170_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_170', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_170', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_170', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_170', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_171', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_171', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_171', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_171', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_85', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_340', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_255', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_341', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_256', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_342', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_257', 'trainable': True, 'dtype': 'float32', 'rate': 0.22806410166314728, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_343', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:00:06.593802                             \n",
            "100%|| 86/86 [07:54<00:00, 474.47s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 86 trials to 87 (+1) trials\n",
            "2023-07-31 02:00:06.717584                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9115174049288475, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_86', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_172_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_172', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_172', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_172', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_172', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_173', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_173', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_173', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_173', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_86', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_344', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_258', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_345', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_259', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_346', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_260', 'trainable': True, 'dtype': 'float32', 'rate': 0.2195744362735619, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_347', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:04:18.168489                             \n",
            "100%|| 87/87 [04:11<00:00, 251.53s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 87 trials to 88 (+1) trials\n",
            "2023-07-31 02:04:18.339809                             \n",
            "{'name': 'Adam', 'learning_rate': 0.1942831107538497, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_87', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_174_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_174', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_174', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_174', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_174', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_175', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_175', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_175', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_175', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_87', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_348', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_261', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_349', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_262', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_350', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_263', 'trainable': True, 'dtype': 'float32', 'rate': 0.23090802937024182, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_351', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:08:27.181482                             \n",
            "100%|| 88/88 [04:08<00:00, 248.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 88 trials to 89 (+1) trials\n",
            "2023-07-31 02:08:27.339240                             \n",
            "{'name': 'Adam', 'learning_rate': 0.0027993636104718246, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_88', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_176_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_176', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_176', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_176', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_176', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_177', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_177', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_177', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_177', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_88', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_352', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_264', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_353', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_265', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_354', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_266', 'trainable': True, 'dtype': 'float32', 'rate': 0.04156098895047722, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_355', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: 2.6962 - acc: 0.0625 - auc: 0.3083 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  32/1170 [..............................] - ETA: 1s - loss: 2.4498 - acc: 0.0332 - auc: 0.6749 - precision: 0.0332 - recall: 1.0000 - f1: 0.0618 \n",
            "  59/1170 [>.............................] - ETA: 1s - loss: 2.4686 - acc: 0.0360 - auc: 0.6604 - precision: 0.0360 - recall: 1.0000 - f1: 0.0672\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: 2.4758 - acc: 0.0371 - auc: 0.6582 - precision: 0.0371 - recall: 1.0000 - f1: 0.0692\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: 2.4679 - acc: 0.0338 - auc: 0.6574 - precision: 0.0338 - recall: 1.0000 - f1: 0.0633\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: 2.4690 - acc: 0.0316 - auc: 0.6580 - precision: 0.0316 - recall: 1.0000 - f1: 0.0592\n",
            " 104/1170 [=>............................] - ETA: 3s - loss: 2.4732 - acc: 0.0309 - auc: 0.6577 - precision: 0.0309 - recall: 1.0000 - f1: 0.0581\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: 2.4696 - acc: 0.0320 - auc: 0.6631 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 141/1170 [==>...........................] - ETA: 3s - loss: 2.4760 - acc: 0.0321 - auc: 0.6637 - precision: 0.0321 - recall: 1.0000 - f1: 0.0605\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: 2.4742 - acc: 0.0327 - auc: 0.6652 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: 2.4804 - acc: 0.0318 - auc: 0.6541 - precision: 0.0318 - recall: 1.0000 - f1: 0.0597\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: 2.4831 - acc: 0.0311 - auc: 0.6468 - precision: 0.0311 - recall: 1.0000 - f1: 0.0585\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: 2.4802 - acc: 0.0305 - auc: 0.6391 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: 2.4816 - acc: 0.0304 - auc: 0.6380 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: 2.4798 - acc: 0.0307 - auc: 0.6392 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: 2.4783 - acc: 0.0301 - auc: 0.6431 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: 2.4817 - acc: 0.0300 - auc: 0.6394 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: 2.4788 - acc: 0.0295 - auc: 0.6438 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: 2.4774 - acc: 0.0301 - auc: 0.6440 - precision: 0.0301 - recall: 1.0000 - f1: 0.0567\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: 2.4762 - acc: 0.0304 - auc: 0.6452 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: 2.4747 - acc: 0.0303 - auc: 0.6445 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: 2.4731 - acc: 0.0305 - auc: 0.6450 - precision: 0.0305 - recall: 1.0000 - f1: 0.0575\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: 2.4752 - acc: 0.0308 - auc: 0.6394 - precision: 0.0308 - recall: 1.0000 - f1: 0.0580\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: 2.4729 - acc: 0.0310 - auc: 0.6438 - precision: 0.0310 - recall: 1.0000 - f1: 0.0583\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: 2.4743 - acc: 0.0307 - auc: 0.6440 - precision: 0.0307 - recall: 1.0000 - f1: 0.0579\n",
            " 455/1170 [==========>...................] - ETA: 2s - loss: 2.4750 - acc: 0.0305 - auc: 0.6452 - precision: 0.0305 - recall: 1.0000 - f1: 0.0574\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: 2.4751 - acc: 0.0304 - auc: 0.6460 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: 2.4730 - acc: 0.0300 - auc: 0.6457 - precision: 0.0300 - recall: 1.0000 - f1: 0.0566\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: 2.4740 - acc: 0.0297 - auc: 0.6435 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: 2.4727 - acc: 0.0297 - auc: 0.6436 - precision: 0.0297 - recall: 1.0000 - f1: 0.0560\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: 2.4746 - acc: 0.0297 - auc: 0.6402 - precision: 0.0297 - recall: 1.0000 - f1: 0.0561\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: 2.4739 - acc: 0.0296 - auc: 0.6398 - precision: 0.0296 - recall: 1.0000 - f1: 0.0559\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: 2.4739 - acc: 0.0295 - auc: 0.6407 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: 2.4747 - acc: 0.0293 - auc: 0.6378 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: 2.4733 - acc: 0.0296 - auc: 0.6369 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: 2.4746 - acc: 0.0293 - auc: 0.6374 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: 2.4766 - acc: 0.0292 - auc: 0.6362 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: 2.4765 - acc: 0.0292 - auc: 0.6371 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: 2.4758 - acc: 0.0292 - auc: 0.6372 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: 2.4756 - acc: 0.0294 - auc: 0.6362 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: 2.4755 - acc: 0.0291 - auc: 0.6345 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: 2.4754 - acc: 0.0290 - auc: 0.6353 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: 2.4751 - acc: 0.0292 - auc: 0.6363 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: 2.4759 - acc: 0.0293 - auc: 0.6347 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: 2.4764 - acc: 0.0291 - auc: 0.6344 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: 2.4765 - acc: 0.0289 - auc: 0.6350 - precision: 0.0289 - recall: 1.0000 - f1: 0.0544\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: 2.4774 - acc: 0.0290 - auc: 0.6327 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: 2.4777 - acc: 0.0291 - auc: 0.6317 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: 2.4789 - acc: 0.0292 - auc: 0.6315 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: 2.4782 - acc: 0.0295 - auc: 0.6329 - precision: 0.0295 - recall: 1.0000 - f1: 0.0555\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: 2.4787 - acc: 0.0292 - auc: 0.6318 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: 2.4785 - acc: 0.0291 - auc: 0.6302 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: 2.4785 - acc: 0.0291 - auc: 0.6296 - precision: 0.0291 - recall: 1.0000 - f1: 0.0548\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: 2.4798 - acc: 0.0290 - auc: 0.6282 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: 2.4803 - acc: 0.0290 - auc: 0.6272 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: 2.4792 - acc: 0.0290 - auc: 0.6301 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: 2.4790 - acc: 0.0290 - auc: 0.6288 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: 2.4791 - acc: 0.0291 - auc: 0.6284 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: 2.4789 - acc: 0.0292 - auc: 0.6304 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: 2.4790 - acc: 0.0294 - auc: 0.6311 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: 2.4791 - acc: 0.0295 - auc: 0.6325 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: 2.4800 - acc: 0.0295 - auc: 0.6309 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: 2.4805 - acc: 0.0295 - auc: 0.6300 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: 2.4801 - acc: 0.0296 - auc: 0.6295 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.4802 - acc: 0.0296 - auc: 0.6295 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 02:12:51.012003                             \n",
            "100%|| 89/89 [04:23<00:00, 263.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 89 trials to 90 (+1) trials\n",
            "2023-07-31 02:12:51.190083                             \n",
            "{'name': 'Adam', 'learning_rate': 0.8944920446616569, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_89', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_178_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_178', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_178', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_178', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_178', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_179', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_179', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_179', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_179', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_89', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_356', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_267', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_357', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_268', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_358', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_269', 'trainable': True, 'dtype': 'float32', 'rate': 0.436637949227791, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_359', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 974/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:20:39.466286                             \n",
            "100%|| 90/90 [07:48<00:00, 468.35s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 90 trials to 91 (+1) trials\n",
            "2023-07-31 02:20:39.549186                             \n",
            "{'name': 'Adam', 'learning_rate': 0.2828772225492993, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_90', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_180_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_180', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_180', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_180', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_180', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_181', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_181', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_181', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_181', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_90', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_360', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_270', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_361', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_271', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_362', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_272', 'trainable': True, 'dtype': 'float32', 'rate': 0.21576211165428547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_363', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:23:56.428166                             \n",
            "100%|| 91/91 [03:16<00:00, 196.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 91 trials to 92 (+1) trials\n",
            "2023-07-31 02:23:56.588463                             \n",
            "{'name': 'Adam', 'learning_rate': 0.12494711923816655, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_91', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_182_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_182', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_182', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_182', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_182', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_183', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_183', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_183', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_183', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_91', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_364', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_273', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_365', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_274', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_366', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_275', 'trainable': True, 'dtype': 'float32', 'rate': 0.2251749345678653, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_367', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 385/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 478/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 914/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:29:30.801066                             \n",
            "100%|| 92/92 [05:34<00:00, 334.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 92 trials to 93 (+1) trials\n",
            "2023-07-31 02:29:30.881192                             \n",
            "{'name': 'Adam', 'learning_rate': 0.4779636574447378, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_92', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_184_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_184', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_184', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_184', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_184', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_185', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_185', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_185', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_185', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_92', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_368', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_276', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_369', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_277', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_370', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_278', 'trainable': True, 'dtype': 'float32', 'rate': 0.43005632673442973, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_371', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1075/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:37:13.374198                             \n",
            "100%|| 93/93 [07:42<00:00, 462.56s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 93 trials to 94 (+1) trials\n",
            "2023-07-31 02:37:13.519540                             \n",
            "{'name': 'Adam', 'learning_rate': 0.9016314767543332, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_93', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_186_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_186', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_186', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_186', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_186', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_187', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_187', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_187', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_187', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_93', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_372', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_279', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_373', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_280', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_374', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_281', 'trainable': True, 'dtype': 'float32', 'rate': 0.21960542389551224, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_375', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:45:20.127697                             \n",
            "100%|| 94/94 [08:06<00:00, 486.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 94 trials to 95 (+1) trials\n",
            "2023-07-31 02:45:20.195780                             \n",
            "{'name': 'Adam', 'learning_rate': 0.11889646878078022, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_94', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_188_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_188', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_188', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_188', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_188', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_189', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_189', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_189', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_189', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_94', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_376', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_282', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_377', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_283', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_378', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_284', 'trainable': True, 'dtype': 'float32', 'rate': 0.23201311317537693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_379', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:53:12.098944                             \n",
            "100%|| 95/95 [07:51<00:00, 471.97s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 95 trials to 96 (+1) trials\n",
            "2023-07-31 02:53:12.263048                             \n",
            "{'name': 'Adam', 'learning_rate': 0.39236795258078544, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_95', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_190_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_190', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_190', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_190', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_190', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_191', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_191', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_191', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_191', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_95', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_380', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_285', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_381', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_286', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_382', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_287', 'trainable': True, 'dtype': 'float32', 'rate': 0.22660071027811238, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_383', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:56:13.962318                             \n",
            "100%|| 96/96 [03:01<00:00, 181.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 96 trials to 97 (+1) trials\n",
            "2023-07-31 02:56:14.046131                             \n",
            "{'name': 'Adam', 'learning_rate': 0.3017889324799028, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_96', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_192_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_192', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_192', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_192', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_192', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_193', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_193', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_193', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_193', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_96', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_384', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_288', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_385', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_289', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_386', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_290', 'trainable': True, 'dtype': 'float32', 'rate': 0.21904727419026745, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_387', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  92/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 798/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 02:59:24.141754                             \n",
            "100%|| 97/97 [03:10<00:00, 190.16s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 97 trials to 98 (+1) trials\n",
            "2023-07-31 02:59:24.217485                             \n",
            "{'name': 'Adam', 'learning_rate': 0.5399404399508391, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_97', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_194_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_194', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_194', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_194', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_194', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_195', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_195', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_195', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_195', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_97', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_388', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_291', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_389', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_292', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_390', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_293', 'trainable': True, 'dtype': 'float32', 'rate': 0.4336728208714064, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_391', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:07:32.685690                             \n",
            "100%|| 98/98 [08:08<00:00, 488.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 98 trials to 99 (+1) trials\n",
            "2023-07-31 03:07:32.871898                             \n",
            "{'name': 'Adam', 'learning_rate': 0.7967569710679964, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_98', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_196_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_196', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_196', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_196', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_196', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_197', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_197', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_197', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_197', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_98', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_392', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_294', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_393', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_295', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_394', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_296', 'trainable': True, 'dtype': 'float32', 'rate': 0.43317691243972584, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_395', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1150/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:10:49.254990                             \n",
            "100%|| 99/99 [03:16<00:00, 196.45s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 99 trials to 100 (+1) trials\n",
            "2023-07-31 03:10:49.326396                              \n",
            "{'name': 'Adam', 'learning_rate': 0.6861134019645244, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_99', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_198_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_198', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_198', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_198', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_198', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_199', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_199', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_199', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_199', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_99', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_396', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_297', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_397', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_298', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_398', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_299', 'trainable': True, 'dtype': 'float32', 'rate': 0.4308741176383089, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_399', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 368/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:18:42.955118                              \n",
            "100%|| 100/100 [07:53<00:00, 473.70s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 100 trials to 101 (+1) trials\n",
            "2023-07-31 03:18:43.111069                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7667599474229218, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_100', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_200_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_200', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_200', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_200', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_200', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_201', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_201', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_201', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_201', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_100', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_400', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_300', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_401', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_301', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_402', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_302', 'trainable': True, 'dtype': 'float32', 'rate': 0.22306338105363288, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_403', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1099/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:22:00.381169                               \n",
            "100%|| 101/101 [03:17<00:00, 197.35s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 101 trials to 102 (+1) trials\n",
            "2023-07-31 03:22:00.477208                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3486892825076603, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_101', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_202_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_202', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_202', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_202', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_202', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_203', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_203', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_203', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_203', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_101', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_404', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_303', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_405', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_304', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_406', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_305', 'trainable': True, 'dtype': 'float32', 'rate': 0.4298415535637269, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_407', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  93/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1117/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:27:14.713552                               \n",
            "100%|| 102/102 [05:14<00:00, 314.31s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 102 trials to 103 (+1) trials\n",
            "2023-07-31 03:27:14.800222                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9392503619423661, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_102', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_204_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_204', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_204', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_204', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_204', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_205', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_205', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_205', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_205', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_102', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_408', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_306', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_409', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_307', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_410', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_308', 'trainable': True, 'dtype': 'float32', 'rate': 0.22259352697507587, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_411', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 536/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:35:10.916444                               \n",
            "100%|| 103/103 [07:56<00:00, 476.19s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 103 trials to 104 (+1) trials\n",
            "2023-07-31 03:35:11.002710                               \n",
            "{'name': 'Adam', 'learning_rate': 0.6991660246432225, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_103', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_206_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_206', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_206', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_206', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_206', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_207', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_207', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_207', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_207', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_103', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_412', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_309', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_413', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_310', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_414', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_311', 'trainable': True, 'dtype': 'float32', 'rate': 0.21544034882656954, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_415', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 21s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  45/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9653 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 577/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 655/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 676/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:40:28.754689                               \n",
            "100%|| 104/104 [05:17<00:00, 317.84s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 104 trials to 105 (+1) trials\n",
            "2023-07-31 03:40:28.946847                               \n",
            "{'name': 'Adam', 'learning_rate': 0.14100621278052225, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_104', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_208_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_208', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_208', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_208', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_208', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_209', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_209', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_209', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_209', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_104', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_416', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_312', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_417', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_313', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_418', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_314', 'trainable': True, 'dtype': 'float32', 'rate': 0.22039179312304882, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_419', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:43:44.421912                               \n",
            "100%|| 105/105 [03:15<00:00, 195.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 105 trials to 106 (+1) trials\n",
            "2023-07-31 03:43:44.513608                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1395947794568499, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_105', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_210_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_210', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_210', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_210', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_210', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_211', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_211', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_211', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_211', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_105', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_420', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_315', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_421', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_316', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_422', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_317', 'trainable': True, 'dtype': 'float32', 'rate': 0.21846608012685215, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_423', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  90/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 545/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 819/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:49:20.706747                               \n",
            "100%|| 106/106 [05:36<00:00, 336.27s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 106 trials to 107 (+1) trials\n",
            "2023-07-31 03:49:20.880808                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5725308147608181, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_106', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_212_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_212', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_212', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_212', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_212', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_213', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_213', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_213', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_213', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_106', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_424', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_318', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_425', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_319', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_426', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_320', 'trainable': True, 'dtype': 'float32', 'rate': 0.2275085049533046, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_427', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 17s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 03:52:37.498248                               \n",
            "100%|| 107/107 [03:16<00:00, 196.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 107 trials to 108 (+1) trials\n",
            "2023-07-31 03:52:37.569889                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1305476129192773, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_107', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_214_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_214', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_214', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_214', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_214', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_215', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_215', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_215', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_215', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_107', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_428', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_321', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_429', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_322', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_430', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_323', 'trainable': True, 'dtype': 'float32', 'rate': 0.2109544449053087, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_431', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:00:33.458029                               \n",
            "100%|| 108/108 [07:55<00:00, 475.96s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 108 trials to 109 (+1) trials\n",
            "2023-07-31 04:00:33.540546                               \n",
            "{'name': 'Adam', 'learning_rate': 0.582004298361701, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_108', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_216_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_216', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_216', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_216', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_216', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_217', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_217', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_217', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_217', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_108', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_432', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_324', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_433', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_325', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_434', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_326', 'trainable': True, 'dtype': 'float32', 'rate': 0.22579048769507476, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_435', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  44/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9659 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 591/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:08:37.261391                               \n",
            "100%|| 109/109 [08:03<00:00, 483.80s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 109 trials to 110 (+1) trials\n",
            "2023-07-31 04:08:37.354093                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3520405344522889, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_109', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_218_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_218', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_218', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_218', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_218', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_219', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_219', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_219', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_219', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_109', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_436', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_327', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_437', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_328', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_438', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_329', 'trainable': True, 'dtype': 'float32', 'rate': 0.22075367791246356, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_439', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 542/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:16:15.179155                               \n",
            "100%|| 110/110 [07:37<00:00, 457.90s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 110 trials to 111 (+1) trials\n",
            "2023-07-31 04:16:15.351619                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9610395331932369, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_110', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_220_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_220', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_220', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_220', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_220', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_221', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_221', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_221', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_221', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_110', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_440', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_330', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_441', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_331', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_442', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_332', 'trainable': True, 'dtype': 'float32', 'rate': 0.2174540923600607, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_443', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  75/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  91/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 627/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:21:18.868778                               \n",
            "100%|| 111/111 [05:03<00:00, 303.60s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 111 trials to 112 (+1) trials\n",
            "2023-07-31 04:21:18.947853                               \n",
            "{'name': 'Adam', 'learning_rate': 0.35221170078001734, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_111', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_222_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_222', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_222', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_222', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_222', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_223', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_223', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_223', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_223', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_111', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_444', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_333', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_445', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_334', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_446', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_335', 'trainable': True, 'dtype': 'float32', 'rate': 0.22375504679145541, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_447', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 505/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 799/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1168/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:24:18.659430                               \n",
            "100%|| 112/112 [02:59<00:00, 179.79s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 112 trials to 113 (+1) trials\n",
            "2023-07-31 04:24:18.744936                               \n",
            "{'name': 'Adam', 'learning_rate': 0.10317751681800515, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_112', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_224_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_224', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_224', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_224', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_224', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_225', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_225', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_225', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_225', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_112', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_448', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_336', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_449', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_337', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_450', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_338', 'trainable': True, 'dtype': 'float32', 'rate': 0.42797496176171357, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_451', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 669/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 995/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:32:06.607046                               \n",
            "100%|| 113/113 [07:47<00:00, 467.93s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 113 trials to 114 (+1) trials\n",
            "2023-07-31 04:32:06.681009                               \n",
            "{'name': 'Adam', 'learning_rate': 0.795454837013674, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_113', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_226_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_226', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_226', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_226', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_226', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_227', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_227', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_227', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_227', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_113', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_452', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_339', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_453', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_340', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_454', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_341', 'trainable': True, 'dtype': 'float32', 'rate': 0.21524949359139395, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_455', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  95/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 156/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9675 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 359/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:39:41.571444                               \n",
            "100%|| 114/114 [07:34<00:00, 454.97s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 114 trials to 115 (+1) trials\n",
            "2023-07-31 04:39:41.740757                               \n",
            "{'name': 'Adam', 'learning_rate': 0.9858889385962811, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_114', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_228_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_228', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_228', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_228', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_228', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_229', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_229', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_229', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_229', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_114', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_456', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_342', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_457', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_343', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_458', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_344', 'trainable': True, 'dtype': 'float32', 'rate': 0.22279618382086963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_459', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 255/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 332/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 361/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 731/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:47:28.751844                               \n",
            "100%|| 115/115 [07:47<00:00, 467.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 115 trials to 116 (+1) trials\n",
            "2023-07-31 04:47:28.833662                               \n",
            "{'name': 'Adam', 'learning_rate': 0.42467851342662793, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_115', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_230_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_230', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_230', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_230', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_230', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_231', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_231', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_231', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_231', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_115', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_460', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_345', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_461', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_346', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_462', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_347', 'trainable': True, 'dtype': 'float32', 'rate': 0.22573050943656292, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_463', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  73/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 275/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 447/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 495/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 576/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 774/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 941/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1047/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:52:45.550570                               \n",
            "100%|| 116/116 [05:16<00:00, 316.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 116 trials to 117 (+1) trials\n",
            "2023-07-31 04:52:45.704135                               \n",
            "{'name': 'Adam', 'learning_rate': 0.834045910594486, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_116', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_232_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_232', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_232', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_232', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_232', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_233', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_233', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_233', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_233', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_116', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_464', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_348', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_465', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_349', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_466', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_350', 'trainable': True, 'dtype': 'float32', 'rate': 0.20975694831375458, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_467', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 336/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 363/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 448/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1131/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 04:55:49.799368                               \n",
            "100%|| 117/117 [03:04<00:00, 184.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 117 trials to 118 (+1) trials\n",
            "2023-07-31 04:55:49.878714                               \n",
            "{'name': 'Adam', 'learning_rate': 0.855118276704653, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_117', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_234_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_234', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_234', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_234', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_234', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_235', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_235', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_235', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_235', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_117', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_468', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_351', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_469', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_352', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_470', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_353', 'trainable': True, 'dtype': 'float32', 'rate': 0.23227477080560258, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_471', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  97/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 613/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 881/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1092/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:03:41.663390                               \n",
            "100%|| 118/118 [07:51<00:00, 471.86s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 118 trials to 119 (+1) trials\n",
            "2023-07-31 05:03:41.831377                               \n",
            "{'name': 'Adam', 'learning_rate': 0.46840495620850564, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_118', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_236_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_236', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_236', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_236', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_236', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_237', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_237', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_237', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_237', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_118', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_472', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_354', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_473', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_355', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_474', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_356', 'trainable': True, 'dtype': 'float32', 'rate': 0.2110646860078896, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_475', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  96/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 916/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1132/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:06:43.476476                               \n",
            "100%|| 119/119 [03:01<00:00, 181.72s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 119 trials to 120 (+1) trials\n",
            "2023-07-31 05:06:43.559794                               \n",
            "{'name': 'Adam', 'learning_rate': 0.009498559702984662, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_119', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_238_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_238', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_238', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_238', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_238', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_239', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_239', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_239', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_239', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_119', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_476', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_357', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_477', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_358', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_478', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_359', 'trainable': True, 'dtype': 'float32', 'rate': 0.22155167459599354, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_479', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: 2.1760 - acc: 0.0625 - auc: 0.1833 - precision: 0.0625 - recall: 1.0000 - f1: 0.1176\n",
            "  33/1170 [..............................] - ETA: 1s - loss: 2.0700 - acc: 0.0331 - auc: 0.6193 - precision: 0.0331 - recall: 1.0000 - f1: 0.0618 \n",
            "  46/1170 [>.............................] - ETA: 2s - loss: 2.0763 - acc: 0.0353 - auc: 0.6341 - precision: 0.0353 - recall: 1.0000 - f1: 0.0658\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: 2.0787 - acc: 0.0371 - auc: 0.6273 - precision: 0.0371 - recall: 1.0000 - f1: 0.0692\n",
            "  85/1170 [=>............................] - ETA: 2s - loss: 2.0760 - acc: 0.0338 - auc: 0.6270 - precision: 0.0338 - recall: 1.0000 - f1: 0.0634\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: 2.0773 - acc: 0.0312 - auc: 0.6287 - precision: 0.0312 - recall: 1.0000 - f1: 0.0586\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: 2.0767 - acc: 0.0323 - auc: 0.6289 - precision: 0.0323 - recall: 1.0000 - f1: 0.0607\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: 2.0765 - acc: 0.0320 - auc: 0.6377 - precision: 0.0320 - recall: 1.0000 - f1: 0.0602\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: 2.0779 - acc: 0.0327 - auc: 0.6382 - precision: 0.0327 - recall: 1.0000 - f1: 0.0615\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: 2.0800 - acc: 0.0320 - auc: 0.6289 - precision: 0.0320 - recall: 1.0000 - f1: 0.0600\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: 2.0800 - acc: 0.0306 - auc: 0.6251 - precision: 0.0306 - recall: 1.0000 - f1: 0.0575\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: 2.0807 - acc: 0.0314 - auc: 0.6191 - precision: 0.0314 - recall: 1.0000 - f1: 0.0590\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: 2.0803 - acc: 0.0307 - auc: 0.6164 - precision: 0.0307 - recall: 1.0000 - f1: 0.0578\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: 2.0800 - acc: 0.0304 - auc: 0.6160 - precision: 0.0304 - recall: 1.0000 - f1: 0.0571\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: 2.0796 - acc: 0.0307 - auc: 0.6102 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: 2.0804 - acc: 0.0302 - auc: 0.6097 - precision: 0.0302 - recall: 1.0000 - f1: 0.0568\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: 2.0795 - acc: 0.0295 - auc: 0.6140 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: 2.0786 - acc: 0.0303 - auc: 0.6168 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 376/1170 [========>.....................] - ETA: 1s - loss: 2.0778 - acc: 0.0303 - auc: 0.6162 - precision: 0.0303 - recall: 1.0000 - f1: 0.0570\n",
            " 387/1170 [========>.....................] - ETA: 2s - loss: 2.0775 - acc: 0.0301 - auc: 0.6161 - precision: 0.0301 - recall: 1.0000 - f1: 0.0568\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: 2.0782 - acc: 0.0306 - auc: 0.6126 - precision: 0.0306 - recall: 1.0000 - f1: 0.0577\n",
            " 431/1170 [==========>...................] - ETA: 1s - loss: 2.0776 - acc: 0.0307 - auc: 0.6153 - precision: 0.0307 - recall: 1.0000 - f1: 0.0577\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: 2.0772 - acc: 0.0304 - auc: 0.6165 - precision: 0.0304 - recall: 1.0000 - f1: 0.0573\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: 2.0774 - acc: 0.0304 - auc: 0.6175 - precision: 0.0304 - recall: 1.0000 - f1: 0.0572\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: 2.0774 - acc: 0.0303 - auc: 0.6163 - precision: 0.0303 - recall: 1.0000 - f1: 0.0571\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: 2.0766 - acc: 0.0300 - auc: 0.6197 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: 2.0760 - acc: 0.0300 - auc: 0.6190 - precision: 0.0300 - recall: 1.0000 - f1: 0.0565\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: 2.0767 - acc: 0.0298 - auc: 0.6172 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: 2.0765 - acc: 0.0297 - auc: 0.6163 - precision: 0.0297 - recall: 1.0000 - f1: 0.0559\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: 2.0768 - acc: 0.0298 - auc: 0.6138 - precision: 0.0298 - recall: 1.0000 - f1: 0.0563\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: 2.0770 - acc: 0.0298 - auc: 0.6120 - precision: 0.0298 - recall: 1.0000 - f1: 0.0562\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: 2.0771 - acc: 0.0295 - auc: 0.6106 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: 2.0770 - acc: 0.0295 - auc: 0.6101 - precision: 0.0295 - recall: 1.0000 - f1: 0.0557\n",
            " 626/1170 [===============>..............] - ETA: 1s - loss: 2.0770 - acc: 0.0293 - auc: 0.6088 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: 2.0766 - acc: 0.0295 - auc: 0.6109 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: 2.0770 - acc: 0.0292 - auc: 0.6104 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: 2.0774 - acc: 0.0293 - auc: 0.6098 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: 2.0772 - acc: 0.0292 - auc: 0.6108 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 704/1170 [=================>............] - ETA: 1s - loss: 2.0767 - acc: 0.0293 - auc: 0.6114 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: 2.0770 - acc: 0.0291 - auc: 0.6097 - precision: 0.0291 - recall: 1.0000 - f1: 0.0550\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: 2.0768 - acc: 0.0291 - auc: 0.6105 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: 2.0774 - acc: 0.0291 - auc: 0.6075 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: 2.0775 - acc: 0.0292 - auc: 0.6081 - precision: 0.0292 - recall: 1.0000 - f1: 0.0551\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: 2.0774 - acc: 0.0294 - auc: 0.6087 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            " 809/1170 [===================>..........] - ETA: 1s - loss: 2.0780 - acc: 0.0293 - auc: 0.6074 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: 2.0777 - acc: 0.0289 - auc: 0.6090 - precision: 0.0289 - recall: 1.0000 - f1: 0.0545\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: 2.0779 - acc: 0.0290 - auc: 0.6066 - precision: 0.0290 - recall: 1.0000 - f1: 0.0546\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: 2.0785 - acc: 0.0291 - auc: 0.6070 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: 2.0786 - acc: 0.0294 - auc: 0.6055 - precision: 0.0294 - recall: 1.0000 - f1: 0.0555\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: 2.0787 - acc: 0.0292 - auc: 0.6038 - precision: 0.0292 - recall: 1.0000 - f1: 0.0550\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: 2.0788 - acc: 0.0291 - auc: 0.6035 - precision: 0.0291 - recall: 1.0000 - f1: 0.0549\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: 2.0793 - acc: 0.0290 - auc: 0.6014 - precision: 0.0290 - recall: 1.0000 - f1: 0.0548\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: 2.0790 - acc: 0.0290 - auc: 0.6026 - precision: 0.0290 - recall: 1.0000 - f1: 0.0547\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: 2.0794 - acc: 0.0293 - auc: 0.6011 - precision: 0.0293 - recall: 1.0000 - f1: 0.0552\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: 2.0792 - acc: 0.0293 - auc: 0.6040 - precision: 0.0293 - recall: 1.0000 - f1: 0.0553\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: 2.0795 - acc: 0.0294 - auc: 0.6035 - precision: 0.0294 - recall: 1.0000 - f1: 0.0554\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: 2.0795 - acc: 0.0295 - auc: 0.6051 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: 2.0801 - acc: 0.0296 - auc: 0.6037 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: 2.0803 - acc: 0.0296 - auc: 0.6016 - precision: 0.0296 - recall: 1.0000 - f1: 0.0557\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: 2.0801 - acc: 0.0295 - auc: 0.6018 - precision: 0.0295 - recall: 1.0000 - f1: 0.0556\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: 2.0802 - acc: 0.0296 - auc: 0.6021 - precision: 0.0296 - recall: 1.0000 - f1: 0.0558\n",
            "\n",
            "2023-07-31 05:09:58.535887                               \n",
            "100%|| 120/120 [03:15<00:00, 195.05s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 120 trials to 121 (+1) trials\n",
            "2023-07-31 05:09:58.627861                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5413406918394045, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_120', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_240_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_240', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_240', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_240', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_240', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_241', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_241', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_241', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_241', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_120', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_480', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_360', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_481', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_361', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_482', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_362', 'trainable': True, 'dtype': 'float32', 'rate': 0.4145184546616105, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_483', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 569/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 714/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1061/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:17:51.081370                               \n",
            "100%|| 121/121 [07:52<00:00, 472.56s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 121 trials to 122 (+1) trials\n",
            "2023-07-31 05:17:51.283140                               \n",
            "{'name': 'Adam', 'learning_rate': 0.821771187159131, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_121', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_242_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_242', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_242', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_242', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_242', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_243', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_243', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_243', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_243', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_121', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_484', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_363', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_485', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_364', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_486', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_365', 'trainable': True, 'dtype': 'float32', 'rate': 0.41692983284190765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_487', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 451/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 861/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1062/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:22:54.663254                               \n",
            "100%|| 122/122 [05:03<00:00, 303.46s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 122 trials to 123 (+1) trials\n",
            "2023-07-31 05:22:54.746975                               \n",
            "{'name': 'Adam', 'learning_rate': 0.40925238898628585, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_122', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_244_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_244', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_244', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_244', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_244', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_245', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_245', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_245', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_245', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_122', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_488', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_366', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_489', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_367', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_490', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_368', 'trainable': True, 'dtype': 'float32', 'rate': 0.4346466843001689, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_491', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 801/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 969/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:28:09.206512                               \n",
            "100%|| 123/123 [05:14<00:00, 314.52s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 123 trials to 124 (+1) trials\n",
            "2023-07-31 05:28:09.270734                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1142522359237881, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_123', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_246_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_246', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_246', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_246', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_246', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_247', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_247', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_247', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_247', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_123', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_492', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_369', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_493', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_370', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_494', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_371', 'trainable': True, 'dtype': 'float32', 'rate': 0.41707102217565156, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_495', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  99/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 550/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 688/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 730/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 902/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1119/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:33:00.649779                               \n",
            "100%|| 124/124 [04:51<00:00, 291.46s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 124 trials to 125 (+1) trials\n",
            "2023-07-31 05:33:00.832396                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4367761070540499, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_124', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_248_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_248', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_248', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_248', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_248', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_249', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_249', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_249', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_249', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_124', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_496', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_372', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_497', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_373', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_498', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_374', 'trainable': True, 'dtype': 'float32', 'rate': 0.4117261248098765, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_499', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 423/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 534/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 685/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:38:11.011235                               \n",
            "100%|| 125/125 [05:10<00:00, 310.26s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 125 trials to 126 (+1) trials\n",
            "2023-07-31 05:38:11.191002                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5824987694628025, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_125', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_250_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_250', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_250', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_250', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_250', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_251', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_251', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_251', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_251', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_125', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_500', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_375', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_501', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_376', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_502', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_377', 'trainable': True, 'dtype': 'float32', 'rate': 0.4312761914731196, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_503', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 417/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 462/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:43:17.851482                               \n",
            "100%|| 126/126 [05:06<00:00, 306.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 126 trials to 127 (+1) trials\n",
            "2023-07-31 05:43:17.931188                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7234946987313463, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_126', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_252_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_252', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_252', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_252', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_252', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_253', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_253', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_253', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_253', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_126', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_504', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_378', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_505', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_379', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_506', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_380', 'trainable': True, 'dtype': 'float32', 'rate': 0.3743774439164693, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_507', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 419/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 792/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:48:11.614391                               \n",
            "100%|| 127/127 [04:53<00:00, 293.75s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 127 trials to 128 (+1) trials\n",
            "2023-07-31 05:48:11.694065                               \n",
            "{'name': 'Adam', 'learning_rate': 0.31867133078351506, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_127', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_254_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_254', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_254', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_254', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_254', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_255', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_255', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_255', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_255', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_127', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_508', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_381', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_509', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_382', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_510', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_383', 'trainable': True, 'dtype': 'float32', 'rate': 0.4306584830654796, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_511', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  47/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9634 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 337/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 490/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 898/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 05:53:16.697999                               \n",
            "100%|| 128/128 [05:05<00:00, 305.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 128 trials to 129 (+1) trials\n",
            "2023-07-31 05:53:16.869209                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3317816372728631, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_128', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_256_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_256', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_256', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_256', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_256', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_257', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_257', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_257', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_257', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_128', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_512', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_384', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_513', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_385', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_514', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_386', 'trainable': True, 'dtype': 'float32', 'rate': 0.41774020578566706, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_515', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 171/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 532/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:00:50.822551                               \n",
            "100%|| 129/129 [07:34<00:00, 454.02s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 129 trials to 130 (+1) trials\n",
            "2023-07-31 06:00:50.981265                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5168269938007773, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_129', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_258_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_258', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_258', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_258', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_258', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_259', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_259', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_259', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_259', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_129', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_516', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_387', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_517', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_388', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_518', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_389', 'trainable': True, 'dtype': 'float32', 'rate': 0.427423429842709, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_519', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 379/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 489/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 675/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 760/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 979/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:08:33.655715                               \n",
            "100%|| 130/130 [07:42<00:00, 462.74s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 130 trials to 131 (+1) trials\n",
            "2023-07-31 06:08:33.831874                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5225799648099021, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_130', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_260_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_260', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_260', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_260', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_260', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_261', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_261', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_261', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_261', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_130', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_520', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_390', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_521', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_391', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_522', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_392', 'trainable': True, 'dtype': 'float32', 'rate': 0.41812101926914746, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_523', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  89/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 491/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 553/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 600/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 821/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 908/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1001/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:13:54.928375                               \n",
            "100%|| 131/131 [05:21<00:00, 321.16s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 131 trials to 132 (+1) trials\n",
            "2023-07-31 06:13:54.998657                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8986164409876551, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_131', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_262_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_262', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_262', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_262', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_262', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_263', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_263', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_263', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_263', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_131', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_524', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_393', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_525', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_394', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_526', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_395', 'trainable': True, 'dtype': 'float32', 'rate': 0.4148546538756369, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_527', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 100/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 197/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 450/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 598/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 708/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1018/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1110/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:18:55.361645                               \n",
            "100%|| 132/132 [05:00<00:00, 300.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 132 trials to 133 (+1) trials\n",
            "2023-07-31 06:18:55.533716                               \n",
            "{'name': 'Adam', 'learning_rate': 0.625991535565348, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_132', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_264_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_264', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_264', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_264', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_264', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_265', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_265', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_265', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_265', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_132', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_528', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_396', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_529', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_397', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_530', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_398', 'trainable': True, 'dtype': 'float32', 'rate': 0.40456725289958, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_531', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  74/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 235/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 446/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 753/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 943/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:24:09.526675                               \n",
            "100%|| 133/133 [05:14<00:00, 314.06s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 133 trials to 134 (+1) trials\n",
            "2023-07-31 06:24:09.599461                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4920692726310007, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_133', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_266_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_266', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_266', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_266', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_266', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_267', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_267', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_267', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_267', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_133', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_532', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_399', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_533', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_400', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_534', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_401', 'trainable': True, 'dtype': 'float32', 'rate': 0.41677565056433963, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_535', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 343/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 568/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 720/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 737/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 874/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1036/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1089/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:31:31.088799                               \n",
            "100%|| 134/134 [07:21<00:00, 441.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 134 trials to 135 (+1) trials\n",
            "2023-07-31 06:31:31.157188                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1686555010616247, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_134', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_268_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_268', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_268', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_268', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_268', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_269', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_269', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_269', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_269', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_134', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_536', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_402', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_537', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_403', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_538', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_404', 'trainable': True, 'dtype': 'float32', 'rate': 0.4168899372646663, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_539', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  50/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9631 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 140/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 172/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 383/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 512/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 721/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 830/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 877/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 936/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1133/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:36:40.571238                               \n",
            "100%|| 135/135 [05:09<00:00, 309.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 135 trials to 136 (+1) trials\n",
            "2023-07-31 06:36:40.650583                               \n",
            "{'name': 'Adam', 'learning_rate': 0.22095108060854524, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_135', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_270_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_270', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_270', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_270', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_270', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_271', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_271', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_271', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_271', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_135', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_540', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_405', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_541', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_406', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_542', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_407', 'trainable': True, 'dtype': 'float32', 'rate': 0.44017154565858146, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_543', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 215/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 276/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 483/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 535/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 667/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 741/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 928/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 964/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1088/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:41:44.289351                               \n",
            "100%|| 136/136 [05:03<00:00, 303.72s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 136 trials to 137 (+1) trials\n",
            "2023-07-31 06:41:44.461515                               \n",
            "{'name': 'Adam', 'learning_rate': 0.06625963243435189, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_136', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_272_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_272', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_272', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_272', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_272', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_273', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_273', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_273', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_273', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_136', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_544', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_408', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_545', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_409', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_546', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_410', 'trainable': True, 'dtype': 'float32', 'rate': 0.33995453124872677, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_547', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  86/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 475/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 779/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 832/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 844/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:49:11.103781                               \n",
            "100%|| 137/137 [07:26<00:00, 446.71s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 137 trials to 138 (+1) trials\n",
            "2023-07-31 06:49:11.173650                               \n",
            "{'name': 'Adam', 'learning_rate': 0.08376672400009921, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_137', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_274_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_274', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_274', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_274', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_274', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_275', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_275', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_275', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_275', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_137', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_548', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_411', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_549', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_412', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_550', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_413', 'trainable': True, 'dtype': 'float32', 'rate': 0.37388049120885, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_551', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 188/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 217/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 266/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 387/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 420/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 622/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 640/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 687/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 895/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1026/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1108/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 06:57:03.318452                               \n",
            "100%|| 138/138 [07:52<00:00, 472.21s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 138 trials to 139 (+1) trials\n",
            "2023-07-31 06:57:03.391437                               \n",
            "{'name': 'Adam', 'learning_rate': 0.24863451029421096, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_138', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_276_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_276', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_276', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_276', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_276', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_277', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_277', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_277', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_277', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_138', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_552', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_414', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_553', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_415', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_554', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_416', 'trainable': True, 'dtype': 'float32', 'rate': 0.3441890181072995, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_555', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 211/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 301/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 354/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 606/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 650/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 780/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 856/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1097/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:00:54.012672                               \n",
            "100%|| 139/139 [03:50<00:00, 230.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 139 trials to 140 (+1) trials\n",
            "2023-07-31 07:00:54.077371                               \n",
            "{'name': 'Adam', 'learning_rate': 0.306481092344722, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_139', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_278_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_278', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_278', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_278', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_278', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_279', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_279', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_279', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_279', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_139', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_556', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_417', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_557', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_418', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_558', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_419', 'trainable': True, 'dtype': 'float32', 'rate': 0.44693813104594854, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_559', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 208/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 232/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 313/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 360/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 517/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 582/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 994/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:08:53.063261                               \n",
            "100%|| 140/140 [07:59<00:00, 479.06s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 140 trials to 141 (+1) trials\n",
            "2023-07-31 07:08:53.232145                               \n",
            "{'name': 'Adam', 'learning_rate': 0.27011196068049326, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_140', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_280_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_280', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_280', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_280', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_280', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_281', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_281', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_281', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_281', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_140', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_560', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_420', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_561', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_421', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_562', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_422', 'trainable': True, 'dtype': 'float32', 'rate': 0.41000386643086834, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_563', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 105/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 230/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 287/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 300/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 355/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 556/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 575/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 592/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 697/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 719/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 793/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 876/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1014/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1048/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:13:57.539405                               \n",
            "100%|| 141/141 [05:04<00:00, 304.37s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 141 trials to 142 (+1) trials\n",
            "2023-07-31 07:13:57.610385                               \n",
            "{'name': 'Adam', 'learning_rate': 0.40135687185279567, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_141', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_282_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_282', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_282', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_282', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_282', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_283', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_283', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_283', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_283', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_141', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_564', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_423', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_565', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_424', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_566', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_425', 'trainable': True, 'dtype': 'float32', 'rate': 0.4255659991551849, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_567', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 99%|| 141/142 [07:42<?, ?trial/s, best loss=?]WARNING:tensorflow:Callback method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0015s). Check your callbacks.\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 283/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 444/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 510/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 533/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 566/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 806/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 826/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 907/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 975/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:21:43.708694                               \n",
            "100%|| 142/142 [07:46<00:00, 466.17s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 142 trials to 143 (+1) trials\n",
            "2023-07-31 07:21:43.868857                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5801427165648934, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_142', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_284_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_284', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_284', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_284', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_284', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_285', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_285', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_285', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_285', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_142', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_568', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_426', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_569', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_427', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_570', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_428', 'trainable': True, 'dtype': 'float32', 'rate': 0.42532211910706547, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_571', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 21s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 245/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 288/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 579/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 763/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 822/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 901/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1046/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1130/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1169/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:29:09.293730                               \n",
            "100%|| 143/143 [07:25<00:00, 445.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 143 trials to 144 (+1) trials\n",
            "2023-07-31 07:29:09.618390                               \n",
            "{'name': 'Adam', 'learning_rate': 0.17732123564908345, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_143', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_286_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_286', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_286', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_286', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_286', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_287', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_287', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_287', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_287', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_143', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_572', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_429', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_573', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_430', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_574', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_431', 'trainable': True, 'dtype': 'float32', 'rate': 0.4154977177249626, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_575', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 207/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 325/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 356/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 521/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 544/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 584/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 624/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 782/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 814/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 838/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 855/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 940/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 990/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1007/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1039/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1163/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:37:02.040191                               \n",
            "100%|| 144/144 [07:52<00:00, 472.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 144 trials to 145 (+1) trials\n",
            "2023-07-31 07:37:02.193546                               \n",
            "{'name': 'Adam', 'learning_rate': 0.29239509830475546, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_144', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_288_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_288', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_288', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_288', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_288', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_289', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_289', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_289', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_289', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_144', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_576', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_432', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_577', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_433', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_578', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_434', 'trainable': True, 'dtype': 'float32', 'rate': 0.4149713398601007, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_579', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 254/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 341/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 400/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 457/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 548/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 563/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 581/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 700/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 757/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 796/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 897/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 967/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1010/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1067/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:45:01.392554                               \n",
            "100%|| 145/145 [07:59<00:00, 479.29s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 145 trials to 146 (+1) trials\n",
            "2023-07-31 07:45:01.571003                               \n",
            "{'name': 'Adam', 'learning_rate': 0.09948120220181846, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_145', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_290_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_290', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_290', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_290', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_290', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_291', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_291', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_291', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_291', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_145', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_580', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_435', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_581', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_436', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_582', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_437', 'trainable': True, 'dtype': 'float32', 'rate': 0.4174053014554533, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_583', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 123/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 153/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 202/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 314/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 393/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 441/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 484/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 499/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 583/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 639/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 671/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 691/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 715/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 836/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1023/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1045/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1139/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:50:09.169793                               \n",
            "100%|| 146/146 [05:07<00:00, 307.67s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 146 trials to 147 (+1) trials\n",
            "2023-07-31 07:50:09.343621                               \n",
            "{'name': 'Adam', 'learning_rate': 0.846820160647481, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_146', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_292_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_292', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_292', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_292', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_292', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_293', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_293', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_293', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_293', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_146', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_584', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_438', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_585', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_439', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_586', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_440', 'trainable': True, 'dtype': 'float32', 'rate': 0.3975600241724424, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_587', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 101/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 237/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 440/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 503/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 631/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 869/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1020/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1112/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 07:55:18.100978                               \n",
            "100%|| 147/147 [05:08<00:00, 308.85s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 147 trials to 148 (+1) trials\n",
            "2023-07-31 07:55:18.287294                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3745277053816565, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_147', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_294_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_294', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_294', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_294', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_294', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_295', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_295', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_295', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_295', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_147', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_588', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_441', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_589', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_442', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_590', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_443', 'trainable': True, 'dtype': 'float32', 'rate': 0.4158519416753821, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_591', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 324/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 365/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 396/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 601/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 637/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 677/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 764/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 805/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 971/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1079/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:00:48.883026                               \n",
            "100%|| 148/148 [05:30<00:00, 330.67s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 148 trials to 149 (+1) trials\n",
            "2023-07-31 08:00:48.965826                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3006440316370092, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_148', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_296_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_296', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_296', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_296', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_296', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_297', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_297', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_297', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_297', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_148', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_592', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_444', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_593', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_445', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_594', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_446', 'trainable': True, 'dtype': 'float32', 'rate': 0.42616727942541455, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_595', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 131/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 170/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 306/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 349/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 807/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 910/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1058/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:08:24.652775                               \n",
            "100%|| 149/149 [07:35<00:00, 455.78s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 149 trials to 150 (+1) trials\n",
            "2023-07-31 08:08:24.841206                               \n",
            "{'name': 'Adam', 'learning_rate': 0.23348164532472845, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_149', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_298_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_298', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_298', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_298', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_298', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_299', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_299', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_299', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_299', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_149', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_596', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_447', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_597', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_448', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_598', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_449', 'trainable': True, 'dtype': 'float32', 'rate': 0.40673613943191245, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_599', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  37/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9645 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 146/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 252/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 351/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 389/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 422/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 541/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 565/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 594/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 632/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 767/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 911/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 947/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 963/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1017/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:13:29.259201                               \n",
            "100%|| 150/150 [05:04<00:00, 304.49s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 150 trials to 151 (+1) trials\n",
            "2023-07-31 08:13:29.337324                               \n",
            "{'name': 'Adam', 'learning_rate': 0.31352032970254423, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_150', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_300_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_300', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_300', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_300', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_300', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_301', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_301', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_301', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_301', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_150', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_600', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_450', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_601', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_451', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_602', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_452', 'trainable': True, 'dtype': 'float32', 'rate': 0.4136176245183774, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_603', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 103/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 159/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9676 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 176/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 205/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 241/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 256/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 305/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 409/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 433/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 463/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 518/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 546/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 570/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 621/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 696/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 893/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 996/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1040/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1076/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1116/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1149/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:18:32.305290                               \n",
            "100%|| 151/151 [05:03<00:00, 303.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 151 trials to 152 (+1) trials\n",
            "2023-07-31 08:18:32.452505                               \n",
            "{'name': 'Adam', 'learning_rate': 0.07687536399162423, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_151', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_302_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_302', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_302', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_302', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_302', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_303', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_303', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_303', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_303', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_151', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_604', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_453', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_605', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_454', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_606', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_455', 'trainable': True, 'dtype': 'float32', 'rate': 0.4233658174135788, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_607', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  85/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 179/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 260/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 274/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 472/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 501/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 524/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 679/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 786/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 924/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 960/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1060/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1083/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:25:56.654437                               \n",
            "100%|| 152/152 [07:24<00:00, 444.28s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 152 trials to 153 (+1) trials\n",
            "2023-07-31 08:25:56.737914                               \n",
            "{'name': 'Adam', 'learning_rate': 0.2847777760406176, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_152', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_304_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_304', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_304', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_304', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_304', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_305', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_305', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_305', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_305', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_152', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_608', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_456', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_609', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_457', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_610', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_458', 'trainable': True, 'dtype': 'float32', 'rate': 0.4072799292766861, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_611', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  49/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9630 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 108/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 151/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 182/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 233/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 344/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 405/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 432/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 464/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 557/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 587/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 668/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 756/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 829/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 850/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 878/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 896/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 929/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 977/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1016/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1070/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1152/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:31:03.982243                               \n",
            "100%|| 153/153 [05:07<00:00, 307.33s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 153 trials to 154 (+1) trials\n",
            "2023-07-31 08:31:04.170813                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8036399433912818, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_153', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_306_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_306', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_306', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_306', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_306', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_307', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_307', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_307', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_307', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_153', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_612', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_459', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_613', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_460', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_614', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_461', 'trainable': True, 'dtype': 'float32', 'rate': 0.3982732489122757, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_615', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  88/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 122/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 1s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 191/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 226/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 270/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 310/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 410/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 429/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 479/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 509/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 537/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 572/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 629/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 775/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 921/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1121/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:36:15.657217                               \n",
            "100%|| 154/154 [05:11<00:00, 311.57s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 154 trials to 155 (+1) trials\n",
            "2023-07-31 08:36:15.762106                               \n",
            "{'name': 'Adam', 'learning_rate': 0.19581596078991026, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_154', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_308_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_308', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_308', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_308', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_308', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_309', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_309', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_309', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_309', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_154', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_616', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_462', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_617', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_463', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_618', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_464', 'trainable': True, 'dtype': 'float32', 'rate': 0.4153823136438406, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_619', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 25s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 114/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 148/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9666 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 167/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 196/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 298/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 342/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 374/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 392/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 439/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 461/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 480/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 531/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 549/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 564/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 595/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 644/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 665/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 689/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 727/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 748/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 811/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 833/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 868/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 885/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 904/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 923/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 959/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 983/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1034/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1164/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:44:00.048654                               \n",
            "100%|| 155/155 [07:44<00:00, 464.41s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 155 trials to 156 (+1) trials\n",
            "2023-07-31 08:44:00.181317                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1935959205551956, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_155', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_310_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_310', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_310', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_310', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_310', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_311', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_311', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_311', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_311', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_155', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_620', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_465', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_621', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_466', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_622', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_467', 'trainable': True, 'dtype': 'float32', 'rate': 0.41166366132048304, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_623', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 19s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  87/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9662 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 137/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 175/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 198/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 220/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 247/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 293/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 308/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 321/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 404/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 473/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 502/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 706/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 742/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 813/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 846/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 853/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 860/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 872/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 986/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1025/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1052/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1114/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1129/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1142/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:51:55.188215                               \n",
            "100%|| 156/156 [07:55<00:00, 475.08s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 156 trials to 157 (+1) trials\n",
            "2023-07-31 08:51:55.358587                               \n",
            "{'name': 'Adam', 'learning_rate': 0.6443787098651779, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_156', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_312_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_312', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_312', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_312', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_312', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_313', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_313', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_313', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_313', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_156', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_624', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_468', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_625', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_469', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_626', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_470', 'trainable': True, 'dtype': 'float32', 'rate': 0.41943894161913414, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_627', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  51/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 138/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 184/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 194/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 203/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 265/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 372/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 386/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 468/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 508/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 525/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 540/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 578/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 616/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 712/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 735/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 745/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 778/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 879/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 903/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 952/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1056/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1072/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1087/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 08:57:09.702067                               \n",
            "100%|| 157/157 [05:14<00:00, 314.43s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 157 trials to 158 (+1) trials\n",
            "2023-07-31 08:57:09.879277                               \n",
            "{'name': 'Adam', 'learning_rate': 0.24697331057966157, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_157', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_314_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_314', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_314', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_314', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_314', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_315', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_315', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_315', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_315', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_157', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_628', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_471', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_629', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_472', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_630', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_473', 'trainable': True, 'dtype': 'float32', 'rate': 0.42984465049946713, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_631', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  54/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  80/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9660 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 124/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 178/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 200/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 213/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 253/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 286/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 350/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 412/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 438/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 458/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 511/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 530/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 558/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 611/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 642/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 658/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 713/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 736/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 772/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 785/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 800/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 823/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 880/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 917/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 933/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1012/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1127/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1148/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:01:13.625989                               \n",
            "100%|| 158/158 [04:03<00:00, 243.82s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 158 trials to 159 (+1) trials\n",
            "2023-07-31 09:01:13.727729                               \n",
            "{'name': 'Adam', 'learning_rate': 0.20520517075346406, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_158', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_316_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_316', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_316', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_316', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_316', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_317', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_317', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_317', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_317', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_158', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_632', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_474', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_633', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_475', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_634', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_476', 'trainable': True, 'dtype': 'float32', 'rate': 0.4350393263327249, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_635', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 110/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 125/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 150/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 204/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 234/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 320/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 377/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 406/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 497/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 523/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 590/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 618/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 651/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 699/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 717/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 754/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 808/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 828/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 842/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 890/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 953/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1022/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1084/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1106/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:06:18.209861                               \n",
            "100%|| 159/159 [05:04<00:00, 304.55s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 159 trials to 160 (+1) trials\n",
            "2023-07-31 09:06:18.377850                               \n",
            "{'name': 'Adam', 'learning_rate': 0.4584487512512802, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_159', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_318_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_318', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_318', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_318', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_318', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_319', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_319', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_319', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_319', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_159', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_636', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_477', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_637', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_478', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_638', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_479', 'trainable': True, 'dtype': 'float32', 'rate': 0.3768900029041891, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_639', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  38/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  84/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 132/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 183/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 240/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 303/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 388/1170 [========>.....................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 459/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 555/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 589/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 623/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 653/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 693/1170 [================>.............] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 723/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 789/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 843/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 918/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 931/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1015/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1031/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1059/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1137/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1167/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:11:33.118188                               \n",
            "100%|| 160/160 [05:14<00:00, 314.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 160 trials to 161 (+1) trials\n",
            "2023-07-31 09:11:33.278567                               \n",
            "{'name': 'Adam', 'learning_rate': 0.8084590558529352, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_160', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_320_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_320', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_320', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_320', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_320', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_321', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_321', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_321', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_321', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_160', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_640', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_480', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_641', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_481', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_642', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_482', 'trainable': True, 'dtype': 'float32', 'rate': 0.40462802218893257, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_643', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  55/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9636 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 134/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 161/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 173/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 192/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 214/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 228/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 268/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 307/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 331/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 403/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 471/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 492/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 551/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 656/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 751/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 770/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 831/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 954/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 982/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 998/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1141/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1155/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:19:01.444823                               \n",
            "100%|| 161/161 [07:28<00:00, 448.23s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 161 trials to 162 (+1) trials\n",
            "2023-07-31 09:19:01.609806                               \n",
            "{'name': 'Adam', 'learning_rate': 0.25693698799879666, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_161', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_322_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_322', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_322', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_322', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_322', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_323', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_323', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_323', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_323', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_161', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_644', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_483', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_645', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_484', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_646', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_485', 'trainable': True, 'dtype': 'float32', 'rate': 0.4123940855247277, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_647', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  33/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  53/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  79/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9656 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 113/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 129/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 144/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 231/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 259/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 289/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 366/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 414/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 427/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 449/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 466/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 527/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 625/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 643/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 666/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 683/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 747/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 765/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 797/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 815/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 837/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 851/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 886/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 922/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 932/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 976/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 993/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1050/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1066/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1081/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1102/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1125/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1146/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:24:11.572070                               \n",
            "100%|| 162/162 [05:10<00:00, 310.03s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 162 trials to 163 (+1) trials\n",
            "2023-07-31 09:24:11.644223                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3817600873667501, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_162', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_324_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_324', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_324', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_324', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_324', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_325', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_325', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_325', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_325', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_162', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_648', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_486', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_649', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_487', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_650', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_488', 'trainable': True, 'dtype': 'float32', 'rate': 0.4173220989002612, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_651', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  82/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 111/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 187/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 209/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 221/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 261/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 281/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 302/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 330/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 357/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 391/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 416/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 445/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 504/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 617/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 645/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 662/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 734/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 848/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 912/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 927/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 950/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 970/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1027/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1065/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1086/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1128/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1140/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1158/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:29:34.385753                               \n",
            "100%|| 163/163 [05:22<00:00, 322.81s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 163 trials to 164 (+1) trials\n",
            "2023-07-31 09:29:34.547427                               \n",
            "{'name': 'Adam', 'learning_rate': 0.12284198520300191, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_163', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_326_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_326', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_326', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_326', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_326', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_327', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_327', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_327', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_327', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_163', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_652', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_489', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_653', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_490', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_654', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_491', 'trainable': True, 'dtype': 'float32', 'rate': 0.4263679673655504, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_655', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  32/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9668 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  67/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9641 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 102/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 118/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 130/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 193/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 248/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 282/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 296/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 319/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 340/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 375/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 407/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 431/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 454/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 481/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 520/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 597/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 674/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 729/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 744/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 795/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 809/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 817/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 891/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 915/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 965/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 980/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 997/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1033/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1080/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1098/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1118/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1154/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:37:32.288263                               \n",
            "100%|| 164/164 [07:57<00:00, 477.83s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 164 trials to 165 (+1) trials\n",
            "2023-07-31 09:37:32.381538                               \n",
            "{'name': 'Adam', 'learning_rate': 0.5778208462619084, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_164', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_328_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_328', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_328', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_328', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_328', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_329', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_329', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_329', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_329', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_164', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_656', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_492', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_657', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_493', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_658', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_494', 'trainable': True, 'dtype': 'float32', 'rate': 0.40887028760135585, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_659', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  48/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9622 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  70/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9629 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  94/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 107/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 120/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 136/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 145/1170 [==>...........................] - ETA: 3s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 3s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 3s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 269/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 292/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 348/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 384/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 435/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 476/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 519/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 538/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 562/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 614/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 660/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 681/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 695/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 725/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 858/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 882/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 930/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 946/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1069/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1091/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1109/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1134/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1151/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:42:41.956966                               \n",
            "100%|| 165/165 [05:09<00:00, 309.64s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 165 trials to 166 (+1) trials\n",
            "2023-07-31 09:42:42.041419                               \n",
            "{'name': 'Adam', 'learning_rate': 0.0004972729685774701, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_165', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_330_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_330', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_330', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_330', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_330', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_331', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_331', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_331', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_331', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_165', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_660', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_495', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_661', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_496', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_662', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_497', 'trainable': True, 'dtype': 'float32', 'rate': 0.3987663040436542, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_663', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 20s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  76/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9646 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 106/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 163/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 185/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 219/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 250/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 278/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 291/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 309/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 335/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 399/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 436/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 469/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 488/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 513/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 543/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 630/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 646/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 710/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 762/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 777/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 812/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 849/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 862/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 883/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 905/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 926/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 942/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 992/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1073/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1095/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1111/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1145/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1162/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:47:48.713186                               \n",
            "100%|| 166/166 [05:06<00:00, 306.77s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 166 trials to 167 (+1) trials\n",
            "2023-07-31 09:47:48.812408                               \n",
            "{'name': 'Adam', 'learning_rate': 0.28821341475605244, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_166', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_332_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_332', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_332', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_332', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_332', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_333', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_333', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_333', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_333', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_166', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_664', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_498', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_665', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_499', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_666', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_500', 'trainable': True, 'dtype': 'float32', 'rate': 0.39169888167272804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_667', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  83/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9665 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 121/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 139/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9683 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 154/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 165/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 218/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 273/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 299/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 318/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 358/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 373/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 394/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 418/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 443/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 467/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 487/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 514/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 580/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 664/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 678/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 692/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 752/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 769/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 791/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 816/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 847/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 865/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 958/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 981/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 999/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1029/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1042/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1055/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1077/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1101/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1124/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1156/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:52:54.329677                               \n",
            "100%|| 167/167 [05:05<00:00, 305.59s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 167 trials to 168 (+1) trials\n",
            "2023-07-31 09:52:54.515984                               \n",
            "{'name': 'Adam', 'learning_rate': 0.25296596518615905, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_167', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_334_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_334', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_334', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_334', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_334', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_335', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_335', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_335', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_335', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_167', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_668', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_501', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_669', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_502', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_670', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_503', 'trainable': True, 'dtype': 'float32', 'rate': 0.40110707271163654, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_671', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 18s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  35/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  77/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9647 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 112/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 127/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 157/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 169/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 186/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 206/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 227/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 243/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 267/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 285/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 315/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 327/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 380/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 411/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 470/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 494/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 554/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 638/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 702/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 722/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 738/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 766/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 790/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 839/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 870/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 887/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 899/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 939/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 956/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 984/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1008/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1030/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1044/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1078/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1093/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1115/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1165/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 2ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 09:57:59.376075                               \n",
            "100%|| 168/168 [05:04<00:00, 304.94s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 168 trials to 169 (+1) trials\n",
            "2023-07-31 09:57:59.457604                               \n",
            "{'name': 'Adam', 'learning_rate': 0.29718313897509513, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_168', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_336_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_336', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_336', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_336', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_336', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_337', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_337', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_337', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_337', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_168', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_672', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_504', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_673', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_505', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_674', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_506', 'trainable': True, 'dtype': 'float32', 'rate': 0.42818740809945915, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_675', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  56/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9637 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 116/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 147/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 160/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 177/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 199/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 224/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 242/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 294/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 317/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 333/1170 [=======>......................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 338/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 352/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 371/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 398/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 442/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 477/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 498/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 522/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 612/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 684/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 709/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 726/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 746/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 761/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 784/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 818/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 841/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 857/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 892/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 913/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 935/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 951/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 972/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 989/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1009/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1028/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1064/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1107/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1143/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1153/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:03:12.904703                               \n",
            "100%|| 169/169 [05:13<00:00, 313.52s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 169 trials to 170 (+1) trials\n",
            "2023-07-31 10:03:12.986742                               \n",
            "{'name': 'Adam', 'learning_rate': 0.22785515075978435, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_169', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_338_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_338', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_338', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_338', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_338', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_339', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_339', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_339', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_339', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_169', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_676', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_507', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_677', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_508', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_678', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_509', 'trainable': True, 'dtype': 'float32', 'rate': 0.39514091066912804, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_679', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 14s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  36/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9670 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  57/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  78/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 115/1170 [=>............................] - ETA: 1s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 141/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 164/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 181/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 212/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 236/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 251/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 264/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 284/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 304/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 322/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 328/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 345/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 367/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 385/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 397/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 415/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 430/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 455/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 515/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 547/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 607/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 636/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 659/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 701/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 724/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 749/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 783/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 802/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 820/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 840/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 866/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 948/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 966/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 985/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1005/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1041/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1105/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1120/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1157/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:10:37.799967                               \n",
            "100%|| 170/170 [07:24<00:00, 444.91s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 170 trials to 171 (+1) trials\n",
            "2023-07-31 10:10:37.989923                               \n",
            "{'name': 'Adam', 'learning_rate': 0.7875690868006465, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_170', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_340_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_340', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_340', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_340', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_340', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_341', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_341', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_341', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_341', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_170', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_680', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_510', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_681', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_511', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_682', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_512', 'trainable': True, 'dtype': 'float32', 'rate': 0.4139988875447067, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_683', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  24/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9661 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  42/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9658 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  63/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  98/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 119/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 135/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9681 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 155/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9673 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 168/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9684 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 189/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 223/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 244/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 257/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 271/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 316/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 334/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 362/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 378/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 402/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 424/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9690 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 434/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 460/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 485/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 506/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 528/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 559/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 586/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 633/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 648/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 663/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 680/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 703/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 718/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 740/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 759/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 776/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 794/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 804/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 827/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 864/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 884/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 900/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 919/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 934/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 949/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 957/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 978/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1002/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1021/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1037/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1054/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1075/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1100/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1122/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1135/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1161/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:15:52.024902                               \n",
            "100%|| 171/171 [05:14<00:00, 314.11s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 171 trials to 172 (+1) trials\n",
            "2023-07-31 10:15:52.103282                               \n",
            "{'name': 'Adam', 'learning_rate': 0.21092334483875513, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_171', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_342_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_342', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_342', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_342', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_342', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_343', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_343', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_343', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_343', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_171', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_684', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_513', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_685', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_514', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_686', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_515', 'trainable': True, 'dtype': 'float32', 'rate': 0.3984865924943769, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_687', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 15s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  31/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  52/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  72/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9640 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 109/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9685 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 126/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 143/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 166/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 180/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9686 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 222/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 239/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 258/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 279/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 323/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 347/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 370/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 390/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 408/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 428/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 456/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 493/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 529/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 560/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 620/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 654/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 686/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 711/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 733/1170 [=================>............] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 755/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 768/1170 [==================>...........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 781/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 803/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 824/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 835/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9712 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 854/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 873/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 889/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 909/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 938/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 962/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 973/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 987/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1003/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1024/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1049/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1071/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1104/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1126/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1144/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1159/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:23:33.695957                               \n",
            "100%|| 172/172 [07:41<00:00, 461.68s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 172 trials to 173 (+1) trials\n",
            "2023-07-31 10:23:33.793515                               \n",
            "{'name': 'Adam', 'learning_rate': 0.3662888333345001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_172', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_344_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_344', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_344', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_344', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_344', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_345', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_345', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_345', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_345', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_172', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_688', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_516', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_689', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_517', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_690', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_518', 'trainable': True, 'dtype': 'float32', 'rate': 0.41705051565229484, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_691', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 16s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  34/1170 [..............................] - ETA: 1s - loss: nan - acc: 0.9651 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  58/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9639 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  81/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9657 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 117/1170 [==>...........................] - ETA: 1s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 133/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9678 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 149/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9669 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 162/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9682 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 195/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9689 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 216/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 229/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 246/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 263/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 277/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 290/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 311/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 329/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 346/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 395/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 421/1170 [=========>....................] - ETA: 1s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 453/1170 [==========>...................] - ETA: 1s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 474/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 482/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 496/1170 [===========>..................] - ETA: 1s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 507/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 539/1170 [============>.................] - ETA: 1s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 571/1170 [=============>................] - ETA: 1s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 603/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 619/1170 [==============>...............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 635/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 649/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 652/1170 [===============>..............] - ETA: 1s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 673/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 694/1170 [================>.............] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 707/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 728/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 739/1170 [=================>............] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 743/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 773/1170 [==================>...........] - ETA: 1s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 787/1170 [===================>..........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 834/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 852/1170 [====================>.........] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 871/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 894/1170 [=====================>........] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 920/1170 [======================>.......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 937/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 945/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 968/1170 [=======================>......] - ETA: 0s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 988/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1006/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1035/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1068/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1085/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1096/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1113/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1136/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1160/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 3s 3ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:31:01.691173                               \n",
            "100%|| 173/173 [07:27<00:00, 447.98s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 173 trials to 174 (+1) trials\n",
            "2023-07-31 10:31:01.781280                               \n",
            "{'name': 'Adam', 'learning_rate': 0.1352301354989742, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_173', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_346_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_346', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_346', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_346', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_346', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_347', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_347', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_347', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_347', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_173', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_692', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_519', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_693', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_520', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_694', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_521', 'trainable': True, 'dtype': 'float32', 'rate': 0.3922664766808983, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_695', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "   1/1170 [..............................] - ETA: 22s - loss: nan - acc: 0.9375 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  30/1170 [..............................] - ETA: 2s - loss: nan - acc: 0.9677 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan \n",
            "  63/1170 [>.............................] - ETA: 1s - loss: nan - acc: 0.9638 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "  71/1170 [>.............................] - ETA: 2s - loss: nan - acc: 0.9635 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 104/1170 [=>............................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 128/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 142/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9679 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 152/1170 [==>...........................] - ETA: 2s - loss: nan - acc: 0.9671 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 158/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9674 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 174/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9680 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 190/1170 [===>..........................] - ETA: 2s - loss: nan - acc: 0.9692 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 201/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 210/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9688 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 225/1170 [====>.........................] - ETA: 2s - loss: nan - acc: 0.9694 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 238/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 249/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 262/1170 [=====>........................] - ETA: 2s - loss: nan - acc: 0.9695 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 272/1170 [=====>........................] - ETA: 3s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 280/1170 [======>.......................] - ETA: 3s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 297/1170 [======>.......................] - ETA: 2s - loss: nan - acc: 0.9700 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 312/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 326/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 339/1170 [=======>......................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 353/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 364/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 376/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 381/1170 [========>.....................] - ETA: 2s - loss: nan - acc: 0.9697 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 401/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 413/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 426/1170 [=========>....................] - ETA: 2s - loss: nan - acc: 0.9691 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 437/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9693 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 452/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9696 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 465/1170 [==========>...................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 486/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9698 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 500/1170 [===========>..................] - ETA: 2s - loss: nan - acc: 0.9699 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 516/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9701 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 526/1170 [============>.................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 552/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 561/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 567/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 573/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 574/1170 [=============>................] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 585/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9702 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 588/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9703 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 593/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 599/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 602/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 605/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 608/1170 [==============>...............] - ETA: 2s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 609/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 610/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 615/1170 [==============>...............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 628/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 634/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 641/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 647/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 657/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 661/1170 [===============>..............] - ETA: 3s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 670/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 672/1170 [================>.............] - ETA: 3s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 682/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 690/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 698/1170 [================>.............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 716/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 732/1170 [=================>............] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 750/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 758/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 771/1170 [==================>...........] - ETA: 2s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 788/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 810/1170 [===================>..........] - ETA: 2s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 825/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9711 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 845/1170 [====================>.........] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 859/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 863/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 867/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 875/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9709 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 888/1170 [=====================>........] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 906/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 925/1170 [======================>.......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 944/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 961/1170 [=======================>......] - ETA: 1s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            " 991/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1004/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1011/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1013/1170 [========================>.....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1019/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9710 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1032/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1043/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1051/1170 [=========================>....] - ETA: 0s - loss: nan - acc: 0.9708 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1053/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1057/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1063/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1074/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9707 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1082/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1090/1170 [==========================>...] - ETA: 0s - loss: nan - acc: 0.9706 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1103/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1123/1170 [===========================>..] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1138/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1147/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9705 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1166/1170 [============================>.] - ETA: 0s - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "1170/1170 [==============================] - 6s 5ms/step - loss: nan - acc: 0.9704 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "\n",
            "2023-07-31 10:38:59.586526                               \n",
            "100%|| 174/174 [07:58<00:00, 478.18s/trial, best loss: -0.07277102768421173]\n",
            "Best: {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
            "Found saved Trials! Loading...\n",
            "Rerunning from 174 trials to 175 (+1) trials\n",
            "2023-07-31 10:38:59.997254                               \n",
            "{'name': 'Adam', 'learning_rate': 0.18790750631772393, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_174', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_348_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_348', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_348', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_348', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_348', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_349', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_349', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_349', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_349', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_174', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_696', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_522', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_697', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_523', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_698', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_524', 'trainable': True, 'dtype': 'float32', 'rate': 0.4212286872225443, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_699', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            " 99%|| 174/175 [01:30<?, ?trial/s, best loss=?]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# loop indefinitely and stop whenever you like\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     run_trials()\n\u001b[1;32m     16\u001b[0m \u001b[39m# best_param = fmin(objective_function,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#                   param_hyperopt,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#                   algo=tpe.suggest,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# loss = [x['result']['loss'] for x in trials.trials]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# best_param_values = [x for x in best_param.values()]\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[20], line 52\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# create a new trials object and start searching\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> 52\u001b[0m best_param \u001b[39m=\u001b[39m fmin(objective_function,\n\u001b[1;32m     53\u001b[0m               param_hyperopt,\n\u001b[1;32m     54\u001b[0m               algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[1;32m     55\u001b[0m               max_evals\u001b[39m=\u001b[39;49mmax_trials,\n\u001b[1;32m     56\u001b[0m               trials\u001b[39m=\u001b[39;49mtrials)\n\u001b[1;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest:\u001b[39m\u001b[39m\"\u001b[39m, best_param)\n\u001b[1;32m     60\u001b[0m \u001b[39m# save the trials object\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
            "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective_function\u001b[39m(params):\n\u001b[1;32m     23\u001b[0m   adam_learning_rate, batch_size, dropout, epochs, loss_weight, patience \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m---> 25\u001b[0m   _, _, result \u001b[39m=\u001b[39m run_nn(num_feature\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconvolution\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     26\u001b[0m           loss\u001b[39m=\u001b[39;49mwbce_custom(loss_weight), optimizer\u001b[39m=\u001b[39;49mAdam(learning_rate\u001b[39m=\u001b[39;49madam_learning_rate), dropout\u001b[39m=\u001b[39;49mdropout, patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m     27\u001b[0m           existing_model \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m'\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, train_pairs \u001b[39m=\u001b[39;49m time_site_pairs_train, test_pairs \u001b[39m=\u001b[39;49m time_site_pairs_valid)\n\u001b[1;32m     29\u001b[0m   f1_score \u001b[39m=\u001b[39m result[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m   \u001b[39mif\u001b[39;00m f1_score \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mnan:\n",
            "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mrun_nn\u001b[0;34m(num_feature, model_type, batch_size, epochs, loss, optimizer, existing_model, metrics, dropout, patience, verbose, train_pairs, test_pairs)\u001b[0m\n\u001b[1;32m     26\u001b[0m xy_data \u001b[39m=\u001b[39m get_train_test_val_nn(input_data_,\n\u001b[1;32m     27\u001b[0m                       train_pairs,\n\u001b[1;32m     28\u001b[0m                       test_pairs)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Get history and result\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m model_, history, result \u001b[39m=\u001b[39m fit_nn(xy_data, model_type, existing_model\u001b[39m=\u001b[39;49mexisting_model,\n\u001b[1;32m     32\u001b[0m                                  batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m     33\u001b[0m                                  optimizer\u001b[39m=\u001b[39;49moptimizer, dropout\u001b[39m=\u001b[39;49mdropout, patience\u001b[39m=\u001b[39;49mpatience, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Plot\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m (existing_model \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m&\u001b[39m (verbose \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m&\u001b[39m (verbose \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n",
            "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mfit_nn\u001b[0;34m(xy_data, model_type, existing_model, metrics, loss, optimizer, batch_size, epochs, dropout, patience, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m es \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_f1\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39mpatience)\n\u001b[1;32m     92\u001b[0m \u001b[39m# callback for model checkpoint\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/CapstoneProject/Models/Checkpoints',\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m#                                                                save_weights_only=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[39m# Fit Model\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m[GarbageCollectorCallback(), es], verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    102\u001b[0m \u001b[39m# Evaluate Model\u001b[39;00m\n\u001b[1;32m    103\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test, callbacks\u001b[39m=\u001b[39m[GarbageCollectorCallback()])\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "param_hyperopt = {\n",
        "    'loss_weight': hp.uniform('loss_weight', 1, 50),\n",
        "    'adam_learning_rate': hp.loguniform('learning_rate', np.log(0.000001), np.log(1)),\n",
        "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
        "    'patience': hp.uniform('patience', 3, 50),\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64\n",
        "}\n",
        "\n",
        "# trials=Trials()\n",
        "\n",
        "# loop indefinitely and stop whenever you like\n",
        "while True:\n",
        "    run_trials()\n",
        "\n",
        "# best_param = fmin(objective_function,\n",
        "#                   param_hyperopt,\n",
        "#                   algo=tpe.suggest,\n",
        "#                   max_evals=5,\n",
        "#                   trials=trials)\n",
        "\n",
        "# loss = [x['result']['loss'] for x in trials.trials]\n",
        "# best_param_values = [x for x in best_param.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-31 10:48:08.620015\n",
            "{'name': 'Adam', 'learning_rate': 0.49621807449983946, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "{'name': 'sequential_175', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 15, 15, 6), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_350_input'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_350', 'trainable': True, 'dtype': 'float32', 'batch_input_shape': (None, 15, 15, 6), 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_350', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_350', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_350', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d_351', 'trainable': True, 'dtype': 'float32', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.0005000000237487257}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_351', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([3]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_351', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}, {'class_name': 'AveragePooling2D', 'config': {'name': 'average_pooling2d_351', 'trainable': True, 'dtype': 'float32', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}}, {'class_name': 'Flatten', 'config': {'name': 'flatten_175', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}}, {'class_name': 'Dense', 'config': {'name': 'dense_700', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_525', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_701', 'trainable': True, 'dtype': 'float32', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_526', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_702', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_527', 'trainable': True, 'dtype': 'float32', 'rate': 1.848375226671289e-06, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_703', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'bias_regularizer': {'class_name': 'L2', 'config': {'l2': 0.009999999776482582}}, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n",
            "1170/1170 [==============================] - 2s 1ms/step - loss: nan - acc: 0.9724 - auc: 0.5000 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1: nan\n",
            "2023-07-31 10:52:45.827179\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[nan, 0.9724233746528625, 0.5, 0.0, 0.0, nan]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = {'dropout': 0.49621807449983946, 'learning_rate': 1.848375226671289e-06, 'loss_weight': 3.3453758544544727, 'patience': 24.876679330641913}\n",
        "\n",
        "adam_learning_rate, dropout, loss_weight, patience = best_params.values()\n",
        "epochs, batch_size = 500, 64\n",
        "\n",
        "model, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "        loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "        existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_test)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 2s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# precision-recall curve\n",
        "\n",
        "# Getting X_test and y_test\n",
        "xy_data = get_train_test_val_nn(sites_data,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[nan],\n",
              "       [nan],\n",
              "       [nan],\n",
              "       ...,\n",
              "       [nan],\n",
              "       [nan],\n",
              "       [nan]], dtype=float32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mhYydRcPVt3",
        "outputId": "cc62a475-4c3b-436d-8f1a-90b5314b1ab5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_param' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_param\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_param' is not defined"
          ]
        }
      ],
      "source": [
        "best_param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters to trial\n",
        "adam_learning_rates = [0.000025, 0.00005, 0.000075]\n",
        "loss_weights = [3.25, 3.5, 3.75]\n",
        "dropouts = [0.05, 0.075]\n",
        "patiences = [30]\n",
        "\n",
        "# Fixed Hyperparameters\n",
        "epochs = 500\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 18/18\n",
            "2023-08-19 04:38:47.896296\n"
          ]
        }
      ],
      "source": [
        "# Loop\n",
        "histories = []\n",
        "results = []\n",
        "model_list = []\n",
        "f1_scores = []\n",
        "run_times = []\n",
        "print(datetime.now())\n",
        "i = 0\n",
        "\n",
        "for adam_learning_rate, loss_weight, dropout, patience in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences):\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    # fit model\n",
        "    model_, history, result = run_nn(num_feature=6, model_type=\"convolution\", batch_size=batch_size, epochs=epochs,\n",
        "          loss=wbce_custom(loss_weight), optimizer=Adam(learning_rate=adam_learning_rate), dropout=dropout, patience=patience,\n",
        "          existing_model = None, metrics=['f1'], verbose=0, train_pairs = time_site_pairs_train, test_pairs = time_site_pairs_valid)\n",
        "\n",
        "    # f1 score\n",
        "    f1_score = result[-1]\n",
        "\n",
        "    if f1_score == np.nan:\n",
        "        f1_score = 0\n",
        "\n",
        "\n",
        "    # run times\n",
        "    end_time = datetime.now()\n",
        "    run_time = end_time - start_time\n",
        "\n",
        "    # save model, history, result\n",
        "    model_list.append(model_)\n",
        "    histories.append(history)\n",
        "    results.append(result)\n",
        "    f1_scores.append(f1_score)\n",
        "    run_times.append(run_time)\n",
        "    \n",
        "    i += 1\n",
        "    clear_output(wait=True)\n",
        "    print(f'Progress: {i}/{len(adam_learning_rates)*len(loss_weights)*len(dropouts)*len(patiences)}')\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07589908689260483\n",
            "6\n",
            "(5e-05, 3.25, 0.05, 30)\n"
          ]
        }
      ],
      "source": [
        "# Best Loss \n",
        "print(max(f1_scores))\n",
        "\n",
        "# Best Param\n",
        "print(f1_scores.index(max(f1_scores)))\n",
        "print([i for i in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences)][6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_list[6].save(f'cnn_f0_best.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 24/24\n",
            "2023-08-16 12:39:33.244546\n"
          ]
        }
      ],
      "source": [
        "# Getting results for all hyperparameters\n",
        "\n",
        "results_all = []\n",
        "i = 0\n",
        "\n",
        "# Getting input data\n",
        "input_data_ = sites_data[['ZSD', 'CHL', 'SPM', 'KD490', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "# Getting X_test and y_test\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_valid)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "# appending result\n",
        "for model in model_list:\n",
        "    result = model.evaluate(X_test, y_test)\n",
        "    results_all.append(result)\n",
        "    i += 1\n",
        "    clear_output(wait=True)\n",
        "    print(f'Progress: {i}/{len(adam_learning_rates)*len(loss_weights)*len(dropouts)*len(patiences)}')\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame([i for i in itertools.product(adam_learning_rates, loss_weights, dropouts, patiences)], columns=['adam_learning_rate', 'loss_weight', 'dropout', 'patience'])\n",
        "df2 = pd.DataFrame(results, columns=['loss', 'acc','AUC','Precision','Recall', 'f1'])\n",
        "results_df_nn = pd.concat([df1, df2], axis=1)\n",
        "results_df_nn.to_csv('~data/nn_hyperparameter_3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adam_learning_rate</th>\n",
              "      <th>loss_weight</th>\n",
              "      <th>dropout</th>\n",
              "      <th>patience</th>\n",
              "      <th>loss</th>\n",
              "      <th>acc</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.776733</td>\n",
              "      <td>0.481882</td>\n",
              "      <td>0.620227</td>\n",
              "      <td>0.041020</td>\n",
              "      <td>0.738698</td>\n",
              "      <td>0.074087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.826258</td>\n",
              "      <td>0.440703</td>\n",
              "      <td>0.627981</td>\n",
              "      <td>0.039662</td>\n",
              "      <td>0.772152</td>\n",
              "      <td>0.071743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.836826</td>\n",
              "      <td>0.443776</td>\n",
              "      <td>0.620904</td>\n",
              "      <td>0.039445</td>\n",
              "      <td>0.763110</td>\n",
              "      <td>0.071781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.747793</td>\n",
              "      <td>0.532147</td>\n",
              "      <td>0.623969</td>\n",
              "      <td>0.042355</td>\n",
              "      <td>0.686257</td>\n",
              "      <td>0.075899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.820060</td>\n",
              "      <td>0.433756</td>\n",
              "      <td>0.622578</td>\n",
              "      <td>0.039358</td>\n",
              "      <td>0.775769</td>\n",
              "      <td>0.071778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.779025</td>\n",
              "      <td>0.520683</td>\n",
              "      <td>0.621900</td>\n",
              "      <td>0.041764</td>\n",
              "      <td>0.693490</td>\n",
              "      <td>0.074988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000050</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.757586</td>\n",
              "      <td>0.504276</td>\n",
              "      <td>0.632135</td>\n",
              "      <td>0.041476</td>\n",
              "      <td>0.713382</td>\n",
              "      <td>0.074445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.773208</td>\n",
              "      <td>0.497034</td>\n",
              "      <td>0.619451</td>\n",
              "      <td>0.041179</td>\n",
              "      <td>0.718807</td>\n",
              "      <td>0.074132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.05</td>\n",
              "      <td>30</td>\n",
              "      <td>0.862302</td>\n",
              "      <td>0.384533</td>\n",
              "      <td>0.611220</td>\n",
              "      <td>0.037699</td>\n",
              "      <td>0.808318</td>\n",
              "      <td>0.069255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000075</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.970445</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    adam_learning_rate  loss_weight  dropout  patience      loss       acc   \n",
              "0             0.000025         3.25     0.05        30  0.776733  0.481882  \\\n",
              "1             0.000025         3.25     0.75        30       NaN  0.970445   \n",
              "2             0.000025         3.50     0.05        30  0.826258  0.440703   \n",
              "3             0.000025         3.50     0.75        30       NaN  0.970445   \n",
              "4             0.000025         3.75     0.05        30  0.836826  0.443776   \n",
              "5             0.000025         3.75     0.75        30       NaN  0.970445   \n",
              "6             0.000050         3.25     0.05        30  0.747793  0.532147   \n",
              "7             0.000050         3.25     0.75        30       NaN  0.970445   \n",
              "8             0.000050         3.50     0.05        30  0.820060  0.433756   \n",
              "9             0.000050         3.50     0.75        30       NaN  0.970445   \n",
              "10            0.000050         3.75     0.05        30  0.779025  0.520683   \n",
              "11            0.000050         3.75     0.75        30       NaN  0.970445   \n",
              "12            0.000075         3.25     0.05        30  0.757586  0.504276   \n",
              "13            0.000075         3.25     0.75        30       NaN  0.970445   \n",
              "14            0.000075         3.50     0.05        30  0.773208  0.497034   \n",
              "15            0.000075         3.50     0.75        30       NaN  0.970445   \n",
              "16            0.000075         3.75     0.05        30  0.862302  0.384533   \n",
              "17            0.000075         3.75     0.75        30       NaN  0.970445   \n",
              "\n",
              "         AUC  Precision    Recall        f1  \n",
              "0   0.620227   0.041020  0.738698  0.074087  \n",
              "1   0.500000   0.000000  0.000000       NaN  \n",
              "2   0.627981   0.039662  0.772152  0.071743  \n",
              "3   0.500000   0.000000  0.000000       NaN  \n",
              "4   0.620904   0.039445  0.763110  0.071781  \n",
              "5   0.500000   0.000000  0.000000       NaN  \n",
              "6   0.623969   0.042355  0.686257  0.075899  \n",
              "7   0.500000   0.000000  0.000000       NaN  \n",
              "8   0.622578   0.039358  0.775769  0.071778  \n",
              "9   0.500000   0.000000  0.000000       NaN  \n",
              "10  0.621900   0.041764  0.693490  0.074988  \n",
              "11  0.500000   0.000000  0.000000       NaN  \n",
              "12  0.632135   0.041476  0.713382  0.074445  \n",
              "13  0.500000   0.000000  0.000000       NaN  \n",
              "14  0.619451   0.041179  0.718807  0.074132  \n",
              "15  0.500000   0.000000  0.000000       NaN  \n",
              "16  0.611220   0.037699  0.808318  0.069255  \n",
              "17  0.500000   0.000000  0.000000       NaN  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data_ = sites_data[['ZSD', 'CHL', 'SPM', 'KD490', 'BBP', 'CDM', 'riskLevelLabel', 'site', 'time']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37422\n",
            "37422\n",
            "1170/1170 [==============================] - 1s 999us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PR AUC: 0.08\n"
          ]
        }
      ],
      "source": [
        "# Getting \"test\" data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_valid)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "# PR curve\n",
        "# predictions from best model on validation data\n",
        "predictions = model_list[59].predict(X_test)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.08152645273200347\n",
            "20967\n",
            "Optimal Threshold from Valid data: 0.5035578012466431\n"
          ]
        }
      ],
      "source": [
        "f1_scores = (2 * precision * recall) / (precision + recall)\n",
        "print(max(f1_scores))\n",
        "print(np.argmax(f1_scores))\n",
        "print(f\"Optimal Threshold from Valid data: {thresholds[np.argmax(f1_scores)]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PR AUC: 0.05\n"
          ]
        }
      ],
      "source": [
        "# predictions from best model on test data\n",
        "\n",
        "# getting actual test data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "predictions = model_list[4].predict(X_test)\n",
        "\n",
        "# PR curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'PR AUC: {pr_auc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37423"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(time_site_pairs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, average_precision_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_metrics_from_test_df(df):\n",
        "    y_pred = df['predictions']\n",
        "    y_actual = df['riskLevelLabel']\n",
        "\n",
        "    percent_pos_test = sum(y_actual) / len(df)\n",
        "    f1_ = round(f1_score(y_actual, y_pred), 3)\n",
        "    roc_auc = round(roc_auc_score(y_actual, y_pred), 3)\n",
        "    pr_auc = round(average_precision_score(y_actual, y_pred), 3)\n",
        "    precision = round(precision_score(y_actual, y_pred), 3)\n",
        "    recall = round(recall_score(y_actual, y_pred), 3)\n",
        "    accuracy = round(accuracy_score(y_actual, y_pred), 3)\n",
        "    \n",
        "    metrics_list = [len(df), percent_pos_test, f1_, roc_auc, pr_auc, precision, recall, accuracy]\n",
        "    return metrics_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EUBWID</th>\n",
              "      <th>Site Name in Files</th>\n",
              "      <th>district</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>seasonStartDate</th>\n",
              "      <th>seasonFinishDate</th>\n",
              "      <th>pollutionRiskForecasting</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ukd5300-41300</td>\n",
              "      <td>Ainsdale</td>\n",
              "      <td>Sefton</td>\n",
              "      <td>53.607927</td>\n",
              "      <td>-3.063988</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>True</td>\n",
              "      <td>North West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ukd1101-46100</td>\n",
              "      <td>Allonby</td>\n",
              "      <td>Allerdale</td>\n",
              "      <td>54.768036</td>\n",
              "      <td>-3.434625</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>True</td>\n",
              "      <td>North West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ukd1101-46000</td>\n",
              "      <td>Allonby_South</td>\n",
              "      <td>Allerdale</td>\n",
              "      <td>54.743960</td>\n",
              "      <td>-3.456454</td>\n",
              "      <td>01/05/2019</td>\n",
              "      <td>30/09/2019</td>\n",
              "      <td>True</td>\n",
              "      <td>North West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ukc2101-04250</td>\n",
              "      <td>Amble_Links</td>\n",
              "      <td>Northumberland</td>\n",
              "      <td>55.325652</td>\n",
              "      <td>-1.552401</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>False</td>\n",
              "      <td>North East</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ukf3102-09400</td>\n",
              "      <td>Anderby</td>\n",
              "      <td>Lincolnshire</td>\n",
              "      <td>53.259774</td>\n",
              "      <td>0.325520</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>False</td>\n",
              "      <td>East Midlands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>uke1200-08900</td>\n",
              "      <td>Withernsea</td>\n",
              "      <td>East Riding of Yorkshire</td>\n",
              "      <td>53.730820</td>\n",
              "      <td>0.036008</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>False</td>\n",
              "      <td>Yorkshire and The Humber</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>ukj1402-11946</td>\n",
              "      <td>Wolvercote_Mill_Stream</td>\n",
              "      <td>Oxford</td>\n",
              "      <td>51.780820</td>\n",
              "      <td>-1.295960</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>False</td>\n",
              "      <td>South East</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>ukk4304-34400</td>\n",
              "      <td>Woolacombe_Village</td>\n",
              "      <td>North Devon</td>\n",
              "      <td>51.171653</td>\n",
              "      <td>-4.209700</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>True</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>ukj2407-15400</td>\n",
              "      <td>Worthing</td>\n",
              "      <td>Worthing</td>\n",
              "      <td>50.807278</td>\n",
              "      <td>-0.384861</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>True</td>\n",
              "      <td>South East</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>ukj3400-18350</td>\n",
              "      <td>Yaverland</td>\n",
              "      <td>Isle of Wight</td>\n",
              "      <td>50.661819</td>\n",
              "      <td>-1.133235</td>\n",
              "      <td>01/05/2022</td>\n",
              "      <td>30/09/2022</td>\n",
              "      <td>False</td>\n",
              "      <td>South East</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>430 rows  9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            EUBWID      Site Name in Files                  district   \n",
              "0    ukd5300-41300                Ainsdale                    Sefton  \\\n",
              "1    ukd1101-46100                 Allonby                 Allerdale   \n",
              "2    ukd1101-46000           Allonby_South                 Allerdale   \n",
              "3    ukc2101-04250             Amble_Links            Northumberland   \n",
              "4    ukf3102-09400                 Anderby              Lincolnshire   \n",
              "..             ...                     ...                       ...   \n",
              "425  uke1200-08900              Withernsea  East Riding of Yorkshire   \n",
              "426  ukj1402-11946  Wolvercote_Mill_Stream                    Oxford   \n",
              "427  ukk4304-34400      Woolacombe_Village               North Devon   \n",
              "428  ukj2407-15400                Worthing                  Worthing   \n",
              "429  ukj3400-18350               Yaverland             Isle of Wight   \n",
              "\n",
              "           lat      long seasonStartDate seasonFinishDate   \n",
              "0    53.607927 -3.063988      01/05/2022       30/09/2022  \\\n",
              "1    54.768036 -3.434625      01/05/2022       30/09/2022   \n",
              "2    54.743960 -3.456454      01/05/2019       30/09/2019   \n",
              "3    55.325652 -1.552401      01/05/2022       30/09/2022   \n",
              "4    53.259774  0.325520      01/05/2022       30/09/2022   \n",
              "..         ...       ...             ...              ...   \n",
              "425  53.730820  0.036008      01/05/2022       30/09/2022   \n",
              "426  51.780820 -1.295960      01/05/2022       30/09/2022   \n",
              "427  51.171653 -4.209700      01/05/2022       30/09/2022   \n",
              "428  50.807278 -0.384861      01/05/2022       30/09/2022   \n",
              "429  50.661819 -1.133235      01/05/2022       30/09/2022   \n",
              "\n",
              "     pollutionRiskForecasting                    region  \n",
              "0                        True                North West  \n",
              "1                        True                North West  \n",
              "2                        True                North West  \n",
              "3                       False                North East  \n",
              "4                       False             East Midlands  \n",
              "..                        ...                       ...  \n",
              "425                     False  Yorkshire and The Humber  \n",
              "426                     False                South East  \n",
              "427                      True                South West  \n",
              "428                      True                South East  \n",
              "429                     False                South East  \n",
              "\n",
              "[430 rows x 9 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "site_with_region = pd.read_csv('~data/site.csv')\n",
        "site_with_region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>site</th>\n",
              "      <th>ZSD</th>\n",
              "      <th>CHL</th>\n",
              "      <th>SPM</th>\n",
              "      <th>KD490</th>\n",
              "      <th>BBP</th>\n",
              "      <th>CDM</th>\n",
              "      <th>riskLevelLabel</th>\n",
              "      <th>EUBWID</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-25</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-07-04</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-08-08</td>\n",
              "      <td>ukk3105-31450</td>\n",
              "      <td>[[3.3500542033613443, 3.3500542033613443, 3.35...</td>\n",
              "      <td>[[7.30686214117647, 7.30686214117647, 7.306862...</td>\n",
              "      <td>[[0.9704474882575758, 0.9704474882575758, 0.97...</td>\n",
              "      <td>[[0.276538217815126, 0.276538217815126, 0.2765...</td>\n",
              "      <td>[[0.0036920992657894, 0.0036920992657894, 0.00...</td>\n",
              "      <td>[[0.144712753622807, 0.144712753622807, 0.1447...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk3105-31450</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-08-27</td>\n",
              "      <td>ukk2206-20800</td>\n",
              "      <td>[[6.7459197, 6.6718507, 6.566642, 6.4457297, 6...</td>\n",
              "      <td>[[1.4204581, 1.4440619, 1.4789817, 1.5192833, ...</td>\n",
              "      <td>[[3.0617452, 3.2359302, 3.3227324, 3.4029508, ...</td>\n",
              "      <td>[[0.114441626, 0.11552915, 0.11712431, 0.11896...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk2206-20800</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-06-08</td>\n",
              "      <td>uke1301-09020</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>uke1301-09020</td>\n",
              "      <td>Yorkshire and The Humber</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112261</th>\n",
              "      <td>2021-07-19</td>\n",
              "      <td>ukj4210-12660</td>\n",
              "      <td>[[3.130174548245614, 3.130174548245614, 3.1301...</td>\n",
              "      <td>[[4.270181615789474, 4.270181615789474, 4.2701...</td>\n",
              "      <td>[[5.880774268461538, 5.880774268461538, 5.8807...</td>\n",
              "      <td>[[0.2214226485964912, 0.2214226485964912, 0.22...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukj4210-12660</td>\n",
              "      <td>South East</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112262</th>\n",
              "      <td>2021-05-10</td>\n",
              "      <td>ukk4200-23200</td>\n",
              "      <td>[[3.55662035, 3.55662035, 3.55662035, 3.556620...</td>\n",
              "      <td>[[3.5508095, 3.5508095, 3.5508095, 3.5508095, ...</td>\n",
              "      <td>[[2.170633925, 2.170633925, 2.170633925, 2.170...</td>\n",
              "      <td>[[0.19762554, 0.19762554, 0.19762554, 0.197625...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk4200-23200</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112263</th>\n",
              "      <td>2020-08-10</td>\n",
              "      <td>ukk3104-33500</td>\n",
              "      <td>[[10.020489333333334, 10.020489333333334, 10.0...</td>\n",
              "      <td>[[0.7708462333333334, 0.7708462333333334, 0.77...</td>\n",
              "      <td>[[0.9726023225, 0.9726023225, 0.9726023225, 0....</td>\n",
              "      <td>[[0.0815022166666666, 0.0815022166666666, 0.08...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk3104-33500</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112264</th>\n",
              "      <td>2021-09-08</td>\n",
              "      <td>ukk1202-35700</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "      <td>ukk1202-35700</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112265</th>\n",
              "      <td>2020-08-09</td>\n",
              "      <td>ukk3101-27000</td>\n",
              "      <td>[[8.295259, 8.60793, 8.488777, 8.288995, 8.061...</td>\n",
              "      <td>[[1.0417088, 0.9814289, 1.0022881, 1.0407631, ...</td>\n",
              "      <td>[[0.36700276, 0.37847954, 0.3985074, 0.4057661...</td>\n",
              "      <td>[[0.09601207, 0.092920884, 0.094014056, 0.0959...</td>\n",
              "      <td>[[0.0017447872, 0.0018362086, 0.0018678783, 0....</td>\n",
              "      <td>[[0.10112062, 0.09079216, 0.08552243, 0.083623...</td>\n",
              "      <td>0</td>\n",
              "      <td>ukk3101-27000</td>\n",
              "      <td>South West</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112266 rows  11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             time           site   \n",
              "0      2021-05-25  ukk4305-25800  \\\n",
              "1      2022-07-04  ukk4305-25800   \n",
              "2      2022-08-08  ukk3105-31450   \n",
              "3      2022-08-27  ukk2206-20800   \n",
              "4      2020-06-08  uke1301-09020   \n",
              "...           ...            ...   \n",
              "112261 2021-07-19  ukj4210-12660   \n",
              "112262 2021-05-10  ukk4200-23200   \n",
              "112263 2020-08-10  ukk3104-33500   \n",
              "112264 2021-09-08  ukk1202-35700   \n",
              "112265 2020-08-09  ukk3101-27000   \n",
              "\n",
              "                                                      ZSD   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[3.3500542033613443, 3.3500542033613443, 3.35...   \n",
              "3       [[6.7459197, 6.6718507, 6.566642, 6.4457297, 6...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[3.130174548245614, 3.130174548245614, 3.1301...   \n",
              "112262  [[3.55662035, 3.55662035, 3.55662035, 3.556620...   \n",
              "112263  [[10.020489333333334, 10.020489333333334, 10.0...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[8.295259, 8.60793, 8.488777, 8.288995, 8.061...   \n",
              "\n",
              "                                                      CHL   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[7.30686214117647, 7.30686214117647, 7.306862...   \n",
              "3       [[1.4204581, 1.4440619, 1.4789817, 1.5192833, ...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[4.270181615789474, 4.270181615789474, 4.2701...   \n",
              "112262  [[3.5508095, 3.5508095, 3.5508095, 3.5508095, ...   \n",
              "112263  [[0.7708462333333334, 0.7708462333333334, 0.77...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[1.0417088, 0.9814289, 1.0022881, 1.0407631, ...   \n",
              "\n",
              "                                                      SPM   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.9704474882575758, 0.9704474882575758, 0.97...   \n",
              "3       [[3.0617452, 3.2359302, 3.3227324, 3.4029508, ...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[5.880774268461538, 5.880774268461538, 5.8807...   \n",
              "112262  [[2.170633925, 2.170633925, 2.170633925, 2.170...   \n",
              "112263  [[0.9726023225, 0.9726023225, 0.9726023225, 0....   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.36700276, 0.37847954, 0.3985074, 0.4057661...   \n",
              "\n",
              "                                                    KD490   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.276538217815126, 0.276538217815126, 0.2765...   \n",
              "3       [[0.114441626, 0.11552915, 0.11712431, 0.11896...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[0.2214226485964912, 0.2214226485964912, 0.22...   \n",
              "112262  [[0.19762554, 0.19762554, 0.19762554, 0.197625...   \n",
              "112263  [[0.0815022166666666, 0.0815022166666666, 0.08...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.09601207, 0.092920884, 0.094014056, 0.0959...   \n",
              "\n",
              "                                                      BBP   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.0036920992657894, 0.0036920992657894, 0.00...   \n",
              "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112262  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112263  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.0017447872, 0.0018362086, 0.0018678783, 0....   \n",
              "\n",
              "                                                      CDM  riskLevelLabel   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "2       [[0.144712753622807, 0.144712753622807, 0.1447...               0   \n",
              "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "...                                                   ...             ...   \n",
              "112261  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "112262  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "112263  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               1   \n",
              "112265  [[0.10112062, 0.09079216, 0.08552243, 0.083623...               0   \n",
              "\n",
              "               EUBWID                    region  \n",
              "0       ukk4305-25800                South West  \n",
              "1       ukk4305-25800                South West  \n",
              "2       ukk3105-31450                South West  \n",
              "3       ukk2206-20800                South West  \n",
              "4       uke1301-09020  Yorkshire and The Humber  \n",
              "...               ...                       ...  \n",
              "112261  ukj4210-12660                South East  \n",
              "112262  ukk4200-23200                South West  \n",
              "112263  ukk3104-33500                South West  \n",
              "112264  ukk1202-35700                South West  \n",
              "112265  ukk3101-27000                South West  \n",
              "\n",
              "[112266 rows x 11 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.merge(time_site_pairs_train, sites_data, on=['time', 'site'])\n",
        "train = train.merge(site_with_region[['EUBWID', 'region']], how='left', left_on='site', right_on='EUBWID')\n",
        "train['time'] = pd.to_datetime(train['time'])\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_percent_pos_train(df):\n",
        "    y_actual = df['riskLevelLabel']\n",
        "    return sum(y_actual) / len(df) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020: 3.291110608459624\n",
            "2021: 3.0794248986861463\n",
            "2022: 3.291110608459624\n",
            "May: 1.5554983260365698\n",
            "Jun: 2.258435922416084\n",
            "Jul: 1.8845450291402561\n",
            "Aug: 3.3907034636679683\n",
            "Sep: 4.620255987156045\n",
            "South West: 2.455636207572167\n",
            "South East: 2.5582128541188127\n",
            "East of England: 2.1468276787425724\n",
            "North East: 2.9177139110479726\n",
            "North West: 5.660134089736977\n",
            "Yorkshire: 4.209988068859724\n",
            "East Midlands: 1.9761029411764706\n",
            "London: 3.2169117647058822\n"
          ]
        }
      ],
      "source": [
        "# % pos train samples by year\n",
        "print(f\"2020: {get_percent_pos_train(train[train['time'] <= '2020-12-31'])}\")\n",
        "print(f\"2021: {get_percent_pos_train(train[(train['time'] <= '2021-12-31') & (train['time'] >= '2021-01-01')])}\")\n",
        "print(f\"2022: {get_percent_pos_train(train[train['time'] <= '2020-12-31'])}\")\n",
        "\n",
        "# % pos train samples by month\n",
        "print(f\"May: {get_percent_pos_train(train[train['time'].dt.month==5])}\")\n",
        "print(f\"Jun: {get_percent_pos_train(train[train['time'].dt.month==6])}\")\n",
        "print(f\"Jul: {get_percent_pos_train(train[train['time'].dt.month==7])}\")\n",
        "print(f\"Aug: {get_percent_pos_train(train[train['time'].dt.month==8])}\")\n",
        "print(f\"Sep: {get_percent_pos_train(train[train['time'].dt.month==9])}\")\n",
        "\n",
        "# % pos train samples by region\n",
        "print(f\"South West: {get_percent_pos_train(train[train['region'] == 'South West'])}\")\n",
        "print(f\"South East: {get_percent_pos_train(train[train['region'] == 'South East'])}\")\n",
        "print(f\"East of England: {get_percent_pos_train(train[train['region'] == 'East of England'])}\")\n",
        "print(f\"North East: {get_percent_pos_train(train[train['region'] == 'North East'])}\")\n",
        "print(f\"North West: {get_percent_pos_train(train[train['region'] == 'North West'])}\")\n",
        "print(f\"Yorkshire: {get_percent_pos_train(train[train['region'] == 'Yorkshire and The Humber'])}\")\n",
        "print(f\"East Midlands: {get_percent_pos_train(train[train['region'] == 'East Midlands'])}\")\n",
        "print(f\"London: {get_percent_pos_train(train[train['region'] == 'London'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11766, 0.079, 0.605, 0.039, 0.042, 0.774, 0.447]\n",
            "[12894, 0.079, 0.602, 0.039, 0.042, 0.771, 0.443]\n",
            "[12763, 0.059, 0.629, 0.029, 0.031, 0.787, 0.478]\n",
            "[6430, 0.049, 0.669, 0.024, 0.025, 0.848, 0.496]\n",
            "[7690, 0.055, 0.608, 0.027, 0.028, 0.78, 0.442]\n",
            "[7829, 0.054, 0.622, 0.027, 0.028, 0.806, 0.445]\n",
            "[7683, 0.089, 0.608, 0.044, 0.047, 0.76, 0.467]\n",
            "[7429, 0.108, 0.588, 0.055, 0.058, 0.752, 0.438]\n",
            "[17122, 0.067, 0.629, 0.033, 0.035, 0.782, 0.484]\n",
            "[6421, 0.084, 0.619, 0.041, 0.044, 0.74, 0.506]\n",
            "[3510, 0.045, 0.575, 0.022, 0.023, 0.738, 0.418]\n",
            "[2953, 0.089, 0.647, 0.044, 0.047, 0.848, 0.458]\n",
            "[2557, 0.116, 0.581, 0.061, 0.062, 0.863, 0.328]\n",
            "[1832, 0.075, 0.516, 0.039, 0.04, 0.629, 0.412]\n",
            "[363, 0.05, 0.515, 0.026, 0.026, 0.778, 0.264]\n"
          ]
        }
      ],
      "source": [
        "classes = [1 if i > 0.5035578012466431 else 0 for i in predictions]\n",
        "\n",
        "# Merge predictions onto test dataset\n",
        "test = pd.merge(time_site_pairs_test, sites_data, on=['time', 'site'])\n",
        "test['time'] = pd.to_datetime(test['time'])\n",
        "test['predictions'] = classes\n",
        "\n",
        "# grouping predictions by year\n",
        "test_2020 = get_metrics_from_test_df(test[test['time'] <= '2020-12-31'])\n",
        "test_2021 = get_metrics_from_test_df(test[(test['time'] <= '2021-12-31') & (test['time'] >= '2021-01-01')])\n",
        "test_2022 = get_metrics_from_test_df(test[test['time'] >= '2022-01-01'])\n",
        "\n",
        "print(test_2020)\n",
        "print(test_2021)\n",
        "print(test_2022)\n",
        "\n",
        "# grouping predictions by month\n",
        "test_may = get_metrics_from_test_df(test[test['time'].dt.month==5])\n",
        "test_jun = get_metrics_from_test_df(test[test['time'].dt.month==6])\n",
        "test_jul = get_metrics_from_test_df(test[test['time'].dt.month==7])\n",
        "test_aug = get_metrics_from_test_df(test[test['time'].dt.month==8])\n",
        "test_sep = get_metrics_from_test_df(test[test['time'].dt.month==9])\n",
        "\n",
        "print(test_may)\n",
        "print(test_jun)\n",
        "print(test_jul)\n",
        "print(test_aug)\n",
        "print(test_sep)\n",
        "\n",
        "# grouping predictions by regions\n",
        "test = test.merge(site_with_region[['EUBWID', 'region']], how='left', left_on='site', right_on='EUBWID')\n",
        "test_sw = get_metrics_from_test_df(test[test['region'] == 'South West'])\n",
        "test_se = get_metrics_from_test_df(test[test['region'] == 'South East'])\n",
        "test_ee = get_metrics_from_test_df(test[test['region'] == 'East of England'])\n",
        "test_ne = get_metrics_from_test_df(test[test['region'] == 'North East'])\n",
        "test_nw = get_metrics_from_test_df(test[test['region'] == 'North West'])\n",
        "test_yh = get_metrics_from_test_df(test[test['region'] == 'Yorkshire and The Humber'])\n",
        "test_em = get_metrics_from_test_df(test[test['region'] == 'East Midlands'])\n",
        "\n",
        "print(test_sw)\n",
        "print(test_se)\n",
        "print(test_ee)\n",
        "print(test_ne)\n",
        "print(test_nw)\n",
        "print(test_yh)\n",
        "print(test_em)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>site</th>\n",
              "      <th>ZSD</th>\n",
              "      <th>CHL</th>\n",
              "      <th>SPM</th>\n",
              "      <th>KD490</th>\n",
              "      <th>BBP</th>\n",
              "      <th>CDM</th>\n",
              "      <th>riskLevelLabel</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-09-21</td>\n",
              "      <td>ukk3106-32100</td>\n",
              "      <td>[[4.913196680952381, 4.913196680952381, 4.9131...</td>\n",
              "      <td>[[2.273623676190476, 2.273623676190476, 2.2736...</td>\n",
              "      <td>[[0.4881589017808219, 0.4881589017808219, 0.48...</td>\n",
              "      <td>[[0.1506011823809523, 0.1506011823809523, 0.15...</td>\n",
              "      <td>[[0.00381895475, 0.00381895475, 0.00381895475,...</td>\n",
              "      <td>[[0.06358246325, 0.06358246325, 0.06358246325,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-08-08</td>\n",
              "      <td>ukh1407-10750</td>\n",
              "      <td>[[3.111512202469136, 3.111512202469136, 3.1115...</td>\n",
              "      <td>[[4.29619640617284, 4.29619640617284, 4.296196...</td>\n",
              "      <td>[[10.269349237931037, 10.269349237931037, 10.2...</td>\n",
              "      <td>[[0.22232296, 0.22232296, 0.22232296, 0.222322...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-09-20</td>\n",
              "      <td>ukd4200-42100</td>\n",
              "      <td>[[3.5333333, 3.5890448, 3.6046667, 3.5832536, ...</td>\n",
              "      <td>[[3.585421, 3.5056612, 3.4843953, 3.5138142, 3...</td>\n",
              "      <td>[[13.234664, 12.486257, 11.963818, 11.449716, ...</td>\n",
              "      <td>[[0.19879897, 0.19607617, 0.19534324, 0.196355...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-07-06</td>\n",
              "      <td>ukk2206-20900</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-08-24</td>\n",
              "      <td>ukk4305-26100</td>\n",
              "      <td>[[7.55586, 7.613758, 7.6196814, 7.302967, 7.06...</td>\n",
              "      <td>[[1.1989712, 1.1844647, 1.1831877, 1.2617273, ...</td>\n",
              "      <td>[[0.9801996, 0.9878833, 0.98672915, 0.9627455,...</td>\n",
              "      <td>[[0.103905775, 0.103202544, 0.10313862, 0.1069...</td>\n",
              "      <td>[[0.006364123, 0.006630237, 0.0068600005, 0.00...</td>\n",
              "      <td>[[0.059718873, 0.06342808, 0.066719994, 0.0632...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37418</th>\n",
              "      <td>2021-09-22</td>\n",
              "      <td>ukj2402-15680</td>\n",
              "      <td>[[3.5983627, 3.7204835, 3.8346908, 3.8554592, ...</td>\n",
              "      <td>[[3.4994717, 3.3326082, 3.193268, 3.1690927, 3...</td>\n",
              "      <td>[[3.6528094, 3.859371, 4.090318, 4.1943636, 4....</td>\n",
              "      <td>[[0.19582026, 0.19007482, 0.18517214, 0.184313...</td>\n",
              "      <td>[[0.026421161, 0.027476275, 0.029065508, 0.029...</td>\n",
              "      <td>[[0.15839544, 0.15553693, 0.15557131, 0.156752...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37419</th>\n",
              "      <td>2021-08-14</td>\n",
              "      <td>ukk3104-33500</td>\n",
              "      <td>[[6.314672, 6.184491423076923, 6.1844914230769...</td>\n",
              "      <td>[[1.5664485, 1.7227391012820514, 1.72273910128...</td>\n",
              "      <td>[[2.1245860052564103, 2.1245860052564103, 2.12...</td>\n",
              "      <td>[[0.121082254, 0.1268671209230769, 0.126867120...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37420</th>\n",
              "      <td>2022-05-20</td>\n",
              "      <td>ukk4305-25600</td>\n",
              "      <td>[[7.233569, 6.788397, 6.497776, 6.179102, 5.66...</td>\n",
              "      <td>[[1.2806963, 1.4113798, 1.507687, 1.6207474, 1...</td>\n",
              "      <td>[[0.7209197, 0.8662733, 0.96065444, 1.0638868,...</td>\n",
              "      <td>[[0.10785786, 0.11398184, 0.11837783, 0.123468...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37421</th>\n",
              "      <td>2022-06-25</td>\n",
              "      <td>ukj3400-18000</td>\n",
              "      <td>[[4.826235210169491, 4.826235210169491, 4.7515...</td>\n",
              "      <td>[[2.309685054237288, 2.309685054237288, 2.3593...</td>\n",
              "      <td>[[5.076520970270271, 5.076520970270271, 5.0765...</td>\n",
              "      <td>[[0.1522006781355932, 0.1522006781355932, 0.15...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37422</th>\n",
              "      <td>2021-09-27</td>\n",
              "      <td>ukj2201-14500</td>\n",
              "      <td>[[4.6731544, 4.6279726, 4.567748, 4.5306134, 4...</td>\n",
              "      <td>[[2.4171996, 2.4523227, 2.4965448, 2.5296576, ...</td>\n",
              "      <td>[[5.148772, 5.1170473, 5.0943193, 4.880226, 4....</td>\n",
              "      <td>[[0.15641254, 0.15776166, 0.15947865, 0.160718...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37423 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            time           site   \n",
              "0     2022-09-21  ukk3106-32100  \\\n",
              "1     2020-08-08  ukh1407-10750   \n",
              "2     2020-09-20  ukd4200-42100   \n",
              "3     2022-07-06  ukk2206-20900   \n",
              "4     2021-08-24  ukk4305-26100   \n",
              "...          ...            ...   \n",
              "37418 2021-09-22  ukj2402-15680   \n",
              "37419 2021-08-14  ukk3104-33500   \n",
              "37420 2022-05-20  ukk4305-25600   \n",
              "37421 2022-06-25  ukj3400-18000   \n",
              "37422 2021-09-27  ukj2201-14500   \n",
              "\n",
              "                                                     ZSD   \n",
              "0      [[4.913196680952381, 4.913196680952381, 4.9131...  \\\n",
              "1      [[3.111512202469136, 3.111512202469136, 3.1115...   \n",
              "2      [[3.5333333, 3.5890448, 3.6046667, 3.5832536, ...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[7.55586, 7.613758, 7.6196814, 7.302967, 7.06...   \n",
              "...                                                  ...   \n",
              "37418  [[3.5983627, 3.7204835, 3.8346908, 3.8554592, ...   \n",
              "37419  [[6.314672, 6.184491423076923, 6.1844914230769...   \n",
              "37420  [[7.233569, 6.788397, 6.497776, 6.179102, 5.66...   \n",
              "37421  [[4.826235210169491, 4.826235210169491, 4.7515...   \n",
              "37422  [[4.6731544, 4.6279726, 4.567748, 4.5306134, 4...   \n",
              "\n",
              "                                                     CHL   \n",
              "0      [[2.273623676190476, 2.273623676190476, 2.2736...  \\\n",
              "1      [[4.29619640617284, 4.29619640617284, 4.296196...   \n",
              "2      [[3.585421, 3.5056612, 3.4843953, 3.5138142, 3...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[1.1989712, 1.1844647, 1.1831877, 1.2617273, ...   \n",
              "...                                                  ...   \n",
              "37418  [[3.4994717, 3.3326082, 3.193268, 3.1690927, 3...   \n",
              "37419  [[1.5664485, 1.7227391012820514, 1.72273910128...   \n",
              "37420  [[1.2806963, 1.4113798, 1.507687, 1.6207474, 1...   \n",
              "37421  [[2.309685054237288, 2.309685054237288, 2.3593...   \n",
              "37422  [[2.4171996, 2.4523227, 2.4965448, 2.5296576, ...   \n",
              "\n",
              "                                                     SPM   \n",
              "0      [[0.4881589017808219, 0.4881589017808219, 0.48...  \\\n",
              "1      [[10.269349237931037, 10.269349237931037, 10.2...   \n",
              "2      [[13.234664, 12.486257, 11.963818, 11.449716, ...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[0.9801996, 0.9878833, 0.98672915, 0.9627455,...   \n",
              "...                                                  ...   \n",
              "37418  [[3.6528094, 3.859371, 4.090318, 4.1943636, 4....   \n",
              "37419  [[2.1245860052564103, 2.1245860052564103, 2.12...   \n",
              "37420  [[0.7209197, 0.8662733, 0.96065444, 1.0638868,...   \n",
              "37421  [[5.076520970270271, 5.076520970270271, 5.0765...   \n",
              "37422  [[5.148772, 5.1170473, 5.0943193, 4.880226, 4....   \n",
              "\n",
              "                                                   KD490   \n",
              "0      [[0.1506011823809523, 0.1506011823809523, 0.15...  \\\n",
              "1      [[0.22232296, 0.22232296, 0.22232296, 0.222322...   \n",
              "2      [[0.19879897, 0.19607617, 0.19534324, 0.196355...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[0.103905775, 0.103202544, 0.10313862, 0.1069...   \n",
              "...                                                  ...   \n",
              "37418  [[0.19582026, 0.19007482, 0.18517214, 0.184313...   \n",
              "37419  [[0.121082254, 0.1268671209230769, 0.126867120...   \n",
              "37420  [[0.10785786, 0.11398184, 0.11837783, 0.123468...   \n",
              "37421  [[0.1522006781355932, 0.1522006781355932, 0.15...   \n",
              "37422  [[0.15641254, 0.15776166, 0.15947865, 0.160718...   \n",
              "\n",
              "                                                     BBP   \n",
              "0      [[0.00381895475, 0.00381895475, 0.00381895475,...  \\\n",
              "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[0.006364123, 0.006630237, 0.0068600005, 0.00...   \n",
              "...                                                  ...   \n",
              "37418  [[0.026421161, 0.027476275, 0.029065508, 0.029...   \n",
              "37419  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37420  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37421  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37422  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                     CDM  riskLevelLabel   \n",
              "0      [[0.06358246325, 0.06358246325, 0.06358246325,...               0  \\\n",
              "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               1   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "4      [[0.059718873, 0.06342808, 0.066719994, 0.0632...               0   \n",
              "...                                                  ...             ...   \n",
              "37418  [[0.15839544, 0.15553693, 0.15557131, 0.156752...               0   \n",
              "37419  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "37420  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "37421  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "37422  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               1   \n",
              "\n",
              "       predictions  \n",
              "0                0  \n",
              "1                1  \n",
              "2                0  \n",
              "3                1  \n",
              "4                0  \n",
              "...            ...  \n",
              "37418            0  \n",
              "37419            1  \n",
              "37420            0  \n",
              "37421            0  \n",
              "37422            0  \n",
              "\n",
              "[37423 rows x 10 columns]"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>site</th>\n",
              "      <th>ZSD</th>\n",
              "      <th>CHL</th>\n",
              "      <th>SPM</th>\n",
              "      <th>KD490</th>\n",
              "      <th>BBP</th>\n",
              "      <th>CDM</th>\n",
              "      <th>riskLevelLabel</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020-05-18</td>\n",
              "      <td>ukh3100-11800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[11.081272150442478, 11.081272150442478, 11.0...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2021-05-17</td>\n",
              "      <td>ukj3400-17800</td>\n",
              "      <td>[[1.7213999670731708, 1.7213999670731708, 1.72...</td>\n",
              "      <td>[[11.465300902439024, 11.465300902439024, 11.4...</td>\n",
              "      <td>[[5.8290533160919535, 5.8290533160919535, 5.82...</td>\n",
              "      <td>[[0.413901141097561, 0.413901141097561, 0.4139...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2022-05-20</td>\n",
              "      <td>ukk3102-28800</td>\n",
              "      <td>[[8.462011428571428, 8.462011428571428, 8.4620...</td>\n",
              "      <td>[[1.0098425657142858, 1.0098425657142858, 1.00...</td>\n",
              "      <td>[[1.5031724625, 1.5031724625, 1.5031724625, 1....</td>\n",
              "      <td>[[0.0943761257142857, 0.0943761257142857, 0.09...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2020-05-29</td>\n",
              "      <td>ukk3103-29100</td>\n",
              "      <td>[[2.536445, 2.8695407, 3.3395765, 3.482277, 3....</td>\n",
              "      <td>[[5.9330025, 4.8541126, 3.9149563, 3.6688542, ...</td>\n",
              "      <td>[[0.5655572, 0.56948805, 0.58077556, 0.5880381...</td>\n",
              "      <td>[[0.2714681, 0.23976888, 0.20967583, 0.2015797...</td>\n",
              "      <td>[[0.0034846168, 0.0034846168, 0.0034846168, 0....</td>\n",
              "      <td>[[0.2220552666666666, 0.2220552666666666, 0.22...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2022-05-22</td>\n",
              "      <td>ukd1103-45600</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37405</th>\n",
              "      <td>2020-05-15</td>\n",
              "      <td>ukk3106-27700</td>\n",
              "      <td>[[1.586071975862069, 1.586071975862069, 1.5860...</td>\n",
              "      <td>[[14.029530068965515, 14.029530068965515, 14.0...</td>\n",
              "      <td>[[1.389701678472222, 1.389701678472222, 1.3897...</td>\n",
              "      <td>[[0.4712203374137931, 0.4712203374137931, 0.47...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37406</th>\n",
              "      <td>2021-05-13</td>\n",
              "      <td>ukk4305-26000</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37407</th>\n",
              "      <td>2021-05-19</td>\n",
              "      <td>ukk3103-29200</td>\n",
              "      <td>[[3.6910598, 3.7330277, 3.6836667, 3.6931899, ...</td>\n",
              "      <td>[[3.369864, 3.3179538, 3.387065, 3.3800738, 3....</td>\n",
              "      <td>[[0.8680219, 0.8355887, 0.81544447, 0.79954606...</td>\n",
              "      <td>[[0.19137645, 0.1895548, 0.19192517, 0.1916487...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37411</th>\n",
              "      <td>2022-05-07</td>\n",
              "      <td>ukk2305-35000</td>\n",
              "      <td>[[2.6402642000000003, 2.6402642000000003, 2.64...</td>\n",
              "      <td>[[5.465555887254902, 5.465555887254902, 5.4655...</td>\n",
              "      <td>[[28.75784358490566, 28.75784358490566, 28.757...</td>\n",
              "      <td>[[0.2583801460784313, 0.2583801460784313, 0.25...</td>\n",
              "      <td>[[0.0587452827857142, 0.0587452827857142, 0.05...</td>\n",
              "      <td>[[0.4374420108333333, 0.4374420108333333, 0.43...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37420</th>\n",
              "      <td>2022-05-20</td>\n",
              "      <td>ukk4305-25600</td>\n",
              "      <td>[[7.233569, 6.788397, 6.497776, 6.179102, 5.66...</td>\n",
              "      <td>[[1.2806963, 1.4113798, 1.507687, 1.6207474, 1...</td>\n",
              "      <td>[[0.7209197, 0.8662733, 0.96065444, 1.0638868,...</td>\n",
              "      <td>[[0.10785786, 0.11398184, 0.11837783, 0.123468...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6430 rows  10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            time           site   \n",
              "12    2020-05-18  ukh3100-11800  \\\n",
              "14    2021-05-17  ukj3400-17800   \n",
              "15    2022-05-20  ukk3102-28800   \n",
              "16    2020-05-29  ukk3103-29100   \n",
              "19    2022-05-22  ukd1103-45600   \n",
              "...          ...            ...   \n",
              "37405 2020-05-15  ukk3106-27700   \n",
              "37406 2021-05-13  ukk4305-26000   \n",
              "37407 2021-05-19  ukk3103-29200   \n",
              "37411 2022-05-07  ukk2305-35000   \n",
              "37420 2022-05-20  ukk4305-25600   \n",
              "\n",
              "                                                     ZSD   \n",
              "12     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "14     [[1.7213999670731708, 1.7213999670731708, 1.72...   \n",
              "15     [[8.462011428571428, 8.462011428571428, 8.4620...   \n",
              "16     [[2.536445, 2.8695407, 3.3395765, 3.482277, 3....   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                  ...   \n",
              "37405  [[1.586071975862069, 1.586071975862069, 1.5860...   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37407  [[3.6910598, 3.7330277, 3.6836667, 3.6931899, ...   \n",
              "37411  [[2.6402642000000003, 2.6402642000000003, 2.64...   \n",
              "37420  [[7.233569, 6.788397, 6.497776, 6.179102, 5.66...   \n",
              "\n",
              "                                                     CHL   \n",
              "12     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "14     [[11.465300902439024, 11.465300902439024, 11.4...   \n",
              "15     [[1.0098425657142858, 1.0098425657142858, 1.00...   \n",
              "16     [[5.9330025, 4.8541126, 3.9149563, 3.6688542, ...   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                  ...   \n",
              "37405  [[14.029530068965515, 14.029530068965515, 14.0...   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37407  [[3.369864, 3.3179538, 3.387065, 3.3800738, 3....   \n",
              "37411  [[5.465555887254902, 5.465555887254902, 5.4655...   \n",
              "37420  [[1.2806963, 1.4113798, 1.507687, 1.6207474, 1...   \n",
              "\n",
              "                                                     SPM   \n",
              "12     [[11.081272150442478, 11.081272150442478, 11.0...  \\\n",
              "14     [[5.8290533160919535, 5.8290533160919535, 5.82...   \n",
              "15     [[1.5031724625, 1.5031724625, 1.5031724625, 1....   \n",
              "16     [[0.5655572, 0.56948805, 0.58077556, 0.5880381...   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                  ...   \n",
              "37405  [[1.389701678472222, 1.389701678472222, 1.3897...   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37407  [[0.8680219, 0.8355887, 0.81544447, 0.79954606...   \n",
              "37411  [[28.75784358490566, 28.75784358490566, 28.757...   \n",
              "37420  [[0.7209197, 0.8662733, 0.96065444, 1.0638868,...   \n",
              "\n",
              "                                                   KD490   \n",
              "12     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "14     [[0.413901141097561, 0.413901141097561, 0.4139...   \n",
              "15     [[0.0943761257142857, 0.0943761257142857, 0.09...   \n",
              "16     [[0.2714681, 0.23976888, 0.20967583, 0.2015797...   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                  ...   \n",
              "37405  [[0.4712203374137931, 0.4712203374137931, 0.47...   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37407  [[0.19137645, 0.1895548, 0.19192517, 0.1916487...   \n",
              "37411  [[0.2583801460784313, 0.2583801460784313, 0.25...   \n",
              "37420  [[0.10785786, 0.11398184, 0.11837783, 0.123468...   \n",
              "\n",
              "                                                     BBP   \n",
              "12     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "14     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "15     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "16     [[0.0034846168, 0.0034846168, 0.0034846168, 0....   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                  ...   \n",
              "37405  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37407  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "37411  [[0.0587452827857142, 0.0587452827857142, 0.05...   \n",
              "37420  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                     CDM  riskLevelLabel   \n",
              "12     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \\\n",
              "14     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "15     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "16     [[0.2220552666666666, 0.2220552666666666, 0.22...               0   \n",
              "19     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "...                                                  ...             ...   \n",
              "37405  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "37406  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               1   \n",
              "37407  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "37411  [[0.4374420108333333, 0.4374420108333333, 0.43...               0   \n",
              "37420  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0   \n",
              "\n",
              "       predictions  \n",
              "12               0  \n",
              "14               0  \n",
              "15               0  \n",
              "16               0  \n",
              "19               1  \n",
              "...            ...  \n",
              "37405            0  \n",
              "37406            1  \n",
              "37407            0  \n",
              "37411            0  \n",
              "37420            0  \n",
              "\n",
              "[6430 rows x 10 columns]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test['time'] = pd.to_datetime(test['time'])\n",
        "test[test['time'].dt.month==5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHgPAVEe2iJ4"
      },
      "source": [
        "### Getting Results on Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qE0A_2xHNzO"
      },
      "source": [
        "#### Hybrid Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPCGspXHLpB",
        "outputId": "ccbd0e9e-218f-4f2b-f8d6-fbc599489717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: L3 1km x 1km 15x15, fillna = mean then 0, num_feature=6, model_type=\"convolution v3 (lenet)\", batch_size=64, epochs=98,\n",
            "           loss=wbce_custom(40), optimizer=Adam(learning_rate=0.00005), dropout=0.25,\n",
            "           existing_model = None, metrics=[\"f1\"]. added weight decay l2 regularisation. added batch norm. removed time and site pairs with >8000. \n",
            "           early stopping patience = 20. \n"
          ]
        }
      ],
      "source": [
        "old_model, old_history = load_model('nn_15x15_7_20_1619')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNisZRGOJnCu",
        "outputId": "f96733fa-42b3-42a7-defe-7eb09b4ff31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7566 1889\n",
            "60/60 [==============================] - 0s 2ms/step\n",
            "CPU times: user 444 ms, sys: 40 ms, total: 484 ms\n",
            "Wall time: 453 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Getting X_test and y_test\n",
        "xy_data = get_train_test_val_nn(sites_data,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "# Get Predictions from neural network model\n",
        "predictions = old_model.predict(X_test)\n",
        "classes = [1 if i > 0.5 else 0 for i in predictions]\n",
        "\n",
        "# Merge predictions onto test dataset\n",
        "test = pd.merge(time_site_pairs_test, sites_data, on=['time', 'site'])\n",
        "test['predictions'] = classes\n",
        "\n",
        "# Add in rest of the data, predict 0\n",
        "\n",
        "\n",
        "# Get metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlhl-8Q0N7xf",
        "outputId": "5cc38b86-6780-40be-d5fc-7dac82e49dc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7529"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(time_site_pairs_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tduDq_rjO9em",
        "outputId": "c2eaee43-154c-4ba8-d65f-50884ae732a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1889"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmKbRHT9La2I",
        "outputId": "6921ac90-b21d-488b-f4de-baaebd15789a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7566"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train) + len(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZi-SG8w2vSz"
      },
      "source": [
        "#### Loading Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIvVMH162n_x"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# model_date = \"6_26_1757\"\n",
        "# model_name = f\"nn_51x51_{model_date}\"\n",
        "\n",
        "model_names = [f\"nn_51x51_{i}\" for i in ['6_26_2221', '6_26_2310', '6_26_2343']]\n",
        "old_models = []\n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "  old_model, old_history = load_model(model_names[i])\n",
        "  old_models.append(old_model)\n",
        "\n",
        "  print(f\"Model: {model_names[i]}. Epochs: {len(old_history['loss'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrkodGP9o0d"
      },
      "source": [
        "#### Results Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVk2ZfsC9p7O"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "input_data_ = sites_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "# Getting X_test and y_test\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(3):\n",
        "  result = old_models[i].evaluate(X_test, y_test)\n",
        "  results.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9TmuzL_Pzrx"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E2nUxTU9rtg"
      },
      "outputs": [],
      "source": [
        "model_names = ['Dropout = 0', 'Dropout = 0.25', 'Dropout = 0.5']\n",
        "\n",
        "df1 = pd.DataFrame(model_names, columns=['Sub-Model'])\n",
        "df2 = pd.DataFrame(results, columns=['loss', 'acc','AUC','Precision','Recall', 'f1'])\n",
        "results_df_nn = pd.concat([df1, df2], axis=1)\n",
        "results_df_nn['Model'] = ['Neural Network']*3\n",
        "results_df_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzvsSyIYQA-S"
      },
      "outputs": [],
      "source": [
        "results_df_nn.to_csv(\"/content/drive/My Drive/CapstoneProject/Results/results_df_nn.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePmz6-rM30k"
      },
      "outputs": [],
      "source": [
        "# model_names = []\n",
        "\n",
        "# for model_type, num_feature, oversampling__ in itertools.product(model_types, num_features, oversampling_):\n",
        "#     oversample = \"Oversampling\" if oversampling__ else \"No Oversampling\"\n",
        "#     model_names.append(f'{model_type.capitalize()}, {num_feature} feature, {oversample}')\n",
        "\n",
        "# df1 = pd.DataFrame(model_names, columns=['Sub-Model'])\n",
        "# df2 = pd.DataFrame(results, columns=['loss', 'acc','AUC','Precision','Recall', 'f1'])\n",
        "# results_df_nn = pd.concat([df1, df2], axis=1)\n",
        "# results_df_nn['Model'] = ['Neural Network']*8\n",
        "# results_df_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnQwJ5nOM30k"
      },
      "outputs": [],
      "source": [
        "# model_names_string = model_names\n",
        "\n",
        "# fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(10, 12), sharex=True, sharey=True)\n",
        "\n",
        "# for j in range(8):\n",
        "#     ax = ([(k,i) for k in range(4) for i in range(2)])[j]\n",
        "#     plot_train_val_loss(histories[j], model_names_string[j], ax)\n",
        "\n",
        "# plt.suptitle('Training Loss vs Validation Loss')\n",
        "# fig.supxlabel(\"Epochs\")\n",
        "# fig.supylabel(\"Loss\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHNQB3sXM30k"
      },
      "outputs": [],
      "source": [
        "# metric='f1'\n",
        "\n",
        "# fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(10, 12), sharex=True, sharey=True)\n",
        "\n",
        "# for j in range(8):\n",
        "#     ax = ([(k,i) for k in range(4) for i in range(2)])[j]\n",
        "#     plot_train_val_metric(histories[j], model_names_string[j], ax, metric=metric)\n",
        "\n",
        "# plt.suptitle(f'Training {metric} vs Validation {metric}')\n",
        "# fig.supxlabel(\"Epochs\")\n",
        "# fig.supylabel(f\"{metric}\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCRcg5wRM30k"
      },
      "source": [
        "# Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mIcDYrmQTcg"
      },
      "outputs": [],
      "source": [
        "results_df_baseline = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Results/baseline\")\n",
        "results_df_rf = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Results/results_df_rf.csv\")\n",
        "results_df_nn = pd.read_csv(\"/content/drive/My Drive/CapstoneProject/Results/results_df_nn.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXN2rW32RJ9Y"
      },
      "outputs": [],
      "source": [
        "results_df_baseline.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "results_df_baseline.rename({'AUC_roc':'AUC'}, axis=1, inplace=True)\n",
        "results_df_baseline['Sub-Model'] = 'Baseline'\n",
        "results_df_baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlgPtL389wn6"
      },
      "outputs": [],
      "source": [
        "results_df_baseline_final = results_df_baseline[['Model', 'Sub-Model', 'AUC','Precision','Recall','f1','acc']]\n",
        "results_df_nn_final = results_df_nn[['Model', 'Sub-Model', 'AUC','Precision','Recall','f1','acc']]\n",
        "results_df_rf_final = results_df_rf[['Model', 'Sub-Model', 'AUC','Precision','Recall','f1','acc']]\n",
        "# results_df_lr_final = results_df_bc[['Model', 'Sub-Model', 'AUC','Precision','Recall','f1','acc']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GytWmwP3M30k"
      },
      "outputs": [],
      "source": [
        "pd.concat([results_df_baseline, results_df_nn_final, results_df_rf_final]).set_index(['Model', 'Sub-Model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhySSdKwM30l"
      },
      "source": [
        "# Window Size Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgmshK1M30l"
      },
      "source": [
        "## BC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4LhFCM_M30l"
      },
      "outputs": [],
      "source": [
        "df_train = df_merged_mean.merge(time_site_pairs_train, on=['time', 'site'], how='inner')\n",
        "df_test = df_merged_mean.merge(time_site_pairs_test, on=['time', 'site'], how='inner')\n",
        "\n",
        "df_train.drop(['time', 'site'], axis=1, inplace=True)\n",
        "df_test.drop(['time', 'site'], axis=1, inplace=True)\n",
        "\n",
        "y_train = df_train.pop('riskLevelLabel')\n",
        "y_test = df_test.pop('riskLevelLabel')\n",
        "\n",
        "X_train = df_train\n",
        "X_test = df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wNyT3LuM30l"
      },
      "outputs": [],
      "source": [
        "dims = ['_1x1', '3x3', '5x5', '7x7', '9x9', '11x11']\n",
        "\n",
        "performance = {}\n",
        "\n",
        "for dim in dims:\n",
        "\n",
        "    cols = [col for col in X_train.columns if dim in col]\n",
        "\n",
        "    X_train_new = X_train[cols]\n",
        "    X_test_new = X_test[cols]\n",
        "\n",
        "    if dim == '_1x1':\n",
        "        dim = '1x1'\n",
        "\n",
        "    X_train_scaled_new = scaler.fit_transform(X_train_new)\n",
        "    X_test_scaled_new = scaler.transform(X_test_new)\n",
        "    new_lg = lg_model(X_train_scaled_new, y_train)\n",
        "    y_pred = new_lg.predict(X_test_scaled_new)\n",
        "\n",
        "    performance[dim] = f1_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhGeAZpAM30l"
      },
      "outputs": [],
      "source": [
        "f1_scores = [i for i in performance.values()]\n",
        "plt.plot([1,3,5,7,9,11], f1_scores)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Window Size')\n",
        "plt.title('F1 Score with different window size, for Random Forest, Fill NA value with mean')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiYFhCJiM30l"
      },
      "source": [
        "## RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aRtAkbyM30l"
      },
      "outputs": [],
      "source": [
        "df_train = df_merged_mean.merge(time_site_pairs_train, on=['time', 'site'], how='inner')\n",
        "df_test = df_merged_mean.merge(time_site_pairs_test, on=['time', 'site'], how='inner')\n",
        "\n",
        "df_train.drop(['time', 'site'], axis=1, inplace=True)\n",
        "df_test.drop(['time', 'site'], axis=1, inplace=True)\n",
        "\n",
        "y_train = df_train.pop('riskLevelLabel')\n",
        "y_test = df_test.pop('riskLevelLabel')\n",
        "\n",
        "X_train = df_train\n",
        "X_test = df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybmWO6jRM30l"
      },
      "outputs": [],
      "source": [
        "dims = ['_1x1', '3x3', '5x5', '7x7', '9x9', '11x11']\n",
        "\n",
        "performance = {}\n",
        "\n",
        "for dim in dims:\n",
        "\n",
        "    cols = [col for col in X_train.columns if dim in col]\n",
        "\n",
        "    X_train_new = X_train[cols]\n",
        "    X_test_new = X_test[cols]\n",
        "\n",
        "    if dim == '_1x1':\n",
        "        dim = '1x1'\n",
        "\n",
        "    performance[dim] = rf_result(X_train_new, y_train, X_test_new, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwxVXIb-M30l"
      },
      "outputs": [],
      "source": [
        "f1_scores = [i['f1'] for i in performance.values()]\n",
        "plt.plot([1,3,5,7,9,11], f1_scores)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Window Size')\n",
        "plt.title('F1 Score with different window size, for Random Forest, Fill NA value with mean')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y54BwAv-M30l"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3vaN9CvM30l"
      },
      "outputs": [],
      "source": [
        "histories = []\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXR2gpX0M30l"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "# Getting Input Data\n",
        "input_data_ = input_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "j = 0\n",
        "for dim in [1,3,5,7,9,11]:\n",
        "\n",
        "    # Getting xy_data\n",
        "    xy_data = get_train_test_val_nn(input_data_,\n",
        "                           time_site_pairs_train,\n",
        "                           time_site_pairs_test,\n",
        "                           dim=dim)\n",
        "\n",
        "    # Get history and result\n",
        "    _, history, result = fit_nn(xy_data, \"convolution\")\n",
        "    histories.append(history)\n",
        "    results.append(result)\n",
        "\n",
        "    j += 1\n",
        "    clear_output(wait=True)\n",
        "    print(f'Progress: {j}/6')\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIVoNZRCM30l"
      },
      "outputs": [],
      "source": [
        "f1_scores = [i[5] for i in results]\n",
        "plt.plot([1,3,5,7,9,11], f1_scores)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Window Size')\n",
        "plt.title('F1 Score with different window size, for Convolutional Neural Network, 3 feature, No Oversampling')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw0R9N0gM30m"
      },
      "source": [
        "# Accuracy Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZhXFnbPM30m"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zJjjPInM30m"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# convolution, 3 features, no oversampling\n",
        "\n",
        "# Getting Input Data\n",
        "input_data_ = input_data[['CHL', 'SPM', 'TUR', 'riskLevelLabel', 'site', 'time']]\n",
        "\n",
        "\n",
        "# Getting xy_data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                       time_site_pairs_train,\n",
        "                       time_site_pairs_test)\n",
        "\n",
        "# Get history and result\n",
        "model, history, result = fit_nn(xy_data, \"convolution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNXrQlNvM30m"
      },
      "outputs": [],
      "source": [
        "test = pd.merge(time_site_pairs_test, input_data_, on=['time', 'site'])\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOnKLhEWM30m"
      },
      "outputs": [],
      "source": [
        "y_test_preds = model.predict(xy_data['X_test'])\n",
        "test['preds'] = [1 if x >= 0.5 else 0 for x in y_test_preds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThK0dlffM30m"
      },
      "outputs": [],
      "source": [
        "test['time'] = pd.to_datetime(test['time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-YfFBeEM30m"
      },
      "outputs": [],
      "source": [
        "test.groupby(pd.Grouper(key='time', freq='M')).apply(lambda x: f1_score(x['riskLevelLabel'], x['preds']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uNZ3MvNxM30m"
      },
      "outputs": [],
      "source": [
        "s2 = pd.read_pickle(\"~data/nn_51x51.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>site</th>\n",
              "      <th>TUR</th>\n",
              "      <th>SPM</th>\n",
              "      <th>CHL</th>\n",
              "      <th>riskLevelLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-02-29</td>\n",
              "      <td>ukk4304-34200</td>\n",
              "      <td>[[-0.9811192569131857, -0.9795547499650448, -0...</td>\n",
              "      <td>[[0.524045278902291, 0.5756248475971223, 0.560...</td>\n",
              "      <td>[[0.34057330828591803, 0.370465598289415, 0.34...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-12</td>\n",
              "      <td>uke1200-08100</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-14</td>\n",
              "      <td>ukc1101-06000</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-14</td>\n",
              "      <td>ukc1101-06100</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-14</td>\n",
              "      <td>ukc1101-06200</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.401122631769...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.27785837711...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24223761712...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52282</th>\n",
              "      <td>2022-09-30</td>\n",
              "      <td>ukj4210-12750</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52283</th>\n",
              "      <td>2022-09-30</td>\n",
              "      <td>ukj4210-12800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52284</th>\n",
              "      <td>2022-09-30</td>\n",
              "      <td>ukj4210-12850</td>\n",
              "      <td>[[1.5549465060235468, 1.5557287594976177, 1.55...</td>\n",
              "      <td>[[1.5127710419438254, 2.94540271916225, 2.7731...</td>\n",
              "      <td>[[1.5251629301217988, 3.2531680809087082, 2.70...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52285</th>\n",
              "      <td>2022-09-30</td>\n",
              "      <td>ukj4210-12900</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5525997...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9585223...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8346373...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52286</th>\n",
              "      <td>2022-12-08</td>\n",
              "      <td>ukk3103-29100</td>\n",
              "      <td>[[-1.3652057126818273, -1.3644234592077567, -1...</td>\n",
              "      <td>[[-0.44285311187494875, -0.4286347028231429, -...</td>\n",
              "      <td>[[-0.3412926844142351, -0.332920686030616, -0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52287 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             time           site   \n",
              "0      2020-02-29  ukk4304-34200  \\\n",
              "1      2020-03-12  uke1200-08100   \n",
              "2      2020-05-14  ukc1101-06000   \n",
              "3      2020-05-14  ukc1101-06100   \n",
              "4      2020-05-14  ukc1101-06200   \n",
              "...           ...            ...   \n",
              "52282  2022-09-30  ukj4210-12750   \n",
              "52283  2022-09-30  ukj4210-12800   \n",
              "52284  2022-09-30  ukj4210-12850   \n",
              "52285  2022-09-30  ukj4210-12900   \n",
              "52286  2022-12-08  ukk3103-29100   \n",
              "\n",
              "                                                     TUR   \n",
              "0      [[-0.9811192569131857, -0.9795547499650448, -0...  \\\n",
              "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.401122631769...   \n",
              "...                                                  ...   \n",
              "52282  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "52283  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "52284  [[1.5549465060235468, 1.5557287594976177, 1.55...   \n",
              "52285  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5525997...   \n",
              "52286  [[-1.3652057126818273, -1.3644234592077567, -1...   \n",
              "\n",
              "                                                     SPM   \n",
              "0      [[0.524045278902291, 0.5756248475971223, 0.560...  \\\n",
              "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.27785837711...   \n",
              "...                                                  ...   \n",
              "52282  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "52283  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "52284  [[1.5127710419438254, 2.94540271916225, 2.7731...   \n",
              "52285  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9585223...   \n",
              "52286  [[-0.44285311187494875, -0.4286347028231429, -...   \n",
              "\n",
              "                                                     CHL riskLevelLabel  \n",
              "0      [[0.34057330828591803, 0.370465598289415, 0.34...              1  \n",
              "1      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...              1  \n",
              "2      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...              0  \n",
              "3      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...              0  \n",
              "4      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.24223761712...              0  \n",
              "...                                                  ...            ...  \n",
              "52282  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...              0  \n",
              "52283  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...              0  \n",
              "52284  [[1.5251629301217988, 3.2531680809087082, 2.70...              0  \n",
              "52285  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8346373...              0  \n",
              "52286  [[-0.3412926844142351, -0.332920686030616, -0....              0  \n",
              "\n",
              "[52287 rows x 6 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New sectoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.4819021  1.7494234  2.7225785  3.542849   3.8239558 ]\n",
            " [1.2576548  2.55228326 3.147176   3.379878   3.4176855 ]\n",
            " [2.55228326 2.55228326 2.55228326 2.911971   2.917507  ]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.50183   ]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.2846975 ]]\n"
          ]
        }
      ],
      "source": [
        "xy_data = get_train_test_val_nn(sites_data, time_site_pairs_train, time_site_pairs_test, dim=5)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_data, train_labels, test_labels = sites_data, time_site_pairs_train, time_site_pairs_test\n",
        "\n",
        "train = pd.merge(train_labels, input_data, on=['time', 'site'])\n",
        "test = pd.merge(test_labels, input_data, on=['time', 'site'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>site</th>\n",
              "      <th>ZSD</th>\n",
              "      <th>CHL</th>\n",
              "      <th>SPM</th>\n",
              "      <th>KD490</th>\n",
              "      <th>BBP</th>\n",
              "      <th>CDM</th>\n",
              "      <th>riskLevelLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-25</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-07-04</td>\n",
              "      <td>ukk4305-25800</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-08-08</td>\n",
              "      <td>ukk3105-31450</td>\n",
              "      <td>[[3.3500542033613443, 3.3500542033613443, 3.35...</td>\n",
              "      <td>[[7.30686214117647, 7.30686214117647, 7.306862...</td>\n",
              "      <td>[[0.9704474882575758, 0.9704474882575758, 0.97...</td>\n",
              "      <td>[[0.276538217815126, 0.276538217815126, 0.2765...</td>\n",
              "      <td>[[0.0036920992657894, 0.0036920992657894, 0.00...</td>\n",
              "      <td>[[0.144712753622807, 0.144712753622807, 0.1447...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-08-27</td>\n",
              "      <td>ukk2206-20800</td>\n",
              "      <td>[[6.7459197, 6.6718507, 6.566642, 6.4457297, 6...</td>\n",
              "      <td>[[1.4204581, 1.4440619, 1.4789817, 1.5192833, ...</td>\n",
              "      <td>[[3.0617452, 3.2359302, 3.3227324, 3.4029508, ...</td>\n",
              "      <td>[[0.114441626, 0.11552915, 0.11712431, 0.11896...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-06-08</td>\n",
              "      <td>uke1301-09020</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112261</th>\n",
              "      <td>2021-07-19</td>\n",
              "      <td>ukj4210-12660</td>\n",
              "      <td>[[3.130174548245614, 3.130174548245614, 3.1301...</td>\n",
              "      <td>[[4.270181615789474, 4.270181615789474, 4.2701...</td>\n",
              "      <td>[[5.880774268461538, 5.880774268461538, 5.8807...</td>\n",
              "      <td>[[0.2214226485964912, 0.2214226485964912, 0.22...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112262</th>\n",
              "      <td>2021-05-10</td>\n",
              "      <td>ukk4200-23200</td>\n",
              "      <td>[[3.55662035, 3.55662035, 3.55662035, 3.556620...</td>\n",
              "      <td>[[3.5508095, 3.5508095, 3.5508095, 3.5508095, ...</td>\n",
              "      <td>[[2.170633925, 2.170633925, 2.170633925, 2.170...</td>\n",
              "      <td>[[0.19762554, 0.19762554, 0.19762554, 0.197625...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112263</th>\n",
              "      <td>2020-08-10</td>\n",
              "      <td>ukk3104-33500</td>\n",
              "      <td>[[10.020489333333334, 10.020489333333334, 10.0...</td>\n",
              "      <td>[[0.7708462333333334, 0.7708462333333334, 0.77...</td>\n",
              "      <td>[[0.9726023225, 0.9726023225, 0.9726023225, 0....</td>\n",
              "      <td>[[0.0815022166666666, 0.0815022166666666, 0.08...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112264</th>\n",
              "      <td>2021-09-08</td>\n",
              "      <td>ukk1202-35700</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112265</th>\n",
              "      <td>2020-08-09</td>\n",
              "      <td>ukk3101-27000</td>\n",
              "      <td>[[8.295259, 8.60793, 8.488777, 8.288995, 8.061...</td>\n",
              "      <td>[[1.0417088, 0.9814289, 1.0022881, 1.0407631, ...</td>\n",
              "      <td>[[0.36700276, 0.37847954, 0.3985074, 0.4057661...</td>\n",
              "      <td>[[0.09601207, 0.092920884, 0.094014056, 0.0959...</td>\n",
              "      <td>[[0.0017447872, 0.0018362086, 0.0018678783, 0....</td>\n",
              "      <td>[[0.10112062, 0.09079216, 0.08552243, 0.083623...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112266 rows  9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              time           site   \n",
              "0       2021-05-25  ukk4305-25800  \\\n",
              "1       2022-07-04  ukk4305-25800   \n",
              "2       2022-08-08  ukk3105-31450   \n",
              "3       2022-08-27  ukk2206-20800   \n",
              "4       2020-06-08  uke1301-09020   \n",
              "...            ...            ...   \n",
              "112261  2021-07-19  ukj4210-12660   \n",
              "112262  2021-05-10  ukk4200-23200   \n",
              "112263  2020-08-10  ukk3104-33500   \n",
              "112264  2021-09-08  ukk1202-35700   \n",
              "112265  2020-08-09  ukk3101-27000   \n",
              "\n",
              "                                                      ZSD   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[3.3500542033613443, 3.3500542033613443, 3.35...   \n",
              "3       [[6.7459197, 6.6718507, 6.566642, 6.4457297, 6...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[3.130174548245614, 3.130174548245614, 3.1301...   \n",
              "112262  [[3.55662035, 3.55662035, 3.55662035, 3.556620...   \n",
              "112263  [[10.020489333333334, 10.020489333333334, 10.0...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[8.295259, 8.60793, 8.488777, 8.288995, 8.061...   \n",
              "\n",
              "                                                      CHL   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[7.30686214117647, 7.30686214117647, 7.306862...   \n",
              "3       [[1.4204581, 1.4440619, 1.4789817, 1.5192833, ...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[4.270181615789474, 4.270181615789474, 4.2701...   \n",
              "112262  [[3.5508095, 3.5508095, 3.5508095, 3.5508095, ...   \n",
              "112263  [[0.7708462333333334, 0.7708462333333334, 0.77...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[1.0417088, 0.9814289, 1.0022881, 1.0407631, ...   \n",
              "\n",
              "                                                      SPM   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.9704474882575758, 0.9704474882575758, 0.97...   \n",
              "3       [[3.0617452, 3.2359302, 3.3227324, 3.4029508, ...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[5.880774268461538, 5.880774268461538, 5.8807...   \n",
              "112262  [[2.170633925, 2.170633925, 2.170633925, 2.170...   \n",
              "112263  [[0.9726023225, 0.9726023225, 0.9726023225, 0....   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.36700276, 0.37847954, 0.3985074, 0.4057661...   \n",
              "\n",
              "                                                    KD490   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.276538217815126, 0.276538217815126, 0.2765...   \n",
              "3       [[0.114441626, 0.11552915, 0.11712431, 0.11896...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[0.2214226485964912, 0.2214226485964912, 0.22...   \n",
              "112262  [[0.19762554, 0.19762554, 0.19762554, 0.197625...   \n",
              "112263  [[0.0815022166666666, 0.0815022166666666, 0.08...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.09601207, 0.092920884, 0.094014056, 0.0959...   \n",
              "\n",
              "                                                      BBP   \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "2       [[0.0036920992657894, 0.0036920992657894, 0.00...   \n",
              "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "...                                                   ...   \n",
              "112261  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112262  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112263  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "112265  [[0.0017447872, 0.0018362086, 0.0018678783, 0....   \n",
              "\n",
              "                                                      CDM  riskLevelLabel  \n",
              "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "2       [[0.144712753622807, 0.144712753622807, 0.1447...               0  \n",
              "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "...                                                   ...             ...  \n",
              "112261  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "112262  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "112263  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               0  \n",
              "112264  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...               1  \n",
              "112265  [[0.10112062, 0.09079216, 0.08552243, 0.083623...               0  \n",
              "\n",
              "[112266 rows x 9 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3.0617452 , 3.2359302 , 3.3227324 , 3.4029508 , 3.1574073 ,\n",
              "        2.8736236 , 2.4701855 , 1.1479132 , 2.55228326, 2.55228326,\n",
              "        2.9266276 , 3.1363342 , 3.3378937 , 3.6139536 , 3.8339348 ],\n",
              "       [3.2429392 , 3.2713253 , 3.239215  , 3.1935227 , 2.8879313 ,\n",
              "        2.5589433 , 2.2047923 , 1.5037377 , 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.9470282 , 2.9978037 , 3.0879948 , 3.4209468 ],\n",
              "       [3.381822  , 3.2321131 , 3.1171763 , 2.9970057 , 2.4799316 ,\n",
              "        2.1646128 , 1.8915683 , 1.70582   , 1.7058163 , 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.463403  , 2.668908  , 3.14289   ],\n",
              "       [3.3067923 , 3.108512  , 2.845334  , 2.5451927 , 1.9333152 ,\n",
              "        1.7099026 , 1.5353427 , 1.542206  , 1.5422037 , 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.2490697 , 2.6466756 , 3.033093  ],\n",
              "       [3.0228162 , 2.6908624 , 2.3387325 , 1.9486151 , 1.5884695 ,\n",
              "        1.4770083 , 1.4317526 , 1.7419043 , 2.6052518 , 2.55228326,\n",
              "        2.55228326, 2.7998922 , 2.7958941 , 2.878689  , 3.040211  ],\n",
              "       [2.6860619 , 2.2902606 , 2.0202277 , 1.7454636 , 1.4193717 ,\n",
              "        1.4819021 , 1.7494234 , 2.7225785 , 3.542849  , 3.8239558 ,\n",
              "        3.493297  , 3.3240626 , 3.2258062 , 3.1263566 , 3.0819123 ],\n",
              "       [2.329229  , 2.0379205 , 1.8425145 , 1.6445456 , 1.2576574 ,\n",
              "        1.2576548 , 2.55228326, 3.147176  , 3.379878  , 3.4176855 ,\n",
              "        3.3158808 , 3.2424042 , 3.187522  , 3.1226444 , 3.0836236 ],\n",
              "       [2.0550864 , 1.7601852 , 1.6403148 , 1.5389395 , 1.2037545 ,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.911971  , 2.917507  ,\n",
              "        2.9349794 , 3.0724533 , 3.1019967 , 3.0935578 , 3.0146942 ],\n",
              "       [1.9335757 , 1.4992673 , 1.4486336 , 1.4676266 , 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.50183   ,\n",
              "        2.5111234 , 2.7767386 , 2.935776  , 3.100465  , 2.9159224 ],\n",
              "       [1.9380358 , 1.4160666 , 1.3451484 , 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.2846975 ,\n",
              "        2.2814038 , 2.4272785 , 2.6087742 , 3.1040683 , 2.8784409 ],\n",
              "       [2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.2383645 , 2.2383645 , 2.55228326, 2.55228326],\n",
              "       [2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326],\n",
              "       [2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326],\n",
              "       [2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326],\n",
              "       [2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326,\n",
              "        2.55228326, 2.55228326, 2.55228326, 2.55228326, 2.55228326]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['SPM'][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim = 5\n",
        "\n",
        "w = int((dim-1)/2)\n",
        "\n",
        "indices = np.array(range(1,226)).reshape(15,15)[7-w:8+w, 7-w:8+w].flatten()\n",
        "indices = [i-1 for i in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "dim = 5\n",
        "\n",
        "w = int((dim-1)/2)\n",
        "\n",
        "train_new = pd.DataFrame()\n",
        "test_new = pd.DataFrame()\n",
        "\n",
        "def get_windowed_data(row):\n",
        "    indices = np.array(range(1,226)).reshape(15,15)[7-w:8+w, 7-w:8+w].flatten()\n",
        "    indices = [i-1 for i in indices]\n",
        "    values = row.flatten()[[indices]].reshape(dim,dim)\n",
        "    return values\n",
        "\n",
        "if dim != 15:\n",
        "    for feature in ['ZSD','CHL','SPM','KD490','BBP','CDM']:\n",
        "        train[f'{feature}'] = train[f'{feature}'].apply(get_windowed_data)\n",
        "        test[f'{feature}'] = test[f'{feature}'].apply(get_windowed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "row = trial\n",
        "\n",
        "indices = np.array(range(1,226)).reshape(15,15)[7-w:8+w, 7-w:8+w].flatten()\n",
        "indices = [i-1 for i in indices]\n",
        "values = row.flatten()[[indices]].reshape(dim,dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0         [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "1         [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "2         [[0.9704474882575758, 0.9704474882575758, 0.97...\n",
              "3         [[3.2713253, 3.239215, 3.1935227, 2.8879313, 2...\n",
              "4         [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "                                ...                        \n",
              "112261    [[5.880774268461538, 5.880774268461538, 5.8807...\n",
              "112262    [[2.170633925, 2.170633925, 2.170633925, 2.170...\n",
              "112263    [[0.9726023225, 0.9726023225, 0.9726023225, 0....\n",
              "112264    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "112265    [[0.37191105, 0.37656707, 0.3811384, 0.3862034...\n",
              "Name: SPM, Length: 112266, dtype: object"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(w)\n",
        "train['SPM'].apply(get_windowed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = wbce_custom(3)\n",
        "metric = f1\n",
        "best_model_f0 = tf.keras.models.load_model('cnn_f0_best.keras', custom_objects={loss.__name__: loss, metric.__name__: metric})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3.0617452  3.2359302  3.3227324  3.4029508  3.1574073  2.8736236\n",
            "  2.4701855  1.1479132  2.55228326 2.55228326 2.9266276  3.1363342\n",
            "  3.3378937  3.6139536  3.8339348 ]\n",
            " [3.2429392  3.2713253  3.239215   3.1935227  2.8879313  2.5589433\n",
            "  2.2047923  1.5037377  2.55228326 2.55228326 2.55228326 2.9470282\n",
            "  2.9978037  3.0879948  3.4209468 ]\n",
            " [3.381822   3.2321131  3.1171763  2.9970057  2.4799316  2.1646128\n",
            "  1.8915683  1.70582    1.7058163  2.55228326 2.55228326 2.55228326\n",
            "  2.463403   2.668908   3.14289   ]\n",
            " [3.3067923  3.108512   2.845334   2.5451927  1.9333152  1.7099026\n",
            "  1.5353427  1.542206   1.5422037  2.55228326 2.55228326 2.55228326\n",
            "  2.2490697  2.6466756  3.033093  ]\n",
            " [3.0228162  2.6908624  2.3387325  1.9486151  1.5884695  1.4770083\n",
            "  1.4317526  1.7419043  2.6052518  2.55228326 2.55228326 2.7998922\n",
            "  2.7958941  2.878689   3.040211  ]\n",
            " [2.6860619  2.2902606  2.0202277  1.7454636  1.4193717  1.4819021\n",
            "  1.7494234  2.7225785  3.542849   3.8239558  3.493297   3.3240626\n",
            "  3.2258062  3.1263566  3.0819123 ]\n",
            " [2.329229   2.0379205  1.8425145  1.6445456  1.2576574  1.2576548\n",
            "  2.55228326 3.147176   3.379878   3.4176855  3.3158808  3.2424042\n",
            "  3.187522   3.1226444  3.0836236 ]\n",
            " [2.0550864  1.7601852  1.6403148  1.5389395  1.2037545  2.55228326\n",
            "  2.55228326 2.55228326 2.911971   2.917507   2.9349794  3.0724533\n",
            "  3.1019967  3.0935578  3.0146942 ]\n",
            " [1.9335757  1.4992673  1.4486336  1.4676266  2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.50183    2.5111234  2.7767386\n",
            "  2.935776   3.100465   2.9159224 ]\n",
            " [1.9380358  1.4160666  1.3451484  2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.2846975  2.2814038  2.4272785\n",
            "  2.6087742  3.1040683  2.8784409 ]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.2383645\n",
            "  2.2383645  2.55228326 2.55228326]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326]\n",
            " [2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326 2.55228326 2.55228326 2.55228326\n",
            "  2.55228326 2.55228326 2.55228326]]\n",
            "1170/1170 [==============================] - 1s 774us/step\n"
          ]
        }
      ],
      "source": [
        "input_data_ = sites_data\n",
        "\n",
        "# Getting \"test\" data\n",
        "xy_data = get_train_test_val_nn(input_data_,\n",
        "                      time_site_pairs_train,\n",
        "                      time_site_pairs_test)\n",
        "\n",
        "X_train, X_test, X_val, y_train, y_test, y_val = xy_data.values()\n",
        "\n",
        "predictions_f0 = best_model_f0.predict(X_test)\n",
        "classes_f0 = [1 if i > 0.5 else 0 for i in predictions_f0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 808us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>pr_auc</th>\n",
              "      <th>roc_auc</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.07418</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.623</td>\n",
              "      <td>0.537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        f1  precision  recall  pr_auc  roc_auc  accuracy\n",
              "0  0.07418      0.039   0.672   0.039    0.623     0.537"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, average_precision_score, f1_score\n",
        "\n",
        "metrics = []\n",
        "\n",
        "for model in [best_model_f0]:\n",
        "\n",
        "    pred_prob = model.predict(X_test)\n",
        "    pred_class = [1 if i>0.5 else 0 for i in pred_prob]\n",
        "    \n",
        "    f1_ = round(f1_score(y_test, pred_class), 5)\n",
        "    precision = round(precision_score(y_test, pred_class), 3)\n",
        "    recall = round(recall_score(y_test, pred_class), 3)\n",
        "    pr_auc = round(average_precision_score(y_test, pred_prob), 3)\n",
        "    roc_auc = round(roc_auc_score(y_test, pred_prob), 3)   \n",
        "    accuracy = round(accuracy_score(y_test, pred_class), 3)\n",
        "\n",
        "    metrics.append([f1_,precision,recall,pr_auc,roc_auc,accuracy])\n",
        "\n",
        "pd.DataFrame(metrics, columns=['f1','precision','recall','pr_auc','roc_auc','accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1170/1170 [==============================] - 1s 1ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = best_model_f0\n",
        "pred_prob = model.predict(X_test)\n",
        "pred_class = [1 if i>0.5 else 0 for i in pred_prob]\n",
        "\n",
        "confusion_matrix_plot(y_test, pred_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc_pr_graphs(model, predictions):\n",
        "    # predictions from best model on validation data\n",
        "    # predictions = model.predict(X_test)\n",
        "\n",
        "    # ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
        "    roc_auc = roc_auc_score(y_test, predictions)\n",
        "\n",
        "    # PR curve and AP\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
        "    average_precision = average_precision_score(y_test, predictions)\n",
        "\n",
        "    # plots\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, marker='.',label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate', fontsize=15)\n",
        "    plt.ylabel('True Positive Rate', fontsize=15)\n",
        "    plt.title('Receiver Operating Characteristic (ROC)', fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"lower right\", fontsize=15)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, marker='.', label='PR curve (AP = %0.4f)' % average_precision)\n",
        "    plt.axhline(y=baseline_val, color=\"gray\", linestyle='--', label='Baseline')\n",
        "    plt.xlabel('Recall', fontsize=15)\n",
        "    plt.ylabel('Precision', fontsize=15)\n",
        "    plt.title('Precision-Recall (PR) Curve', fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"upper right\", fontsize=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.027960379812231664"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.merge(time_site_pairs_train, sites_data, on=['time', 'site'])\n",
        "baseline_val = sum(train['riskLevelLabel'])/len(train)\n",
        "baseline_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_roc_pr_graphs(best_model_f0, predictions_f0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1vIrrlI5qYYy",
        "WCq5OTnjM30d",
        "fCm5MtmEM30d",
        "Inlpz-2fyRS-",
        "CQ2WRerRdX5E",
        "7_MflnfHQXCz",
        "qbDjSSIaoPmH",
        "um8esTkeoRAe"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "local-venv-kernel",
      "language": "python",
      "name": "local-venv-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
